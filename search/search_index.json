{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"wiperf V2: An Open Source UX Performance Probe Wiperf is a utility that can be installed on a WLAN Pi or a Raspberry Pi to act as a network probe that runs a series of network performance tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as an ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla/Librespeed) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP SMB Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is ( usually ) no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where issues are being experienced and longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana Data Server The core focus of this project is the probe device that gathers the network performance data in which we are interested. However, the data server is a critical component that allows visualization of that performance data. High-level configuration details will be provided to \"get you going\", but detailed information about the operation of these platforms is beyond the scope of this project. Both of the data servers supported are \"NoSQL\" servers, which means that no data structures have to be pre-defined in database tables. This means we can send our data structures, that contain network performance data, to the server with very little set-up compared to traditional database servers. As long as we have a valid set of credentials for the data server, we can just send JSON-formatted data over HTTPS in whatever structure we choose. A database query language on the data server allows us to retrieve and graph the data collected by the wiperf probe. Splunk Splunk is supported on all popular operating systems and is very easy to set up on your server of choice. It acts as both the data store and visualization platform. Splunk is a commercial, rather than open-source product. The volume of data returned by the probe is very low, so the free tier of Splunk may be used to gather and report on data. For details on how to set up a Splunk server, start at this documentation page: [link][splunk_platform.md] Splunk product web site: https://www.splunk.com/ InfluxDB/Grafana Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data server from which to pull its network performance data. To meet this requirement, the InfluxDB database server is used. Like Grafana, InfluxDB is also an open-source package. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana web site (v6.7): https://grafana.com/ Influx web site (v.1.8): https://www.influxdata.com Workflow to Setup Wiperf The workflow to get Wiperf fully operational consists of a number of steps that break down in to two main areas: Probe setup (the RPi or WLAN Pi device itself) Data server setup (the Splunk or Influx/Grafana server) The data server setup tends to be a task that needs completion only once (or at least very infrequently). Conversely, some or all of the probe setup will need to be completed each time a probe is deployed - this is mainly due to the fact that in each environment in which it is deployed, the connectivity for the probe will vary (e.g. different SSID, different network connection type etc.). Here is an overview of the workflow:: Data server setup: Prepare a server platform Obtain the data server application software Install the data server application(s) Configure the data server application(s) Probe setup: Obtain a probe device (Raspberry Pi or WLAN Pi) Prepare the device for the wiperf software Install the wiperf software Configure the wiperf software Deploy & test the wiperf probe Links: Start here for Splunk: link Start here for InfluxDB/Grafana: link Start here for the probe: link In addition to the setup and deployment of the components, there may also be a requirement to troubleshoot the setup. The following pages provide useful support information: Troubleshooting steps Review known issues / FAQ ] Further Documentation References Configuration file parameters Data points sent by the probe to the data server platform Credits This project has had some great input from a number of people. Here are a few words of thanks to those who have been so generous in helping out. Thanks to Kristian Roberts for his invaluable project input, testing and guidance on Splunk. He kicked this whole thing off and it definitely wouldn't have happened without him. A top bloke. Thanks also to Eric Garnel and James Whitehead for their invaluable help in providing me so many Grafana dashboard files to \"borrow\" from. It was a steep learning curve for me, but the generosity of Eric & James really helped me to get to grips with Grafana. Also thanks to Eric for providing the idea to use InfluxDB as a data source (....even if I did use the wrong version initially! Lol). The code for the MOS score calculation was kindly provided by Mario Gingras. What a great idea...I wish I'd thought of that! Thanks Mario, it's great addition. Caveats This free software is provided for you to use at your own risk. There are no guarantees around its operation, suitability or accuracy of the data that it provides. Please consult the license file shipped with this software. Developer Nigel Bowden (WifiNigel): (https://wifinigel.blogspot.com)[https://wifinigel.blogspot.com]{target=_blank} (https://github.com/wifinigel)[https://github.com/wifinigel]{target=_blank}","title":"Home"},{"location":"#wiperf-v2-an-open-source-ux-performance-probe","text":"Wiperf is a utility that can be installed on a WLAN Pi or a Raspberry Pi to act as a network probe that runs a series of network performance tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as an ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla/Librespeed) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP SMB Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is ( usually ) no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where issues are being experienced and longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"wiperf V2: An Open Source UX Performance Probe"},{"location":"#data-server","text":"The core focus of this project is the probe device that gathers the network performance data in which we are interested. However, the data server is a critical component that allows visualization of that performance data. High-level configuration details will be provided to \"get you going\", but detailed information about the operation of these platforms is beyond the scope of this project. Both of the data servers supported are \"NoSQL\" servers, which means that no data structures have to be pre-defined in database tables. This means we can send our data structures, that contain network performance data, to the server with very little set-up compared to traditional database servers. As long as we have a valid set of credentials for the data server, we can just send JSON-formatted data over HTTPS in whatever structure we choose. A database query language on the data server allows us to retrieve and graph the data collected by the wiperf probe.","title":"Data Server"},{"location":"#splunk","text":"Splunk is supported on all popular operating systems and is very easy to set up on your server of choice. It acts as both the data store and visualization platform. Splunk is a commercial, rather than open-source product. The volume of data returned by the probe is very low, so the free tier of Splunk may be used to gather and report on data. For details on how to set up a Splunk server, start at this documentation page: [link][splunk_platform.md] Splunk product web site: https://www.splunk.com/","title":"Splunk"},{"location":"#influxdbgrafana","text":"Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data server from which to pull its network performance data. To meet this requirement, the InfluxDB database server is used. Like Grafana, InfluxDB is also an open-source package. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana web site (v6.7): https://grafana.com/ Influx web site (v.1.8): https://www.influxdata.com","title":"InfluxDB/Grafana"},{"location":"#workflow-to-setup-wiperf","text":"The workflow to get Wiperf fully operational consists of a number of steps that break down in to two main areas: Probe setup (the RPi or WLAN Pi device itself) Data server setup (the Splunk or Influx/Grafana server) The data server setup tends to be a task that needs completion only once (or at least very infrequently). Conversely, some or all of the probe setup will need to be completed each time a probe is deployed - this is mainly due to the fact that in each environment in which it is deployed, the connectivity for the probe will vary (e.g. different SSID, different network connection type etc.). Here is an overview of the workflow:: Data server setup: Prepare a server platform Obtain the data server application software Install the data server application(s) Configure the data server application(s) Probe setup: Obtain a probe device (Raspberry Pi or WLAN Pi) Prepare the device for the wiperf software Install the wiperf software Configure the wiperf software Deploy & test the wiperf probe Links: Start here for Splunk: link Start here for InfluxDB/Grafana: link Start here for the probe: link In addition to the setup and deployment of the components, there may also be a requirement to troubleshoot the setup. The following pages provide useful support information: Troubleshooting steps Review known issues / FAQ ]","title":"Workflow to Setup Wiperf"},{"location":"#further-documentation-references","text":"Configuration file parameters Data points sent by the probe to the data server platform","title":"Further Documentation References"},{"location":"#credits","text":"This project has had some great input from a number of people. Here are a few words of thanks to those who have been so generous in helping out. Thanks to Kristian Roberts for his invaluable project input, testing and guidance on Splunk. He kicked this whole thing off and it definitely wouldn't have happened without him. A top bloke. Thanks also to Eric Garnel and James Whitehead for their invaluable help in providing me so many Grafana dashboard files to \"borrow\" from. It was a steep learning curve for me, but the generosity of Eric & James really helped me to get to grips with Grafana. Also thanks to Eric for providing the idea to use InfluxDB as a data source (....even if I did use the wrong version initially! Lol). The code for the MOS score calculation was kindly provided by Mario Gingras. What a great idea...I wish I'd thought of that! Thanks Mario, it's great addition.","title":"Credits"},{"location":"#caveats","text":"This free software is provided for you to use at your own risk. There are no guarantees around its operation, suitability or accuracy of the data that it provides. Please consult the license file shipped with this software.","title":"Caveats"},{"location":"#developer","text":"Nigel Bowden (WifiNigel): (https://wifinigel.blogspot.com)[https://wifinigel.blogspot.com]{target=_blank} (https://github.com/wifinigel)[https://github.com/wifinigel]{target=_blank}","title":"Developer"},{"location":"404/","text":"404 - Unknown Page Sorry, looks like we've hit an unknown page address. Please check the address you entered, or let us know if we've sent you here from a bad link on our site.","title":"Unknown Page"},{"location":"404/#404-unknown-page","text":"Sorry, looks like we've hit an unknown page address. Please check the address you entered, or let us know if we've sent you here from a bad link on our site.","title":"404 - Unknown Page"},{"location":"about/","text":"About Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla/Librespeed) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP SMB Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana Developer The primary developer for this project is Nigel Bowden. Check me out in the following places: LinkedIn Twitter Blog","title":"About"},{"location":"about/#about","text":"Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla/Librespeed) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP SMB Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"About"},{"location":"about/#developer","text":"The primary developer for this project is Nigel Bowden. Check me out in the following places: LinkedIn Twitter Blog","title":"Developer"},{"location":"adv_fixed_bssid/","text":"Configuring the Probe to Test Against Only One Access Point There may be instances when there is a requirement to ensure that the wiperf probe connects only to a specific wireless access point (AP). In an environment where there are multiple APs that broadcast the same SSID (network name), the probe may roam between different access points over time (which is normal 802.11 operation) If you like to ensure that the probe never moves from a specific AP, you will need to add a \"BSSID\" address to the wiperf probe supplicant file. (Note: if the AP ever becomes unavailable, the probe will be stranded without comms as it will not roam to another AP, even if it broadcasts the same network name) When an AP broadcasts an SSID (i.e. the network name), it will use a specific \"BSSID\" for that network name, which is effectively a unique MAC address for that network name. To fix the probe to use only one specific AP, you will need to find the BSSID of the network name to which you would like it fixed. If you use a wireless network scanner tool, it will generally show you SSID names and their associated BSSIDs (MAC addresses). Once you have obtained the BSSID you need, this needs to be added to the /etc/wpa_supplicant/wpa_supplicant.conf file for the RPI, or the /etc/wiperf/conf/etc/wpa_supplicant/wpa_supplicant.conf file for the WLAN Pi. Here is an simple example configuration file that shows the BSSID configuration parameter being used - note the \"bssid\" configuration parameter: ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 country=GB network={ ssid=\"My_Home_Network\" psk=\"s3cret_pwd\" bssid=01:23:45:67:89:ab } After setting this new configuration parameter, reboot the probe to ensure it is used by the wireless interface of the probe.","title":"Configuring a Fixed Access Point"},{"location":"adv_fixed_bssid/#configuring-the-probe-to-test-against-only-one-access-point","text":"There may be instances when there is a requirement to ensure that the wiperf probe connects only to a specific wireless access point (AP). In an environment where there are multiple APs that broadcast the same SSID (network name), the probe may roam between different access points over time (which is normal 802.11 operation) If you like to ensure that the probe never moves from a specific AP, you will need to add a \"BSSID\" address to the wiperf probe supplicant file. (Note: if the AP ever becomes unavailable, the probe will be stranded without comms as it will not roam to another AP, even if it broadcasts the same network name) When an AP broadcasts an SSID (i.e. the network name), it will use a specific \"BSSID\" for that network name, which is effectively a unique MAC address for that network name. To fix the probe to use only one specific AP, you will need to find the BSSID of the network name to which you would like it fixed. If you use a wireless network scanner tool, it will generally show you SSID names and their associated BSSIDs (MAC addresses). Once you have obtained the BSSID you need, this needs to be added to the /etc/wpa_supplicant/wpa_supplicant.conf file for the RPI, or the /etc/wiperf/conf/etc/wpa_supplicant/wpa_supplicant.conf file for the WLAN Pi. Here is an simple example configuration file that shows the BSSID configuration parameter being used - note the \"bssid\" configuration parameter: ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 country=GB network={ ssid=\"My_Home_Network\" psk=\"s3cret_pwd\" bssid=01:23:45:67:89:ab } After setting this new configuration parameter, reboot the probe to ensure it is used by the wireless interface of the probe.","title":"Configuring the Probe to Test Against Only One Access Point"},{"location":"adv_proxy/","text":"Proxy Server If you need to deal with using a proxy on your network, please supply the details of your proxy by completing the following section in your /etc/wiperf/config.ini file: ; If proxy server access is required to run a speedtest, enter the proxy server details here for https & https ; e.g. https_proxy: http://10.1.1.1:8080 ; ; For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) ; e.g. no_proxy: \"mail.local, intranet.local\" http_proxy: https_proxy: no_proxy:","title":"Proxy Server"},{"location":"adv_proxy/#proxy-server","text":"If you need to deal with using a proxy on your network, please supply the details of your proxy by completing the following section in your /etc/wiperf/config.ini file: ; If proxy server access is required to run a speedtest, enter the proxy server details here for https & https ; e.g. https_proxy: http://10.1.1.1:8080 ; ; For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) ; e.g. no_proxy: \"mail.local, intranet.local\" http_proxy: https_proxy: no_proxy:","title":"Proxy Server"},{"location":"adv_remote_cfg/","text":"Remote Configuration Server In wiperf V2, we have added a rudimentary remote configuration server feature to allow the probe's config.ini file to be changed remotely. The feature relies on having a private repository in GutHub to store the remote configuration file(s). To help understand how this can work for you, and to understand the limitations of the solution, here is an overview of the process: A private GitHub repo must be created on GitHub - it must be private, otherwise the whole world can read your config files....which is not a good thing (See this doc for details on creating a private repo). An authorization token for the GitHub repo must be created to allow the probe to access it and read its config file. See this guide to find out how to create a personal access token. Note that the token must have the full repo section enabled to allow the the repo config files to be pulled (yeah, wouldn't you think there'd be some type of read-only attribute...). No other scopes should be enabled. Also, be aware that this token can access all of your private repos (which may be a issue if you have multiple private repos) Each time a test cycle starts (i.e. every 5 mins), wiperf will check its local configuration file config.ini to see if a remote repository is configured If a remote repo is configured, then the wiperf process will check to see if it is time to check its remote config file - it doesn't check every poll cycle, to keep the network traffic overhead low. The interval is configured via the cfg_refresh_interval field in config.ini If it's time to check the config file, wiperf will pull the config file from the private GutHub repo (using its access token) and overwrite its local config file with its newly retrieved file. This will be used for the next test cycle. ( Note : The remote config file name can be anything you choose, it will get re-written as the local config.ini file on the probe once downloaded) The section of the config.ini file that controls remote repo usage is shown below (with sample input): ; central configuration server details cfg_url: https://raw.githubusercontent.com/<your_username>/<repo_name_removed>/master/probe1.ini cfg_username: cfg_password: cfg_token: 8299626c2729a034b5f7b8a1e6294951da544b47 ; how often shall we pull the config (in secs) cfg_refresh_interval: 1800 See the following reference guide for an explanation of each field: config.ini reference guide Note : Although the username and password fields are provided to allow GitHub login credentials to be used, this is generally a bad idea - go with the personal access token. Please consider all of the security implications of storing any type of credentials in local or centrally stored files Note : This is an advanced configuration option that requires thorough testing before deploying your probe. Mis-configuration of your remote config file can cause significant operational issues. Note : Don't forget, your probe config.ini file will be over-written each time the remote configuration file is pulled down. Therefore, the file pulled must contain the required access credentials to access the remote config file (i.e. you must complete the central configuration server details section of the remote config file)","title":"Remote Configuration Server"},{"location":"adv_remote_cfg/#remote-configuration-server","text":"In wiperf V2, we have added a rudimentary remote configuration server feature to allow the probe's config.ini file to be changed remotely. The feature relies on having a private repository in GutHub to store the remote configuration file(s). To help understand how this can work for you, and to understand the limitations of the solution, here is an overview of the process: A private GitHub repo must be created on GitHub - it must be private, otherwise the whole world can read your config files....which is not a good thing (See this doc for details on creating a private repo). An authorization token for the GitHub repo must be created to allow the probe to access it and read its config file. See this guide to find out how to create a personal access token. Note that the token must have the full repo section enabled to allow the the repo config files to be pulled (yeah, wouldn't you think there'd be some type of read-only attribute...). No other scopes should be enabled. Also, be aware that this token can access all of your private repos (which may be a issue if you have multiple private repos) Each time a test cycle starts (i.e. every 5 mins), wiperf will check its local configuration file config.ini to see if a remote repository is configured If a remote repo is configured, then the wiperf process will check to see if it is time to check its remote config file - it doesn't check every poll cycle, to keep the network traffic overhead low. The interval is configured via the cfg_refresh_interval field in config.ini If it's time to check the config file, wiperf will pull the config file from the private GutHub repo (using its access token) and overwrite its local config file with its newly retrieved file. This will be used for the next test cycle. ( Note : The remote config file name can be anything you choose, it will get re-written as the local config.ini file on the probe once downloaded) The section of the config.ini file that controls remote repo usage is shown below (with sample input): ; central configuration server details cfg_url: https://raw.githubusercontent.com/<your_username>/<repo_name_removed>/master/probe1.ini cfg_username: cfg_password: cfg_token: 8299626c2729a034b5f7b8a1e6294951da544b47 ; how often shall we pull the config (in secs) cfg_refresh_interval: 1800 See the following reference guide for an explanation of each field: config.ini reference guide Note : Although the username and password fields are provided to allow GitHub login credentials to be used, this is generally a bad idea - go with the personal access token. Please consider all of the security implications of storing any type of credentials in local or centrally stored files Note : This is an advanced configuration option that requires thorough testing before deploying your probe. Mis-configuration of your remote config file can cause significant operational issues. Note : Don't forget, your probe config.ini file will be over-written each time the remote configuration file is pulled down. Therefore, the file pulled must contain the required access credentials to access the remote config file (i.e. you must complete the central configuration server details section of the remote config file)","title":"Remote Configuration Server"},{"location":"adv_rpi_standalone/","text":"RPi Standalone Probe With Reporting Overview The wiperf project is primarily concerned with the functionality of the probe function that performs network tests to give an indication of the user network experience. It provides data to either Splunk or InfluxDB/Grafana that then provide the reporting function. It is generally recommended that the probe be deployed with a centrally located, remote, reporting server. However, it may be useful to run the probe as a standalone device with the reporting features also installed on the probe. This cannot be done with Splunk, as it cannot run its server software on the RPi. Testing has shown that InfluxDB and Grafana can be successfully installed on to an RPi so that the probe and reporting features can co-exist on one device. This has been tested with an RPi 3B+, but will likely be fine with all subsequent models of RPi. It is not recommended to install Grafana and InfluxDB on to an RPi to act as a centralised server for multiple probes, but providing reporting for a local probe on the same RPi seems to work OK based on initial testing. The only caveat is that I would recommend setting a data retention policy as outlined below to ensure the InfluxDB does not consume too much space on the RPi as data is gathered over time. Note: The steps outlined below are notes taken from initial testing. This is an advanced level topic that requires that you are familiar with Linux commands and the operation of wiperf. Apologies that there is very concise information provided, but this really is an advanced level topic that I cannot generally provide support for. What You'll Need In this article, I'll run through the build process to get all all-in-one wiperf probe built (wiperf + InfluxDB + Grafana on the same RPi). To perform this build, you'll need: A Raspberry Pi (3B+ or 4 is probably the best choice as they have internal wireless NIC) A 16Gb or better Micro SD card A power source/adapter for the Rpi A USB to micro-SD adapter to burn the image (for example: an adapter like this A copy of balenaEtcher (free) to burn the image on to your SD card You'll need to burn a fresh image by downloading a copy of the latest Raspberry Pi OS . I'd advise using the 'Lite' version of the image, as this is a simple, headless probe device (and that is what I have tested). Use balenaEtcher to burn the image on to your micro-SD card. Put it in to the SD slot of the RPi and boot it. You can find more extensive instructions here about building and accessing the RPi to use as a wiperf probe - the focus of this document is the all-in-one software build. Setup Procedure All of the steps below are performed from the RPi CLI (this guide assumes you are running Raspbian Buster and that you have remote access to the RPI, with all networking configured and ready to go (if not, see here )): 1.Update RPi packages and reboot (unless already done as part of the probe build process). sudo apt update sudo apt upgrade sudo reboot 2.Install wiperf: curl -s https://raw.githubusercontent.com/wifinigel/wiperf/main/setup.sh | sudo bash -s install rpi 3.Add a cron job as show below: line=\"0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1\" USERNAME=root (sudo crontab -u $USERNAME -l; echo \"$line\" ) | sudo crontab -u $USERNAME - 4.Add the required InfluxDB key & repo details: sudo wget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add - echo \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list sudo apt update sudo apt install influxdb 5.Once installed, start up InfluxDB & set InfluxDB to start-up on boot sudo systemctl unmask influxdb sudo systemctl enable influxdb sudo systemctl start influxdb 6.Add pre-requisite packages for Grafana, get & install the Grafana package: sudo apt-get install -y adduser libfontconfig1 wget https://dl.grafana.com/oss/release/grafana-rpi_6.7.4_armhf.deb sudo dpkg -i grafana-rpi_6.7.4_armhf.deb 7.Once Grafana installed, set it to startup on boot & start the Grafana process: sudo systemctl enable grafana-server sudo systemctl start grafana-server 8.On the RPI CLI, prepare Influx DB for data using Influx CLI client (you may want to set your own credentials here!): influx create database wiperf create retention policy \"wiperf_30_days\" on \"wiperf\" duration 30d replication 1 CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES exit 9.Edit InfluxDB to use login authentication & restart its processes to activate the change: sudo nano /etc/influxdb/influxdb.conf (uncomment \"# auth-enabled = false\" -> \"auth-enabled = true\") sudo systemctl restart influxdb 10.On RPI CLI, drop in to the Influx CLI client again and create some credentials for the probe login (you may want to set your own credentials here!) influx -username admin -password letmein CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' GRANT read ON \"wiperf\" TO \"grafana\" exit 11.Edit the wiperf configuration file to use the loopback interface as the management interface and set the InfluxDB details (use the probe credentials you created for InfluxDB) sudo nano /etc/wiperf/config.ini mgt_if: lo exporter_type: influxdb influx_host: 127.0.0.1 influx_port: 8086 influx_username: wiperf_probe influx_password: s3cr3tpwd99 influx_database: wiperf 12.Grafana GUI (these steps are completed using a browser): Browser: http:<ip>:3000 (login admin/admin, changed on first login) In web GUI, add datasource: Configuration > Datasources - Type: InfluxDB - Name: WiperfDB - URL: htttp://127.0.0.1:8086 - Access: Server - Database: wiperf - User: grafana - Password: R34dth3DB - HTTP Method: GET 13.Obtain the Grafana dashboard files from the wiperf dashboards folder and place on your browser machine (/usr/share/wiperf/dashboards) Using the web GUI, import dashboards using these menu options: \"+\" > Create > Import > Upload .json file > select dashboard file Select folder (General by default) Select WiperfDB Repeat for each dashboard required. At this point, you should be good to go! Verify the status of polling using the usual wiperf log file (runs every 5 minutes): tail -f /var/log/wiperf_agent.log","title":"RPi Standalone Probe With Reporting"},{"location":"adv_rpi_standalone/#rpi-standalone-probe-with-reporting","text":"","title":"RPi Standalone Probe With Reporting"},{"location":"adv_rpi_standalone/#overview","text":"The wiperf project is primarily concerned with the functionality of the probe function that performs network tests to give an indication of the user network experience. It provides data to either Splunk or InfluxDB/Grafana that then provide the reporting function. It is generally recommended that the probe be deployed with a centrally located, remote, reporting server. However, it may be useful to run the probe as a standalone device with the reporting features also installed on the probe. This cannot be done with Splunk, as it cannot run its server software on the RPi. Testing has shown that InfluxDB and Grafana can be successfully installed on to an RPi so that the probe and reporting features can co-exist on one device. This has been tested with an RPi 3B+, but will likely be fine with all subsequent models of RPi. It is not recommended to install Grafana and InfluxDB on to an RPi to act as a centralised server for multiple probes, but providing reporting for a local probe on the same RPi seems to work OK based on initial testing. The only caveat is that I would recommend setting a data retention policy as outlined below to ensure the InfluxDB does not consume too much space on the RPi as data is gathered over time. Note: The steps outlined below are notes taken from initial testing. This is an advanced level topic that requires that you are familiar with Linux commands and the operation of wiperf. Apologies that there is very concise information provided, but this really is an advanced level topic that I cannot generally provide support for.","title":"Overview"},{"location":"adv_rpi_standalone/#what-youll-need","text":"In this article, I'll run through the build process to get all all-in-one wiperf probe built (wiperf + InfluxDB + Grafana on the same RPi). To perform this build, you'll need: A Raspberry Pi (3B+ or 4 is probably the best choice as they have internal wireless NIC) A 16Gb or better Micro SD card A power source/adapter for the Rpi A USB to micro-SD adapter to burn the image (for example: an adapter like this A copy of balenaEtcher (free) to burn the image on to your SD card You'll need to burn a fresh image by downloading a copy of the latest Raspberry Pi OS . I'd advise using the 'Lite' version of the image, as this is a simple, headless probe device (and that is what I have tested). Use balenaEtcher to burn the image on to your micro-SD card. Put it in to the SD slot of the RPi and boot it. You can find more extensive instructions here about building and accessing the RPi to use as a wiperf probe - the focus of this document is the all-in-one software build.","title":"What You'll Need"},{"location":"adv_rpi_standalone/#setup-procedure","text":"All of the steps below are performed from the RPi CLI (this guide assumes you are running Raspbian Buster and that you have remote access to the RPI, with all networking configured and ready to go (if not, see here )): 1.Update RPi packages and reboot (unless already done as part of the probe build process). sudo apt update sudo apt upgrade sudo reboot 2.Install wiperf: curl -s https://raw.githubusercontent.com/wifinigel/wiperf/main/setup.sh | sudo bash -s install rpi 3.Add a cron job as show below: line=\"0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1\" USERNAME=root (sudo crontab -u $USERNAME -l; echo \"$line\" ) | sudo crontab -u $USERNAME - 4.Add the required InfluxDB key & repo details: sudo wget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add - echo \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list sudo apt update sudo apt install influxdb 5.Once installed, start up InfluxDB & set InfluxDB to start-up on boot sudo systemctl unmask influxdb sudo systemctl enable influxdb sudo systemctl start influxdb 6.Add pre-requisite packages for Grafana, get & install the Grafana package: sudo apt-get install -y adduser libfontconfig1 wget https://dl.grafana.com/oss/release/grafana-rpi_6.7.4_armhf.deb sudo dpkg -i grafana-rpi_6.7.4_armhf.deb 7.Once Grafana installed, set it to startup on boot & start the Grafana process: sudo systemctl enable grafana-server sudo systemctl start grafana-server 8.On the RPI CLI, prepare Influx DB for data using Influx CLI client (you may want to set your own credentials here!): influx create database wiperf create retention policy \"wiperf_30_days\" on \"wiperf\" duration 30d replication 1 CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES exit 9.Edit InfluxDB to use login authentication & restart its processes to activate the change: sudo nano /etc/influxdb/influxdb.conf (uncomment \"# auth-enabled = false\" -> \"auth-enabled = true\") sudo systemctl restart influxdb 10.On RPI CLI, drop in to the Influx CLI client again and create some credentials for the probe login (you may want to set your own credentials here!) influx -username admin -password letmein CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' GRANT read ON \"wiperf\" TO \"grafana\" exit 11.Edit the wiperf configuration file to use the loopback interface as the management interface and set the InfluxDB details (use the probe credentials you created for InfluxDB) sudo nano /etc/wiperf/config.ini mgt_if: lo exporter_type: influxdb influx_host: 127.0.0.1 influx_port: 8086 influx_username: wiperf_probe influx_password: s3cr3tpwd99 influx_database: wiperf 12.Grafana GUI (these steps are completed using a browser): Browser: http:<ip>:3000 (login admin/admin, changed on first login) In web GUI, add datasource: Configuration > Datasources - Type: InfluxDB - Name: WiperfDB - URL: htttp://127.0.0.1:8086 - Access: Server - Database: wiperf - User: grafana - Password: R34dth3DB - HTTP Method: GET 13.Obtain the Grafana dashboard files from the wiperf dashboards folder and place on your browser machine (/usr/share/wiperf/dashboards) Using the web GUI, import dashboards using these menu options: \"+\" > Create > Import > Upload .json file > select dashboard file Select folder (General by default) Select WiperfDB Repeat for each dashboard required. At this point, you should be good to go! Verify the status of polling using the usual wiperf log file (runs every 5 minutes): tail -f /var/log/wiperf_agent.log","title":"Setup Procedure"},{"location":"adv_secure/","text":"Security Hardening WLAN Pi Wiperf employs the following security mechanisms in an attempt to harden the WLAN Pi when deployed in wiperf mode: No forwarding is allowed between interfaces The internal UFW firewall is configured to only allow incoming connectivity on port 22 on the wlan0 & eth0 interfaces RPi If you'd like to harden the RPi when deployed in a network, a quick solution is to install & activate the 'ufw' firewall. This can be configured to stop all incoming connections except those on SSH, which will still allow remote administration. All outgoing traffic from the probe (i.e. network tests and management traffic) will flow as normal. Install ufw apt-get update apt-get install ufw Add Firewall Rules ufw allow in on eth0 to any port ssh ufw deny in on eth0 ufw allow in on wlan0 to any port ssh ufw deny in on wlan0 Activate Firewall ufw enable Useful Commands # Disable the firewall comletely ufw enable # List fw rules with numbers ufw status numbered # See firewall status ufw status","title":"Security Hardening"},{"location":"adv_secure/#security-hardening","text":"","title":"Security Hardening"},{"location":"adv_secure/#wlan-pi","text":"Wiperf employs the following security mechanisms in an attempt to harden the WLAN Pi when deployed in wiperf mode: No forwarding is allowed between interfaces The internal UFW firewall is configured to only allow incoming connectivity on port 22 on the wlan0 & eth0 interfaces","title":"WLAN Pi"},{"location":"adv_secure/#rpi","text":"If you'd like to harden the RPi when deployed in a network, a quick solution is to install & activate the 'ufw' firewall. This can be configured to stop all incoming connections except those on SSH, which will still allow remote administration. All outgoing traffic from the probe (i.e. network tests and management traffic) will flow as normal.","title":"RPi"},{"location":"adv_secure/#install-ufw","text":"apt-get update apt-get install ufw","title":"Install ufw"},{"location":"adv_secure/#add-firewall-rules","text":"ufw allow in on eth0 to any port ssh ufw deny in on eth0 ufw allow in on wlan0 to any port ssh ufw deny in on wlan0","title":"Add Firewall Rules"},{"location":"adv_secure/#activate-firewall","text":"ufw enable","title":"Activate Firewall"},{"location":"adv_secure/#useful-commands","text":"# Disable the firewall comletely ufw enable # List fw rules with numbers ufw status numbered # See firewall status ufw status","title":"Useful Commands"},{"location":"blog_neo3/","text":"Using the Neo3 as a wiperf Probe 28th January 2021 - Author: Nigel Bowden Although I tend to recommend running wiperf on either a WLAN Pi or A Raspberry Pi, there is no reason you can't try running it up on other Linux-based small board computers. One of my favourite SBCs is the Neo3 from Friendly Elec . It's a very compact, high performance unit. The only downside it has is that it does not have a wireless NIC if you wish to test over wireless. To fix this, you can use a Comfast CF-912AC USB adapter, which will give you a reasonably good 802.11ac, 2 stream connection. In this article, I'll run through the basic build process to get you going with wiperf using a Neo3 kit. To perform this build, you'll need: NanoPi Neo3-LTS (Select combo with case when purchasing) A 16Gb or better Micro SD card A CF-912AC USB wireless NIC (checkout eBay & Aliexpress for other sources) Some kind of power source. The Neo3 needs a Type-C power connection (I use a phone charger with a USB-A to Type-C cable - the power requirements are quite low) A USB to micro-SD adapter to burn the image (for example: an adapter like this A copy of balenaEtcher (free) to burn the image on to your SD card Here are the steps I followed to build wiperf on Neo3: Go to the image download page for the Neo3 on the Armbian site: https://www.armbian.com/nanopineo3/ Select the \"Armbian Buster\" direct download link. Put the micro-SD card in to your SD to USB adapter and plug it in to your computer, ready to burn an image. Burn the image on to the micro-SD card using balenaEtcher (https://www.balena.io/etcher/){target=_blank}. This takes a few minutes to complete. Once the image is burned on to the micro-SD card, put the micro-SD card in to the slot on the back of the Neo3, plug in your wireless USB NIC and power up the Neo3. Plug the Neo3 in to a switch Ethernet port so that it can get an IP address (You will likely need to look on your DHCP server or switch MAC/ARP table to figure out the IP address assigned(). The default hostname you may see listed on your DHCP server client list is nanopineo3 . The Ethernet port used will also need Internet access to allow package downloads. Once you have the IP address of the Neo3, SSH to it and login with the following credentials: username: root password: 1234 During this initial login, you will be prompted to provide: a. New root password to replace the default (don't forget this once you set it!) b. Configure your locale c. Choose a default shell (go with bash) c. Create an \"every day\" user account that should be used to administer the probe. Suggested settings: username: wiperf password: [select your own] real name: wiperf user Drop the SSH session and establish a new session using the new \"every day\" account username (wiperf). The NetworkManager package can be very problematic for networking, so we need to disable it. Follow these steps to disable it and create a static configuration file for eth0: a. Create a static config file to ensure eth0 gets an IP address from DHCP (you will be prompted to enter your everyday user password again) sudo sh -c \"printf '\\n\\nallow-hotplug eth0 \\nauto eth0\\niface eth0 inet dhcp\\n' >> /etc/network/interfaces\" b. Disable NetworkManager Disable the NetworkManager process and prevent it from staring at boot-up: sudo systemctl stop NetworkManager sudo systemctl disable NetworkManager c. By default, traditional interface names are not used for wireless NICs....lets fix that. Add this command: # Comment: add update to use traditional interface names (e.g. wlan0) sudo sh -c \"echo 'extraargs=net.ifnames=0' >> /boot/armbianEnv.txt\" d. Reboot: sudo reboot Log back in to the Neo3 over an SSH session once it has come back up (may take a couple of minutes) Perform an update of all packages before adding any new software, followed by a reboot of the probe by entering these commands: # Comment: the update may take a few minutes to complete...go get a coffee sudo apt-get update && sudo apt-get -y upgrade sudo sync; sudo reboot Log back in to the Neo3 over an SSH session once it has come back up (may take a couple of minutes) Add a couple of package that we need to install python modules (these may already exist): sudo apt install python3-pip sudo pip3 install setuptools wheel At this stage, the Neo3 is prepared ready for the installation of required packages and scripts. To complete the installation and configuration of wiperf, follow the instructions provided for the Raspberry Pi installation starting on this page: Raspberry Pi Hostname Configuration (Note that the default hostname for the Neo3 is nanopineo3 , rather than raspberrypi that is specified in the RPi instructions) Once you have completed the wiperf installation, you may use it in the same manner as the instructions provided for the Raspberry Pi on the wiperf web site. Have fun!","title":"Using the Neo3 as a wiperf Probe"},{"location":"blog_neo3/#using-the-neo3-as-a-wiperf-probe","text":"28th January 2021 - Author: Nigel Bowden Although I tend to recommend running wiperf on either a WLAN Pi or A Raspberry Pi, there is no reason you can't try running it up on other Linux-based small board computers. One of my favourite SBCs is the Neo3 from Friendly Elec . It's a very compact, high performance unit. The only downside it has is that it does not have a wireless NIC if you wish to test over wireless. To fix this, you can use a Comfast CF-912AC USB adapter, which will give you a reasonably good 802.11ac, 2 stream connection. In this article, I'll run through the basic build process to get you going with wiperf using a Neo3 kit. To perform this build, you'll need: NanoPi Neo3-LTS (Select combo with case when purchasing) A 16Gb or better Micro SD card A CF-912AC USB wireless NIC (checkout eBay & Aliexpress for other sources) Some kind of power source. The Neo3 needs a Type-C power connection (I use a phone charger with a USB-A to Type-C cable - the power requirements are quite low) A USB to micro-SD adapter to burn the image (for example: an adapter like this A copy of balenaEtcher (free) to burn the image on to your SD card Here are the steps I followed to build wiperf on Neo3: Go to the image download page for the Neo3 on the Armbian site: https://www.armbian.com/nanopineo3/ Select the \"Armbian Buster\" direct download link. Put the micro-SD card in to your SD to USB adapter and plug it in to your computer, ready to burn an image. Burn the image on to the micro-SD card using balenaEtcher (https://www.balena.io/etcher/){target=_blank}. This takes a few minutes to complete. Once the image is burned on to the micro-SD card, put the micro-SD card in to the slot on the back of the Neo3, plug in your wireless USB NIC and power up the Neo3. Plug the Neo3 in to a switch Ethernet port so that it can get an IP address (You will likely need to look on your DHCP server or switch MAC/ARP table to figure out the IP address assigned(). The default hostname you may see listed on your DHCP server client list is nanopineo3 . The Ethernet port used will also need Internet access to allow package downloads. Once you have the IP address of the Neo3, SSH to it and login with the following credentials: username: root password: 1234 During this initial login, you will be prompted to provide: a. New root password to replace the default (don't forget this once you set it!) b. Configure your locale c. Choose a default shell (go with bash) c. Create an \"every day\" user account that should be used to administer the probe. Suggested settings: username: wiperf password: [select your own] real name: wiperf user Drop the SSH session and establish a new session using the new \"every day\" account username (wiperf). The NetworkManager package can be very problematic for networking, so we need to disable it. Follow these steps to disable it and create a static configuration file for eth0: a. Create a static config file to ensure eth0 gets an IP address from DHCP (you will be prompted to enter your everyday user password again) sudo sh -c \"printf '\\n\\nallow-hotplug eth0 \\nauto eth0\\niface eth0 inet dhcp\\n' >> /etc/network/interfaces\" b. Disable NetworkManager Disable the NetworkManager process and prevent it from staring at boot-up: sudo systemctl stop NetworkManager sudo systemctl disable NetworkManager c. By default, traditional interface names are not used for wireless NICs....lets fix that. Add this command: # Comment: add update to use traditional interface names (e.g. wlan0) sudo sh -c \"echo 'extraargs=net.ifnames=0' >> /boot/armbianEnv.txt\" d. Reboot: sudo reboot Log back in to the Neo3 over an SSH session once it has come back up (may take a couple of minutes) Perform an update of all packages before adding any new software, followed by a reboot of the probe by entering these commands: # Comment: the update may take a few minutes to complete...go get a coffee sudo apt-get update && sudo apt-get -y upgrade sudo sync; sudo reboot Log back in to the Neo3 over an SSH session once it has come back up (may take a couple of minutes) Add a couple of package that we need to install python modules (these may already exist): sudo apt install python3-pip sudo pip3 install setuptools wheel At this stage, the Neo3 is prepared ready for the installation of required packages and scripts. To complete the installation and configuration of wiperf, follow the instructions provided for the Raspberry Pi installation starting on this page: Raspberry Pi Hostname Configuration (Note that the default hostname for the Neo3 is nanopineo3 , rather than raspberrypi that is specified in the RPi instructions) Once you have completed the wiperf installation, you may use it in the same manner as the instructions provided for the Raspberry Pi on the wiperf web site. Have fun!","title":"Using the Neo3 as a wiperf Probe"},{"location":"config.ini/","text":"config.ini Reference Guide Background The config.ini file controls the operation of the wiperf utility. It has many options available for maximum flexibility, but some may need some clarification. Many options will be fine using the defaults that are supplied with the installed package. However, some will definitely require configuration as they may require values such as IP addresses and port numbers which will vary in each instance where wiperf is used. The config.ini file is located in the directory : /etc/wiperf Note that an initial sample configuration file is supplied which is named: config.default.ini . This file should be copied to config.ini and this new file customised with the settings required for your environment. (Note: the config.default.ini file is not used by wiperf) The file is organised in a number of sections that relate to different areas of operation. Each section begins with a name enclosed is square brackets, like this: [Speedtest] Within each section are a number of configurable parameters that are in the format: parameter: value You may also see some lines that begin with a semi-colon. These are comments and have no effect on the operation of wiperf. You may add, remove or change these as you wish. Here is an example comment: ; wlan interface name set this as per the output of an iwconfig command (usually wlan0) Parameter Reference Guide We'll take a look at each section of the config file and provide some guidance on suitable parameter values: General Section probe_mode eth_if wlan_if mgt_if platform exporter_type results_spool_enabled results_spool_max_age error_messages_enabled error_messages_limit poller_reporting_enabled splunk_host splunk_port splunk_token influx_host influx_port influx_username influx_password influx_database influx2_host influx2_port influx2_token influx2_bucket influx2_org cache_enabled cache_data_format cache_retention_period cache_filter test_interval test_offset connectivity_lookup location debug cfg_url cfg_username cfg_password cfg_token cfg_refresh_interval unit_bouncer Network_Test Section network_data_file Speetest Section enabled provider server_id librespeed_args http_proxy https_proxy no_proxy speedtest_data_file Ping_Test Section enabled ping_targets_count ping_host1 ping_host2 ping_hostN ping_count ping_timeout ping_interval ping_data_file Iperf3_tcp_test Section enabled server_hostname port duration iperf3_tcp_data_file Iperf3_udp_test Section enabled server_hostname port duration bandwidth iperf3_udp_data_file DNS_test Section enabled dns_targets_count dns_target1 dns_target2 dns_targetN dns_data_file HTTP_test Section enabled http_targets_count http_target1 http_target2 http_targetN http_data_file DHCP_test Section enabled mode dhcp_data_file SMB_test Section enabled smb_targets_count smb_global_username smb_global_password smb_hostN smb_usernameN smb_passwordN smb_pathN smb_filenameN smb_data_file [General] Section Note: any changes to this section on the WLAN Pi should only be made when it is running in classic mode (not while in wiperf mode). probe_mode The probe may be run in one of two modes: wireless ethernet In 'wireless' mode, all tests are run over the Wi-Fi NIC interface to test wireless connectivity & performance. In 'ethernet' mode, all tests are performed over the wired, ethernet interface. Default setting: probe_mode: wireless top eth_if This parameter contains the name of the ethernet interface on the probe. This will almost always be 'eth0', but is provided in case of new use-cases in the future. You can see the Ethernet interface name by running the 'ifconfig' command from the CLI of the probe. Default setting: eth_if: eth0 top wlan_if This parameter contains the name of the WLAN interface on the probe. This will almost always be 'wlan0', but is provided in case of new use-cases in the future. You can see the WLAN interface name by running the 'ifconfig' command from the CLI of the probe. Default setting: wlan_if: wlan0 top mgt_if When performance tests have been completed, the results data needs to be sent to a reporting server (e.g. Splunk/InfluxDb). This parameter configures the interface over which this management traffic needs to be sent. Getting this parameter correct for your environment is very important to ensure that test results data makes it back to your reporting server. The available options are: wlan0 (the first available WLAN port - usually a USB dongle plugged in to the WLAN Pi, or the internal wireless NIC on the RPi) eth0 (the internal Ethernet port of the probe) ztxxxxxxxx (Zerotier (the virtual network service) is installed and used to connect to the reporting server - note that the 'xxxxxxx' string needs to be replaced with the actual detail of your Zerotier interface, which will vary on each probe) lo (Local loopback interface 0 - this may be used on an RPi when running Influx and Grafana on the probe...yes, it can be done, but this isn't the intended way of running wiperf...your mileage may vary.) The WLANPi is configured to assign a higher cost default route to eth0 by default so that all traffic (tests & test results) will choose the default route provided by wlan0. If eth0 is used as the path to return test results to the reporting server, then a static route is injected in to the probe route table on start-up to ensure correct routing. If this parameter is not correctly set, then results data may not make it back to the reporting server. Default setting: mgt_if: wlan0 top platform (Deprecated) (This setting is now deprecated (and unused) - it has been included for historical reference) Wiperf is supported on both the WLAN Pi and Raspberry Pi platforms. The available options are: wlanpi rpi Default setting: platform: wlanpi top exporter_type Wiperf supports a number of remote data repositories that can be used as targets to store test result data. The available options are: splunk influxdb influxdb2 Default setting: exporter_type: splunk top results_spool_enabled Note New for V2.1 If the mgt platform becomes unavailable, results may be spooled to a local directory for later upload when connectivity is restored.This is enabled by default, but may be disabled for purposes of management traffic bandwidth reduction if required. Data files are spooled in to the directory: /etc/spool/wiperf . Options: yes or no. If set to no, no results are save for later transmission to the management server. Default setting: results_spool_enabled: yes top results_spool_max_age Note New for V2.1 If results spooling is enabled, it may be desirable to set the duration for which files are retained (e.g. to avoid running out of storage space). This parameter defines the amount of time (in minutes) that data files are retained. By default, this parameter is set to 60 minutes. This means that if communication to the management server is lost for more than 60 minutes, locally stored data files older than 60 minutes are deleted. Default setting: results_spool_max_age: 60 top error_messages_enabled Note New for V2.1 Errors experienced by the poller are reported back as data to the management platform by default. This allows visibility of failing tests and may provide useful diagnostics information. If this data is not required (e.g. to preserve bandwitdh), then the export of these message to the management server may be disabled. Options: yes or no. If set to no, no poller error data is exported to the management platform. Default setting: error_messages_enabled: yes top error_messages_limit Note New for V2.1 To prevent overwhelming the management platform with error messages, this parameter set the maximum number of error messages which may be exported per poll cycle. Default setting: error_messages_limit: 5 top poller_reporting_enabled Note New for V2.1 At the end of each poll cycle, a summary of which tests passed/failed may be returned to allow reporting. This may be disabled for purposes of management traffic bandwidth reduction, if required. Options: yes or no. If set to no, no summary data is exported to the management platform. Default setting: poller_reporting_enabled: yes top splunk_host This is the hostname or IP address (ipv4 or ipv6) of the Splunk platform where test result data is sent to. If the hostname of the Splunk server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): splunk_host: top splunk_port The network port used to send updates to the Splunk server. By default this is 8088, but this may be changed within the Splunk application if an alternative port is required for your environment Default setting: splunk_port: 8088 top splunk_token Splunk will only receive HEC updates from devices that are authorised to send it data. Splunk uses tokens to decide if an update is from a valid device. To view available (or create) tokens within Splunk, view the menu option: \"Settings > Data > Data Inputs > HTTP Event Collector\" Here is example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d Default setting (none): splunk_token: top influx_host This is the hostname or IP address (ipv4 or ipv6) of the Influx (v1.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): influx_host: top influx_port The network port used to send updates to the Influx (v1.x) server. By default this is 8086, but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx_port: 8086 top influx_username The username that will be used to access the Influx (v1.x) server DB to post results data. This username must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_username: top influx_password The password that will be used to access the Influx (v1.x) server DB to post results data. This password must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_password: top influx_database The name of the database on the Influx (v1.x) server DB where wiperf will post results data. This database must have been created on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_database: top influx2_host This is the hostname or IP address of the Influx (v2.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your InfluxDb2 server in the Zerotier dashboard for your network) Default setting (none): influx2_host: top influx2_port The network port used to send updates to the Influx (v2.x) server. By default this is 443 (this assumes the cloud service is used), but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx2_port: 443 top influx2_token InfluxDB2 allows the use of authentication tokens when sending results data to the InfluxDB2 server. This provides an easier authentication methods than using a username and password. Once a token has been created on InfluxDB server, it can be used by wiperf to authenticate the results data sent to the InfluxDB2 server Default setting (none): influx2_token: top influx2_bucket Data sent to the InfluxDB2 server from wiperf is stored in a \"bucket\" in the data store. This field is used to configure the bucket to which wiperf should send it's data. Default setting (none): influx2_bucket: top influx2_org The InfluxDB2 server can be partitioned in to a number of organizations, which contain the buckets where data will be stored. Use this field to configure wiperf to send data to the correct organisation on InfluxDB2. Default setting (none): influx2_org: top cache_enabled Note New for V2.1 Results data may be cached in the local file system of the probe for later inspection or retrieval by user defined methods. By default, files are stored in local probe directory: /var/cache/wiperf. Note: This mechanism is different to the spooling feature which is purely for store and forward of data during breaks in comms to the management platform. Options: yes or no. If set to no, no test data is cached on the local probe file system Default setting: cache_enabled: no top cache_data_format Note New for V2.1 If data caching is enabled, it may be stored in CSV of JSON format. Options: csv or json Default setting: cache_data_format: csv top cache_retention_period Note New for V2.1 This is the number of days of data that will be retained local cache files before being deleted. This is to conserve local storage space. Default setting: cache_retention_period: 3 top cache_filter Note New for V2.1 This provides a data source filter so that only specific data sources are locally cached (e.g. to cache only http & ping data, specify: wiperf-http, wiperf-ping ) Default setting (none): cache_filter: top test_interval (WLAN Pi only) This is the interval (in minutes) at which we would like to run the performance tests. The recommened minimum is 5, which is also the default. (Note: if this setting is too low, scheduled tests may try to run before the previous test sequence has completed, which could cause gaps in your data) Default setting: test_interval: 5 top test_offset (WLAN Pi only) By default test run at the interval specified by the test_interval parameter, which is referenced to the to of the hours (e.g. 5 mins interval will run at 5, 10, 15, 20, 25...etc. mins past the hour). If multiple proes are running, it mau be useful to stagger their start times. By setting test_offset to a value of one, this will offset all test start times by 1 minutes (i.e. 6,11,16,21,26...etc. mins past the hour) The default value is zero which means that the default 5,10,15,20... run pattern will be used. Default setting: test_offset: 0 top connectivity_lookup At the start of each test cycle, a DNS lookup is performed to ensure that DNS is working. By default this is 'google.com' (this was 'bbc.co.uk' on older versions of wiperf). This may be set to any required hostname lookup in instances when the default site may not be available for some reason (e.g. DNS restrictions due to filtering or lack of Internet access) Default setting: connectivity_lookup: google.com top location This is a string that can be added to assist with report filtering, if required. Its default value in an empty string. It could be be used in an expression within your reports to filter units based on a location field (for instance) Default setting: location: top cfg_url If using centralized configuration file retrieval, this field specifies the full URL of the config file on the remote repo. (Note that on GitHub this is the URL of the raw file itself) If this field is not set, then centralized configuration retrieval is disabled Default setting (none): cfg_url: top cfg_username If username/pasword credentials are used to retrieve the centralized config, this field specifies the usename to be used. (Note: using an access token is a MUCH better idea...see below) Default setting (none): cfg_username: top cfg_password If username/pasword credentials are used to retrieve the centralized config, this field specifies the password to be used. (Note: using an access token is a MUCH better idea...see below) Default setting (none): cfg_password: top cfg_token If a GitHub authentication token is used to retrieve the centralized config, this field specifies the token to be used. (Note: this is used instead of a username/pwd) Check out this page to find out more about creating access tokens: https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token Default setting (none): cfg_token: top cfg_refresh_interval This field specifies how often (in seconds) the centralized config file should be retrieved . Recommended value: 900 (i.e. 15 mins) Default setting (none): cfg_refresh_interval: top debug To enable enhanced logging in the agent.log file, change this setting to \"on\" Default setting: debug: off top unit_bouncer If you need to bounce (reboot) the unit for some reason on a regular basis, this field can be used to signal to the WLAN Pi each hour at which it must reboot. The field is a comma separated string that lists the hours at which the unit must reboot (in 24-hour format). The number-format and comma separation are important to get right! Note that the reboot is not exactly on the hour, but will occur at the end of the next test cycle that it is within the hour where a reboot is required. It will only happen once per hour. Example: the following config will reboot at midnight, 04:00, 08:00, 12:00, 16:00: unit_bouncer: 00, 06, 12, 18 This parameter is commented out by default as it is obviously not something you necessarily want to switch on accidentally. Default setting: ; unit_bouncer: 00, 06, 12, 18 top [Network_Test] Section network_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for network tests in remote data repositories (e.g. Splunk, InfluxDB) Default setting: network_data_file: wiperf-network top [Speedtest] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no Speedtest is run. When enabled, a speedtest to the speedtest service is run each test cycle. Default setting: enabled: yes top provider Note New for V2.1 Starting in version 2.1 a choice of speedtest provider is available. Tests may be run against the Ookla or Librespeed speedtest services Options: ookla or librespeed ( Note : ensure that the Librespeed client is installed on your probe if using the librespeed option - it is not installed as part of the probe package - see wiperf.net for more details) Default setting: provider: ookla top server_id Note Updated for V2.1 If you wish to specify a particular Ookla speedtest server that the test needs to be run against, you can enter its ID here. This must be the (numeric) server ID of a specific Ookla server taken from : https://c.speedtest.net/speedtest-servers-static.php Note this must be the number (NOT url!) taken from the field id=\"xxxxx\". If you wish to specify a Librespeed server, enter the numeric server ID of the server listed in the available servers seen by running the Librespeed CLI command: librespeed-cli --list If no value is specified, best server is used (default) - Note : testing has shown that some versions of the Librespeed client will not successfully choose a best server - specify a server ID if you are having issues with the test not running as expected. Default setting: server_id: top librespeed_args Note New for V2.1 If you wish to pass additional args to pass to Librespeed CLI command, then add them to this configuration parameter (e.g. --local-json /etc/wipef/localserver.json --duration 20) - Note: Librespeed only Default setting (no value): librespeed_args: top http_proxy https_proxy no_proxy If proxy server access is required to run a speedtest, enter the proxy server details here for https & https e.g. https_proxy: http://10.1.1.1:8080 For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) e.g. no_proxy: \"mail.local, intranet.local\" Default settings (no value): http_proxy: https_proxy: no_proxy: top speedtest_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for Speedtests in the reporting server (e.g. Splunk/InfluxDB) Default setting: speedtest_data_file: wiperf-speedtest top [Ping_Test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no ping tests are run. When enabled, up to 5 entries will be targetted with an ICMP ping and the RRT times recorded Default setting: enabled: yes top ping_targets_count Note New for V2.2 The number of targets hosts/addresses that will be pinged. There should be a corresponding number of ping_host entries for this value Default setting: ping_targets_count: 2 top ping_host1 IP address or hostname of first ping target. No target details = no test run Default setting: ping_host1: google.com top ping_host2 IP address or hostname of second ping target. No target details = no test run Default setting: ping_host2: cisco.com top ping_hostN IP address or hostname of \"Nth\" ping target. No target details = no test run. As many of these entries as are required may be added (must match number of entries specified in ping_targets_count field). Default setting: ping_hostN: top ping_count The number of pings to send for each ping target Default setting: ping_count: 10 top ping_timeout Note New for V2.2 The timeout in seconds that is used for each ping test. Default setting: ping_count: 1 top ping_interval Note New for V2.2 The timeout in seconds that is used between each ping test Default setting: ping_count: 0.2 top ping_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for ping tests in the reporting server (e.g. Splunk.InfuxDB) Default setting: ping_data_file: wiperf-ping top [Iperf3_tcp_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no tcp iperf3 test is run. When enabled, a tcp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs) Default setting: enabled: yes top server_hostname The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: top port The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top duration The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 10 top iperf3_tcp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for tcp iperf3 tests in Splunk Default setting: iperf3_tcp_data_file: wiperf-iperf3-tcp top [Iperf3_udp_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no udp iperf3 test is run. When enabled, a udp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs), attempting to achieve a data transfer rate of bandwidth bps. Default setting: enabled: yes top server_hostname The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: top port The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top duration The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 10 top bandwidth The data rate that will be attempted for the UDP iperf3 test in bps Default setting: bandwidth: 2000000 top iperf3_udp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for udp iperf3 tests in Splunk Default setting: iperf3_udp_data_file: wiperf-iperf3-udp top [DNS_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no DNS tests are run. When enabled, DNS tests are run for each of the dns_target paramters defined in this section. Any targets that have no value entered will be ignored. Default setting: enabled: yes top dns_targets_count Note New for V2.2 The number of targets hosts that will be tested. There should be a corresponding number of dns_target entries for this value Default setting: dns_targets_count: 2 top dns_target1 Hostname of first DNS target. No target details = no test run Default setting: dns_target1: google.com top dns_target2 Hostname of second DNS target. No target details = no test run Default setting: dns_target2: cisco.com top dns_targetN Hostname of \"Nth\" DNS target. No target details = no test run As many of these entries as are required may be added (must match number of entries specified in dns_targets_count field). Default setting: dns_targetN: top dns_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DNS tests in Splunk Default setting: dns_data_file: wiperf-dns top [HTTP_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no HTTP tests are run. When enabled, HTTP tests are run for each of the http_target paramters defined in this section. Any targets that have no value entered will be ignored. Targets must include the full url of each site to be queried (including http:// or https:// element). Valid site address examples: http://bbc.co.uk https://ebay.com A http get will be performed for each target and the result code returned. Default setting: enabled: yes top http_targets_count Note New for V2.2 The number of targets hosts that will be tested. There should be a corresponding number of http_target entries for this value Default setting: http_targets_count: 2 top http_target1 Hostname of first HTTP target. No target details = no test run Default setting: http_target1: https://google.com top http_target2 Hostname of second HTTP target. No target details = no test run Default setting: http_target2: https://cisco.com top http_targetN Hostname of \"Nth\" HTTP target. No target details = no test run As many of these entries as are required may be added (must match number of entries specified in http_targets_count field). Default setting: http_targetN: top http_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for HTTP tests in Splunk Default setting: http_data_file: wiperf-http top [DHCP_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no DHCP test is run. Note that the DHCP test has 2 modes : passive: only a renewal request is sent (no release of IP) active: a release and renew request is performed. Note that the active setting has shown varying degrees of usefulness in esting. In some scenarios (e.g. when connected via ZeroTier), it has caused connectivity issues, hence the passive setting is a better choice. Obviously, the passive setting does not perform such a rigorous DHCP test and is completed much quicker than the active mode. However, it still provides a useful comparative measure of the reponsivemess of DHCP servers. Default setting: enabled: yes top mode (deprecated) Note: This setting has been removed as it caused probe connectivity issues. The probe now only operates in the passive mode. These notes have bene left in for reference for those who used older versions of code or old configuration file. This setting is silently ignored if supplied. Available options: passive active The active settings performs a full release/request and may be disruptve to connectivity - use with caution. The passive setting is the recommended option for most situations. Default setting: mode: passive top dhcp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DHCP tests in Splunk Default setting: dhcp_data_file: wiperf-dhcp top [SMB_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Note New for V2.1 Options: yes or no. If set to no, entire section is ignored and no SMB tests are run. Note that the DHCP test has 2 modes : Default setting: enabled: no top smb_targets_count Note New for V2.2 The number of SMB target hosts that will be tested. There should be a corresponding number of SMB host target entries for this value Default setting: smb_targets_count: 2 top smb_global_username smb_global_password Note New for V2.1 These are the username and password credentials to be used for all SMB tests where a test-specific username/password has not been provided. Default setting (no value): smb_global_username: smb_global_passwrod: top smb_hostN Note New for V2.1 The hostname or IP address to be used for SMB test number 'N' (1-N) Default setting: smb_hostN: top smb_usernameN smb_passwordN Note New for V2.1 The username & password to be used for SMB test number 'N' (1-N). Note the global username/password credential is use if a per-test credential is not provided. Default setting (no value): smb_usernameN: smb_passwordN: top smb_pathN Note New for V2.1 The volume path to be used for SMB test number 'N' (1-N) Default setting (no value): smb_pathN: top smb_filenameN Note New for V2.1 The filename to be used for SMB test number 'N' (1-N) Default setting (no value): smb_filenameN: top smb_data_file Note New for V2.1 (Advanced setting, do not change) The name used for SMB the file/data group/data source. Default setting (no value): smb_data_file: top","title":"config.ini Reference Guide"},{"location":"config.ini/#configini-reference-guide","text":"","title":"config.ini Reference Guide"},{"location":"config.ini/#background","text":"The config.ini file controls the operation of the wiperf utility. It has many options available for maximum flexibility, but some may need some clarification. Many options will be fine using the defaults that are supplied with the installed package. However, some will definitely require configuration as they may require values such as IP addresses and port numbers which will vary in each instance where wiperf is used. The config.ini file is located in the directory : /etc/wiperf Note that an initial sample configuration file is supplied which is named: config.default.ini . This file should be copied to config.ini and this new file customised with the settings required for your environment. (Note: the config.default.ini file is not used by wiperf) The file is organised in a number of sections that relate to different areas of operation. Each section begins with a name enclosed is square brackets, like this: [Speedtest] Within each section are a number of configurable parameters that are in the format: parameter: value You may also see some lines that begin with a semi-colon. These are comments and have no effect on the operation of wiperf. You may add, remove or change these as you wish. Here is an example comment: ; wlan interface name set this as per the output of an iwconfig command (usually wlan0)","title":"Background"},{"location":"config.ini/#parameter-reference-guide","text":"We'll take a look at each section of the config file and provide some guidance on suitable parameter values: General Section probe_mode eth_if wlan_if mgt_if platform exporter_type results_spool_enabled results_spool_max_age error_messages_enabled error_messages_limit poller_reporting_enabled splunk_host splunk_port splunk_token influx_host influx_port influx_username influx_password influx_database influx2_host influx2_port influx2_token influx2_bucket influx2_org cache_enabled cache_data_format cache_retention_period cache_filter test_interval test_offset connectivity_lookup location debug cfg_url cfg_username cfg_password cfg_token cfg_refresh_interval unit_bouncer Network_Test Section network_data_file Speetest Section enabled provider server_id librespeed_args http_proxy https_proxy no_proxy speedtest_data_file Ping_Test Section enabled ping_targets_count ping_host1 ping_host2 ping_hostN ping_count ping_timeout ping_interval ping_data_file Iperf3_tcp_test Section enabled server_hostname port duration iperf3_tcp_data_file Iperf3_udp_test Section enabled server_hostname port duration bandwidth iperf3_udp_data_file DNS_test Section enabled dns_targets_count dns_target1 dns_target2 dns_targetN dns_data_file HTTP_test Section enabled http_targets_count http_target1 http_target2 http_targetN http_data_file DHCP_test Section enabled mode dhcp_data_file SMB_test Section enabled smb_targets_count smb_global_username smb_global_password smb_hostN smb_usernameN smb_passwordN smb_pathN smb_filenameN smb_data_file","title":"Parameter Reference Guide"},{"location":"config.ini/#general-section","text":"Note: any changes to this section on the WLAN Pi should only be made when it is running in classic mode (not while in wiperf mode).","title":"[General] Section"},{"location":"config.ini/#probe_mode","text":"The probe may be run in one of two modes: wireless ethernet In 'wireless' mode, all tests are run over the Wi-Fi NIC interface to test wireless connectivity & performance. In 'ethernet' mode, all tests are performed over the wired, ethernet interface. Default setting: probe_mode: wireless top","title":"probe_mode"},{"location":"config.ini/#eth_if","text":"This parameter contains the name of the ethernet interface on the probe. This will almost always be 'eth0', but is provided in case of new use-cases in the future. You can see the Ethernet interface name by running the 'ifconfig' command from the CLI of the probe. Default setting: eth_if: eth0 top","title":"eth_if"},{"location":"config.ini/#wlan_if","text":"This parameter contains the name of the WLAN interface on the probe. This will almost always be 'wlan0', but is provided in case of new use-cases in the future. You can see the WLAN interface name by running the 'ifconfig' command from the CLI of the probe. Default setting: wlan_if: wlan0 top","title":"wlan_if"},{"location":"config.ini/#mgt_if","text":"When performance tests have been completed, the results data needs to be sent to a reporting server (e.g. Splunk/InfluxDb). This parameter configures the interface over which this management traffic needs to be sent. Getting this parameter correct for your environment is very important to ensure that test results data makes it back to your reporting server. The available options are: wlan0 (the first available WLAN port - usually a USB dongle plugged in to the WLAN Pi, or the internal wireless NIC on the RPi) eth0 (the internal Ethernet port of the probe) ztxxxxxxxx (Zerotier (the virtual network service) is installed and used to connect to the reporting server - note that the 'xxxxxxx' string needs to be replaced with the actual detail of your Zerotier interface, which will vary on each probe) lo (Local loopback interface 0 - this may be used on an RPi when running Influx and Grafana on the probe...yes, it can be done, but this isn't the intended way of running wiperf...your mileage may vary.) The WLANPi is configured to assign a higher cost default route to eth0 by default so that all traffic (tests & test results) will choose the default route provided by wlan0. If eth0 is used as the path to return test results to the reporting server, then a static route is injected in to the probe route table on start-up to ensure correct routing. If this parameter is not correctly set, then results data may not make it back to the reporting server. Default setting: mgt_if: wlan0 top","title":"mgt_if"},{"location":"config.ini/#platform-deprecated","text":"(This setting is now deprecated (and unused) - it has been included for historical reference) Wiperf is supported on both the WLAN Pi and Raspberry Pi platforms. The available options are: wlanpi rpi Default setting: platform: wlanpi top","title":"platform (Deprecated)"},{"location":"config.ini/#exporter_type","text":"Wiperf supports a number of remote data repositories that can be used as targets to store test result data. The available options are: splunk influxdb influxdb2 Default setting: exporter_type: splunk top","title":"exporter_type"},{"location":"config.ini/#results_spool_enabled","text":"Note New for V2.1 If the mgt platform becomes unavailable, results may be spooled to a local directory for later upload when connectivity is restored.This is enabled by default, but may be disabled for purposes of management traffic bandwidth reduction if required. Data files are spooled in to the directory: /etc/spool/wiperf . Options: yes or no. If set to no, no results are save for later transmission to the management server. Default setting: results_spool_enabled: yes top","title":"results_spool_enabled"},{"location":"config.ini/#results_spool_max_age","text":"Note New for V2.1 If results spooling is enabled, it may be desirable to set the duration for which files are retained (e.g. to avoid running out of storage space). This parameter defines the amount of time (in minutes) that data files are retained. By default, this parameter is set to 60 minutes. This means that if communication to the management server is lost for more than 60 minutes, locally stored data files older than 60 minutes are deleted. Default setting: results_spool_max_age: 60 top","title":"results_spool_max_age"},{"location":"config.ini/#error_messages_enabled","text":"Note New for V2.1 Errors experienced by the poller are reported back as data to the management platform by default. This allows visibility of failing tests and may provide useful diagnostics information. If this data is not required (e.g. to preserve bandwitdh), then the export of these message to the management server may be disabled. Options: yes or no. If set to no, no poller error data is exported to the management platform. Default setting: error_messages_enabled: yes top","title":"error_messages_enabled"},{"location":"config.ini/#error_messages_limit","text":"Note New for V2.1 To prevent overwhelming the management platform with error messages, this parameter set the maximum number of error messages which may be exported per poll cycle. Default setting: error_messages_limit: 5 top","title":"error_messages_limit"},{"location":"config.ini/#poller_reporting_enabled","text":"Note New for V2.1 At the end of each poll cycle, a summary of which tests passed/failed may be returned to allow reporting. This may be disabled for purposes of management traffic bandwidth reduction, if required. Options: yes or no. If set to no, no summary data is exported to the management platform. Default setting: poller_reporting_enabled: yes top","title":"poller_reporting_enabled"},{"location":"config.ini/#splunk_host","text":"This is the hostname or IP address (ipv4 or ipv6) of the Splunk platform where test result data is sent to. If the hostname of the Splunk server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): splunk_host: top","title":"splunk_host"},{"location":"config.ini/#splunk_port","text":"The network port used to send updates to the Splunk server. By default this is 8088, but this may be changed within the Splunk application if an alternative port is required for your environment Default setting: splunk_port: 8088 top","title":"splunk_port"},{"location":"config.ini/#splunk_token","text":"Splunk will only receive HEC updates from devices that are authorised to send it data. Splunk uses tokens to decide if an update is from a valid device. To view available (or create) tokens within Splunk, view the menu option: \"Settings > Data > Data Inputs > HTTP Event Collector\" Here is example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d Default setting (none): splunk_token: top","title":"splunk_token"},{"location":"config.ini/#influx_host","text":"This is the hostname or IP address (ipv4 or ipv6) of the Influx (v1.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): influx_host: top","title":"influx_host"},{"location":"config.ini/#influx_port","text":"The network port used to send updates to the Influx (v1.x) server. By default this is 8086, but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx_port: 8086 top","title":"influx_port"},{"location":"config.ini/#influx_username","text":"The username that will be used to access the Influx (v1.x) server DB to post results data. This username must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_username: top","title":"influx_username"},{"location":"config.ini/#influx_password","text":"The password that will be used to access the Influx (v1.x) server DB to post results data. This password must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_password: top","title":"influx_password"},{"location":"config.ini/#influx_database","text":"The name of the database on the Influx (v1.x) server DB where wiperf will post results data. This database must have been created on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_database: top","title":"influx_database"},{"location":"config.ini/#influx2_host","text":"This is the hostname or IP address of the Influx (v2.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your InfluxDb2 server in the Zerotier dashboard for your network) Default setting (none): influx2_host: top","title":"influx2_host"},{"location":"config.ini/#influx2_port","text":"The network port used to send updates to the Influx (v2.x) server. By default this is 443 (this assumes the cloud service is used), but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx2_port: 443 top","title":"influx2_port"},{"location":"config.ini/#influx2_token","text":"InfluxDB2 allows the use of authentication tokens when sending results data to the InfluxDB2 server. This provides an easier authentication methods than using a username and password. Once a token has been created on InfluxDB server, it can be used by wiperf to authenticate the results data sent to the InfluxDB2 server Default setting (none): influx2_token: top","title":"influx2_token"},{"location":"config.ini/#influx2_bucket","text":"Data sent to the InfluxDB2 server from wiperf is stored in a \"bucket\" in the data store. This field is used to configure the bucket to which wiperf should send it's data. Default setting (none): influx2_bucket: top","title":"influx2_bucket"},{"location":"config.ini/#influx2_org","text":"The InfluxDB2 server can be partitioned in to a number of organizations, which contain the buckets where data will be stored. Use this field to configure wiperf to send data to the correct organisation on InfluxDB2. Default setting (none): influx2_org: top","title":"influx2_org"},{"location":"config.ini/#cache_enabled","text":"Note New for V2.1 Results data may be cached in the local file system of the probe for later inspection or retrieval by user defined methods. By default, files are stored in local probe directory: /var/cache/wiperf. Note: This mechanism is different to the spooling feature which is purely for store and forward of data during breaks in comms to the management platform. Options: yes or no. If set to no, no test data is cached on the local probe file system Default setting: cache_enabled: no top","title":"cache_enabled"},{"location":"config.ini/#cache_data_format","text":"Note New for V2.1 If data caching is enabled, it may be stored in CSV of JSON format. Options: csv or json Default setting: cache_data_format: csv top","title":"cache_data_format"},{"location":"config.ini/#cache_retention_period","text":"Note New for V2.1 This is the number of days of data that will be retained local cache files before being deleted. This is to conserve local storage space. Default setting: cache_retention_period: 3 top","title":"cache_retention_period"},{"location":"config.ini/#cache_filter","text":"Note New for V2.1 This provides a data source filter so that only specific data sources are locally cached (e.g. to cache only http & ping data, specify: wiperf-http, wiperf-ping ) Default setting (none): cache_filter: top","title":"cache_filter"},{"location":"config.ini/#test_interval","text":"(WLAN Pi only) This is the interval (in minutes) at which we would like to run the performance tests. The recommened minimum is 5, which is also the default. (Note: if this setting is too low, scheduled tests may try to run before the previous test sequence has completed, which could cause gaps in your data) Default setting: test_interval: 5 top","title":"test_interval"},{"location":"config.ini/#test_offset","text":"(WLAN Pi only) By default test run at the interval specified by the test_interval parameter, which is referenced to the to of the hours (e.g. 5 mins interval will run at 5, 10, 15, 20, 25...etc. mins past the hour). If multiple proes are running, it mau be useful to stagger their start times. By setting test_offset to a value of one, this will offset all test start times by 1 minutes (i.e. 6,11,16,21,26...etc. mins past the hour) The default value is zero which means that the default 5,10,15,20... run pattern will be used. Default setting: test_offset: 0 top","title":"test_offset"},{"location":"config.ini/#connectivity_lookup","text":"At the start of each test cycle, a DNS lookup is performed to ensure that DNS is working. By default this is 'google.com' (this was 'bbc.co.uk' on older versions of wiperf). This may be set to any required hostname lookup in instances when the default site may not be available for some reason (e.g. DNS restrictions due to filtering or lack of Internet access) Default setting: connectivity_lookup: google.com top","title":"connectivity_lookup"},{"location":"config.ini/#location","text":"This is a string that can be added to assist with report filtering, if required. Its default value in an empty string. It could be be used in an expression within your reports to filter units based on a location field (for instance) Default setting: location: top","title":"location"},{"location":"config.ini/#cfg_url","text":"If using centralized configuration file retrieval, this field specifies the full URL of the config file on the remote repo. (Note that on GitHub this is the URL of the raw file itself) If this field is not set, then centralized configuration retrieval is disabled Default setting (none): cfg_url: top","title":"cfg_url"},{"location":"config.ini/#cfg_username","text":"If username/pasword credentials are used to retrieve the centralized config, this field specifies the usename to be used. (Note: using an access token is a MUCH better idea...see below) Default setting (none): cfg_username: top","title":"cfg_username"},{"location":"config.ini/#cfg_password","text":"If username/pasword credentials are used to retrieve the centralized config, this field specifies the password to be used. (Note: using an access token is a MUCH better idea...see below) Default setting (none): cfg_password: top","title":"cfg_password"},{"location":"config.ini/#cfg_token","text":"If a GitHub authentication token is used to retrieve the centralized config, this field specifies the token to be used. (Note: this is used instead of a username/pwd) Check out this page to find out more about creating access tokens: https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token Default setting (none): cfg_token: top","title":"cfg_token"},{"location":"config.ini/#cfg_refresh_interval","text":"This field specifies how often (in seconds) the centralized config file should be retrieved . Recommended value: 900 (i.e. 15 mins) Default setting (none): cfg_refresh_interval: top","title":"cfg_refresh_interval"},{"location":"config.ini/#debug","text":"To enable enhanced logging in the agent.log file, change this setting to \"on\" Default setting: debug: off top","title":"debug"},{"location":"config.ini/#unit_bouncer","text":"If you need to bounce (reboot) the unit for some reason on a regular basis, this field can be used to signal to the WLAN Pi each hour at which it must reboot. The field is a comma separated string that lists the hours at which the unit must reboot (in 24-hour format). The number-format and comma separation are important to get right! Note that the reboot is not exactly on the hour, but will occur at the end of the next test cycle that it is within the hour where a reboot is required. It will only happen once per hour. Example: the following config will reboot at midnight, 04:00, 08:00, 12:00, 16:00: unit_bouncer: 00, 06, 12, 18 This parameter is commented out by default as it is obviously not something you necessarily want to switch on accidentally. Default setting: ; unit_bouncer: 00, 06, 12, 18 top","title":"unit_bouncer"},{"location":"config.ini/#network_test-section","text":"","title":"[Network_Test] Section"},{"location":"config.ini/#network_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for network tests in remote data repositories (e.g. Splunk, InfluxDB) Default setting: network_data_file: wiperf-network top","title":"network_data_file"},{"location":"config.ini/#speedtest-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[Speedtest] Section"},{"location":"config.ini/#enabled","text":"Options: yes or no. If set to no, entire section is ignored and no Speedtest is run. When enabled, a speedtest to the speedtest service is run each test cycle. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#provider","text":"Note New for V2.1 Starting in version 2.1 a choice of speedtest provider is available. Tests may be run against the Ookla or Librespeed speedtest services Options: ookla or librespeed ( Note : ensure that the Librespeed client is installed on your probe if using the librespeed option - it is not installed as part of the probe package - see wiperf.net for more details) Default setting: provider: ookla top","title":"provider"},{"location":"config.ini/#server_id","text":"Note Updated for V2.1 If you wish to specify a particular Ookla speedtest server that the test needs to be run against, you can enter its ID here. This must be the (numeric) server ID of a specific Ookla server taken from : https://c.speedtest.net/speedtest-servers-static.php Note this must be the number (NOT url!) taken from the field id=\"xxxxx\". If you wish to specify a Librespeed server, enter the numeric server ID of the server listed in the available servers seen by running the Librespeed CLI command: librespeed-cli --list If no value is specified, best server is used (default) - Note : testing has shown that some versions of the Librespeed client will not successfully choose a best server - specify a server ID if you are having issues with the test not running as expected. Default setting: server_id: top","title":"server_id"},{"location":"config.ini/#librespeed_args","text":"Note New for V2.1 If you wish to pass additional args to pass to Librespeed CLI command, then add them to this configuration parameter (e.g. --local-json /etc/wipef/localserver.json --duration 20) - Note: Librespeed only Default setting (no value): librespeed_args: top","title":"librespeed_args"},{"location":"config.ini/#http_proxy","text":"","title":"http_proxy"},{"location":"config.ini/#https_proxy","text":"","title":"https_proxy"},{"location":"config.ini/#no_proxy","text":"If proxy server access is required to run a speedtest, enter the proxy server details here for https & https e.g. https_proxy: http://10.1.1.1:8080 For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) e.g. no_proxy: \"mail.local, intranet.local\" Default settings (no value): http_proxy: https_proxy: no_proxy: top","title":"no_proxy"},{"location":"config.ini/#speedtest_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for Speedtests in the reporting server (e.g. Splunk/InfluxDB) Default setting: speedtest_data_file: wiperf-speedtest top","title":"speedtest_data_file"},{"location":"config.ini/#ping_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[Ping_Test] Section"},{"location":"config.ini/#enabled_1","text":"Options: yes or no. If set to no, entire section is ignored and no ping tests are run. When enabled, up to 5 entries will be targetted with an ICMP ping and the RRT times recorded Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#ping_targets_count","text":"Note New for V2.2 The number of targets hosts/addresses that will be pinged. There should be a corresponding number of ping_host entries for this value Default setting: ping_targets_count: 2 top","title":"ping_targets_count"},{"location":"config.ini/#ping_host1","text":"IP address or hostname of first ping target. No target details = no test run Default setting: ping_host1: google.com top","title":"ping_host1"},{"location":"config.ini/#ping_host2","text":"IP address or hostname of second ping target. No target details = no test run Default setting: ping_host2: cisco.com top","title":"ping_host2"},{"location":"config.ini/#ping_hostn","text":"IP address or hostname of \"Nth\" ping target. No target details = no test run. As many of these entries as are required may be added (must match number of entries specified in ping_targets_count field). Default setting: ping_hostN: top","title":"ping_hostN"},{"location":"config.ini/#ping_count","text":"The number of pings to send for each ping target Default setting: ping_count: 10 top","title":"ping_count"},{"location":"config.ini/#ping_timeout","text":"Note New for V2.2 The timeout in seconds that is used for each ping test. Default setting: ping_count: 1 top","title":"ping_timeout"},{"location":"config.ini/#ping_interval","text":"Note New for V2.2 The timeout in seconds that is used between each ping test Default setting: ping_count: 0.2 top","title":"ping_interval"},{"location":"config.ini/#ping_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for ping tests in the reporting server (e.g. Splunk.InfuxDB) Default setting: ping_data_file: wiperf-ping top","title":"ping_data_file"},{"location":"config.ini/#iperf3_tcp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[Iperf3_tcp_test] Section"},{"location":"config.ini/#enabled_2","text":"Options: yes or no. If set to no, entire section is ignored and no tcp iperf3 test is run. When enabled, a tcp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs) Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_hostname","text":"The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: top","title":"server_hostname"},{"location":"config.ini/#port","text":"The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top","title":"port"},{"location":"config.ini/#duration","text":"The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 10 top","title":"duration"},{"location":"config.ini/#iperf3_tcp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for tcp iperf3 tests in Splunk Default setting: iperf3_tcp_data_file: wiperf-iperf3-tcp top","title":"iperf3_tcp_data_file"},{"location":"config.ini/#iperf3_udp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[Iperf3_udp_test] Section"},{"location":"config.ini/#enabled_3","text":"Options: yes or no. If set to no, entire section is ignored and no udp iperf3 test is run. When enabled, a udp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs), attempting to achieve a data transfer rate of bandwidth bps. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_hostname_1","text":"The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: top","title":"server_hostname"},{"location":"config.ini/#port_1","text":"The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top","title":"port"},{"location":"config.ini/#duration_1","text":"The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 10 top","title":"duration"},{"location":"config.ini/#bandwidth","text":"The data rate that will be attempted for the UDP iperf3 test in bps Default setting: bandwidth: 2000000 top","title":"bandwidth"},{"location":"config.ini/#iperf3_udp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for udp iperf3 tests in Splunk Default setting: iperf3_udp_data_file: wiperf-iperf3-udp top","title":"iperf3_udp_data_file"},{"location":"config.ini/#dns_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[DNS_test] Section"},{"location":"config.ini/#enabled_4","text":"Options: yes or no. If set to no, entire section is ignored and no DNS tests are run. When enabled, DNS tests are run for each of the dns_target paramters defined in this section. Any targets that have no value entered will be ignored. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#dns_targets_count","text":"Note New for V2.2 The number of targets hosts that will be tested. There should be a corresponding number of dns_target entries for this value Default setting: dns_targets_count: 2 top","title":"dns_targets_count"},{"location":"config.ini/#dns_target1","text":"Hostname of first DNS target. No target details = no test run Default setting: dns_target1: google.com top","title":"dns_target1"},{"location":"config.ini/#dns_target2","text":"Hostname of second DNS target. No target details = no test run Default setting: dns_target2: cisco.com top","title":"dns_target2"},{"location":"config.ini/#dns_targetn","text":"Hostname of \"Nth\" DNS target. No target details = no test run As many of these entries as are required may be added (must match number of entries specified in dns_targets_count field). Default setting: dns_targetN: top","title":"dns_targetN"},{"location":"config.ini/#dns_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DNS tests in Splunk Default setting: dns_data_file: wiperf-dns top","title":"dns_data_file"},{"location":"config.ini/#http_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[HTTP_test] Section"},{"location":"config.ini/#enabled_5","text":"Options: yes or no. If set to no, entire section is ignored and no HTTP tests are run. When enabled, HTTP tests are run for each of the http_target paramters defined in this section. Any targets that have no value entered will be ignored. Targets must include the full url of each site to be queried (including http:// or https:// element). Valid site address examples: http://bbc.co.uk https://ebay.com A http get will be performed for each target and the result code returned. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#http_targets_count","text":"Note New for V2.2 The number of targets hosts that will be tested. There should be a corresponding number of http_target entries for this value Default setting: http_targets_count: 2 top","title":"http_targets_count"},{"location":"config.ini/#http_target1","text":"Hostname of first HTTP target. No target details = no test run Default setting: http_target1: https://google.com top","title":"http_target1"},{"location":"config.ini/#http_target2","text":"Hostname of second HTTP target. No target details = no test run Default setting: http_target2: https://cisco.com top","title":"http_target2"},{"location":"config.ini/#http_targetn","text":"Hostname of \"Nth\" HTTP target. No target details = no test run As many of these entries as are required may be added (must match number of entries specified in http_targets_count field). Default setting: http_targetN: top","title":"http_targetN"},{"location":"config.ini/#http_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for HTTP tests in Splunk Default setting: http_data_file: wiperf-http top","title":"http_data_file"},{"location":"config.ini/#dhcp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[DHCP_test] Section"},{"location":"config.ini/#enabled_6","text":"Options: yes or no. If set to no, entire section is ignored and no DHCP test is run. Note that the DHCP test has 2 modes : passive: only a renewal request is sent (no release of IP) active: a release and renew request is performed. Note that the active setting has shown varying degrees of usefulness in esting. In some scenarios (e.g. when connected via ZeroTier), it has caused connectivity issues, hence the passive setting is a better choice. Obviously, the passive setting does not perform such a rigorous DHCP test and is completed much quicker than the active mode. However, it still provides a useful comparative measure of the reponsivemess of DHCP servers. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#mode-deprecated","text":"Note: This setting has been removed as it caused probe connectivity issues. The probe now only operates in the passive mode. These notes have bene left in for reference for those who used older versions of code or old configuration file. This setting is silently ignored if supplied. Available options: passive active The active settings performs a full release/request and may be disruptve to connectivity - use with caution. The passive setting is the recommended option for most situations. Default setting: mode: passive top","title":"mode (deprecated)"},{"location":"config.ini/#dhcp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DHCP tests in Splunk Default setting: dhcp_data_file: wiperf-dhcp top","title":"dhcp_data_file"},{"location":"config.ini/#smb_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[SMB_test] Section"},{"location":"config.ini/#enabled_7","text":"Note New for V2.1 Options: yes or no. If set to no, entire section is ignored and no SMB tests are run. Note that the DHCP test has 2 modes : Default setting: enabled: no top","title":"enabled"},{"location":"config.ini/#smb_targets_count","text":"Note New for V2.2 The number of SMB target hosts that will be tested. There should be a corresponding number of SMB host target entries for this value Default setting: smb_targets_count: 2 top","title":"smb_targets_count"},{"location":"config.ini/#smb_global_username","text":"","title":"smb_global_username"},{"location":"config.ini/#smb_global_password","text":"Note New for V2.1 These are the username and password credentials to be used for all SMB tests where a test-specific username/password has not been provided. Default setting (no value): smb_global_username: smb_global_passwrod: top","title":"smb_global_password"},{"location":"config.ini/#smb_hostn","text":"Note New for V2.1 The hostname or IP address to be used for SMB test number 'N' (1-N) Default setting: smb_hostN: top","title":"smb_hostN"},{"location":"config.ini/#smb_usernamen","text":"","title":"smb_usernameN"},{"location":"config.ini/#smb_passwordn","text":"Note New for V2.1 The username & password to be used for SMB test number 'N' (1-N). Note the global username/password credential is use if a per-test credential is not provided. Default setting (no value): smb_usernameN: smb_passwordN: top","title":"smb_passwordN"},{"location":"config.ini/#smb_pathn","text":"Note New for V2.1 The volume path to be used for SMB test number 'N' (1-N) Default setting (no value): smb_pathN: top","title":"smb_pathN"},{"location":"config.ini/#smb_filenamen","text":"Note New for V2.1 The filename to be used for SMB test number 'N' (1-N) Default setting (no value): smb_filenameN: top","title":"smb_filenameN"},{"location":"config.ini/#smb_data_file","text":"Note New for V2.1 (Advanced setting, do not change) The name used for SMB the file/data group/data source. Default setting (no value): smb_data_file: top","title":"smb_data_file"},{"location":"data_points/","text":"Data Points Reference Guide Background The wiperf probe collects a variety of data points about various aspects of network connectivity and performance. It then makes those data points available to a number of databases via their standard API (e.g. Splunk, InfluxDB etc.). The data collected in all instances is the same, but the format of the data presented to each type of database varies depending on the their API and formatting rules and syntax. This document details the data points collected by the probe. These field names and the data values should be the same no matter which database is used. The probe may collect data for the following network tests, depending upon its configuration: Wireless network connectivity details Speedtest testing results data ICMP ping tests to various destinations DNS lookup tests to various destinations ] HTTP (web) tests to various destinations iperf3 TCP test to a nominated iperf3 server iperf3 UDP test to a nominated iperf3 server DHCP renewal test to test DHCP performance on network to which the WLAN Pi is connected SMB/CIFS file copy tests from various desinations Poller Errors Poll Status Information The tests are run each time the wiperf process is triggered (usually every 5 minutes from a local cron job). The tests that are run, together with test configuration parameters are configured in the config.ini file. Here are the data points that may be collected, displayed by test type: Data Points Details Wireless Network Connectivity (Data source: wiperf-network) time : Unix timestamp of time test was performed ssid : The network name of the wireless network to which the wiperf probe is currently connected bssid : The basic service set identifier (i.e. MAC address) of the radio to which the wiperf probe is currently connected freq_ghz : The centre frequency of the channel on which the probe is operating (note this may be different to the primary channel centre freq if a bonded channel is in use) center_freq_ghz : The centre frequency of the primary channel on which the probe is operating channel : The channel number on which the probe is operating channel_width: The channel width (e.g. 20MHz, 40MHz, 80MHz) of the channel on which the probe is operating tx_rate_mbps : The PHY rate at which data is being sent from the probe to the AP (note this is not a throughput rate, just a physical connection rate) rx_rate_mbps : The PHY rate at which data is being sent from the AP to the probe (note this is not a throughput rate, just a physical connection rate) tx_mcs : For HT & VHT connections, this is the the MCS value used by the probe to the AP rx_mcs : For HT & VHT connections, this is the the MCS value used by the AP to the probe signal_level_dbm : The power level of the AP radio signal as observed by the probe (in dBm) tx_retries : The number of transmitted frames that have had to be sent gain (retried) ip_address : The IP address assigned to the probe WLAN NIC Speedtest Results (Data source: wiperf-speedtest) time : Unix timestamp of time test was performed ping_time : The RTT of a ping test to the speedtest server download_rate_mbps : The throughput rate achieved when receiving data from the speedtest server in megabits per second upload_rate_mbps : The throughput rate achieved when sending data to the speedtest server in megabits per second server_name : The name of the speedtest server used for this test Ping Results (Data source: wiperf-ping) time : Unix timestamp of time test was performed ping_index : wiperf runs up to 5 instances of ping test via its configuration file. This index uniquely identifies each instance. ping_host : The IP address or hostname of the target host/site being pinged pkts_tx : The number of ping request packets sent during the ping test pkts_rx : The number of ping response packets received back during the ping test percent_loss : The percentage (%) of packets lost during the test (i.e. how many responses were received compared to requests sent) test_time_ms : How long the ping test took in total rtt_min_ms : The minimum round trip time of all ping tests to this test instance in milliseconds rtt_avg_ms : The average round trip time of all ping tests to this test instance in milliseconds rtt_max_ms : The maximum round trip time of all ping tests to this test instance in milliseconds rtt_mdev_ms : Standard deviation of all ping tests to this test instance (...no, I don't know either...but you'll look cool at dinner parties if you mention it.) DNS Results (Data source: wiperf-ping) time : Unix timestamp of time test was performed dns_index : wiperf runs up to 5 instances of DNS test via its configuration file. This index uniquely identifies each instance. dns_target : The domain name of the target host/site which is the subject of the DNS lookup test lookup_time_ms : The time taken to perform the DNS lookup in milliseconds HTTP Results (Data source: wiperf-http) time : Unix timestamp of time test was performed http_index : wiperf runs up to 5 instances of HTTP test via its configuration file. This index uniquely identifies each instance. http_target : The domain name (or IP address) of the target site which is the subject of the HTTP test http_get_time_ms : The time taken (in mS) to retrieve the html page from the target site in milliseconds http_server_response_time_ms: The time taken (in mS) to receive the response headers from the target site. This is a more useful figure in many instances, as it does not include the page load time to is more indicative of the web server RTT. http_status_code : The HTTP status code returned from the target site in this test instance (200 is good, other values have varying meanings: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes ) iperf3 TCP Results (Data source: wiperf-iperf3-tcp) time : Unix timestamp of time test was performed sent_mbps : The transmit throughput achieved (in megabits per seconds) during the TCP iperf test received_mbps : The receive throughput achieved (in megabits per seconds) during the TCP iperf test sent_bytes : The number of bytes (i.e. data volume) sent from the probe to the iperf server during the test received_bytes : The number of bytes (i.e. data volume) received by the probe from the iperf server during the test retransmits : The number of times frames had to be re=transmitted during the test iperf3 UDP Results (Data source: wiperf-iperf3-udp) time : Unix timestamp of time test was performed bytes : The number of bytes transferred from the probe to the iperf server during the test mbps : The throughput achieved (in megabits per second) during the iperf test when sending data to the iperf server jitter_ms : The level of jitter measured (in milliseconds) during the test packets : The number of packets sent from the probe to the iperf server during the test lost_packets : The number of transmitted packets lost during the test lost_percent : The percentage of transmitted packets lost during the test DHCP Test Results (Data source: wiperf-dhcp) time : Unix timestamp of time test was performed renewal_time_ms : The time taken for the probe to renew it's IP address in milliseconds SMB Results (Data source: wiperf-smb) time : Unix timestamp of time test was performed smb_index: test index number smb_host: name/IP of host under test filename: name of file being copied during the test smb_time: the file transfer time in mS smb_rate: the file transfer rate in Mbps Poller Errors (Data source: wiperf-poll-errors) time : Unix timestamp of time test was performed error_message: text string containing error message Poll Status Info (Data source: wiperf-poll-status) time : Unix timestamp of time test was performed network: text string indicating if network connection up OK ip: IP address being used by probe network connection speedtest: text string indicating if test completed/failed/disabled ping: text string indicating if test completed/failed/disabled dns: text string indicating if test completed/failed/disabled http: text string indicating if test completed/failed/disabled iperf_tcp: text string indicating if test completed/failed/disabled iperf_udp: text string indicating if test completed/failed/disabled dhcp: text string indicating if test completed/failed/disabled smb: text string indicating if test completed/failed/disabled probe_mode: string indicating if probe in wireless or ethernet mode mgt_if: name of management interface (e.g. eth0, wlan0) run_time: time is secs that poll cycle took to run","title":"Data Points Reference Guide"},{"location":"data_points/#data-points-reference-guide","text":"","title":"Data Points Reference Guide"},{"location":"data_points/#background","text":"The wiperf probe collects a variety of data points about various aspects of network connectivity and performance. It then makes those data points available to a number of databases via their standard API (e.g. Splunk, InfluxDB etc.). The data collected in all instances is the same, but the format of the data presented to each type of database varies depending on the their API and formatting rules and syntax. This document details the data points collected by the probe. These field names and the data values should be the same no matter which database is used. The probe may collect data for the following network tests, depending upon its configuration: Wireless network connectivity details Speedtest testing results data ICMP ping tests to various destinations DNS lookup tests to various destinations ] HTTP (web) tests to various destinations iperf3 TCP test to a nominated iperf3 server iperf3 UDP test to a nominated iperf3 server DHCP renewal test to test DHCP performance on network to which the WLAN Pi is connected SMB/CIFS file copy tests from various desinations Poller Errors Poll Status Information The tests are run each time the wiperf process is triggered (usually every 5 minutes from a local cron job). The tests that are run, together with test configuration parameters are configured in the config.ini file. Here are the data points that may be collected, displayed by test type:","title":"Background"},{"location":"data_points/#data-points-details","text":"","title":"Data Points Details"},{"location":"data_points/#wireless-network-connectivity","text":"(Data source: wiperf-network) time : Unix timestamp of time test was performed ssid : The network name of the wireless network to which the wiperf probe is currently connected bssid : The basic service set identifier (i.e. MAC address) of the radio to which the wiperf probe is currently connected freq_ghz : The centre frequency of the channel on which the probe is operating (note this may be different to the primary channel centre freq if a bonded channel is in use) center_freq_ghz : The centre frequency of the primary channel on which the probe is operating channel : The channel number on which the probe is operating channel_width: The channel width (e.g. 20MHz, 40MHz, 80MHz) of the channel on which the probe is operating tx_rate_mbps : The PHY rate at which data is being sent from the probe to the AP (note this is not a throughput rate, just a physical connection rate) rx_rate_mbps : The PHY rate at which data is being sent from the AP to the probe (note this is not a throughput rate, just a physical connection rate) tx_mcs : For HT & VHT connections, this is the the MCS value used by the probe to the AP rx_mcs : For HT & VHT connections, this is the the MCS value used by the AP to the probe signal_level_dbm : The power level of the AP radio signal as observed by the probe (in dBm) tx_retries : The number of transmitted frames that have had to be sent gain (retried) ip_address : The IP address assigned to the probe WLAN NIC","title":"Wireless Network Connectivity"},{"location":"data_points/#speedtest-results","text":"(Data source: wiperf-speedtest) time : Unix timestamp of time test was performed ping_time : The RTT of a ping test to the speedtest server download_rate_mbps : The throughput rate achieved when receiving data from the speedtest server in megabits per second upload_rate_mbps : The throughput rate achieved when sending data to the speedtest server in megabits per second server_name : The name of the speedtest server used for this test","title":"Speedtest Results"},{"location":"data_points/#ping-results","text":"(Data source: wiperf-ping) time : Unix timestamp of time test was performed ping_index : wiperf runs up to 5 instances of ping test via its configuration file. This index uniquely identifies each instance. ping_host : The IP address or hostname of the target host/site being pinged pkts_tx : The number of ping request packets sent during the ping test pkts_rx : The number of ping response packets received back during the ping test percent_loss : The percentage (%) of packets lost during the test (i.e. how many responses were received compared to requests sent) test_time_ms : How long the ping test took in total rtt_min_ms : The minimum round trip time of all ping tests to this test instance in milliseconds rtt_avg_ms : The average round trip time of all ping tests to this test instance in milliseconds rtt_max_ms : The maximum round trip time of all ping tests to this test instance in milliseconds rtt_mdev_ms : Standard deviation of all ping tests to this test instance (...no, I don't know either...but you'll look cool at dinner parties if you mention it.)","title":"Ping Results"},{"location":"data_points/#dns-results","text":"(Data source: wiperf-ping) time : Unix timestamp of time test was performed dns_index : wiperf runs up to 5 instances of DNS test via its configuration file. This index uniquely identifies each instance. dns_target : The domain name of the target host/site which is the subject of the DNS lookup test lookup_time_ms : The time taken to perform the DNS lookup in milliseconds","title":"DNS Results"},{"location":"data_points/#http-results","text":"(Data source: wiperf-http) time : Unix timestamp of time test was performed http_index : wiperf runs up to 5 instances of HTTP test via its configuration file. This index uniquely identifies each instance. http_target : The domain name (or IP address) of the target site which is the subject of the HTTP test http_get_time_ms : The time taken (in mS) to retrieve the html page from the target site in milliseconds http_server_response_time_ms: The time taken (in mS) to receive the response headers from the target site. This is a more useful figure in many instances, as it does not include the page load time to is more indicative of the web server RTT. http_status_code : The HTTP status code returned from the target site in this test instance (200 is good, other values have varying meanings: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes )","title":"HTTP Results"},{"location":"data_points/#iperf3-tcp-results","text":"(Data source: wiperf-iperf3-tcp) time : Unix timestamp of time test was performed sent_mbps : The transmit throughput achieved (in megabits per seconds) during the TCP iperf test received_mbps : The receive throughput achieved (in megabits per seconds) during the TCP iperf test sent_bytes : The number of bytes (i.e. data volume) sent from the probe to the iperf server during the test received_bytes : The number of bytes (i.e. data volume) received by the probe from the iperf server during the test retransmits : The number of times frames had to be re=transmitted during the test","title":"iperf3 TCP Results"},{"location":"data_points/#iperf3-udp-results","text":"(Data source: wiperf-iperf3-udp) time : Unix timestamp of time test was performed bytes : The number of bytes transferred from the probe to the iperf server during the test mbps : The throughput achieved (in megabits per second) during the iperf test when sending data to the iperf server jitter_ms : The level of jitter measured (in milliseconds) during the test packets : The number of packets sent from the probe to the iperf server during the test lost_packets : The number of transmitted packets lost during the test lost_percent : The percentage of transmitted packets lost during the test","title":"iperf3 UDP Results"},{"location":"data_points/#dhcp-test-results","text":"(Data source: wiperf-dhcp) time : Unix timestamp of time test was performed renewal_time_ms : The time taken for the probe to renew it's IP address in milliseconds","title":"DHCP Test Results"},{"location":"data_points/#smb-results","text":"(Data source: wiperf-smb) time : Unix timestamp of time test was performed smb_index: test index number smb_host: name/IP of host under test filename: name of file being copied during the test smb_time: the file transfer time in mS smb_rate: the file transfer rate in Mbps","title":"SMB Results"},{"location":"data_points/#poller-errors","text":"(Data source: wiperf-poll-errors) time : Unix timestamp of time test was performed error_message: text string containing error message","title":"Poller Errors"},{"location":"data_points/#poll-status-info","text":"(Data source: wiperf-poll-status) time : Unix timestamp of time test was performed network: text string indicating if network connection up OK ip: IP address being used by probe network connection speedtest: text string indicating if test completed/failed/disabled ping: text string indicating if test completed/failed/disabled dns: text string indicating if test completed/failed/disabled http: text string indicating if test completed/failed/disabled iperf_tcp: text string indicating if test completed/failed/disabled iperf_udp: text string indicating if test completed/failed/disabled dhcp: text string indicating if test completed/failed/disabled smb: text string indicating if test completed/failed/disabled probe_mode: string indicating if probe in wireless or ethernet mode mgt_if: name of management interface (e.g. eth0, wlan0) run_time: time is secs that poll cycle took to run","title":"Poll Status Info"},{"location":"db_health/","text":"Dashboard - Probe Health Grafana Help page - TBA Splunk Help page - TBA","title":"Probe Health"},{"location":"db_health/#dashboard-probe-health","text":"","title":"Dashboard - Probe Health"},{"location":"db_health/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_health/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"db_http/","text":"Dashboard - HTTP Grafana Help page - TBA Splunk Help page - TBA","title":"HTTP"},{"location":"db_http/#dashboard-http","text":"","title":"Dashboard - HTTP"},{"location":"db_http/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_http/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"db_probe_summary/","text":"Dashboard - 01 - Probe Summary Overview The probe summary report attempts to provide a high level view of the tests that are run by the probe. By providing summary data in one place, it allows view of correlation between any factors that may be causing performance issues. For instance, if HTTP tests are showing a drop in response times, by looking at other test results, it is possible to see if this issue is limited to just HTTP traffic or maybe other network conditions are the root cause of the issue. If a low physical connection rate on the wireless connection is also observed, the the issue is more likely to be related to a wireless connection issue than a connectivity elsewhere on the network, such as an overloaded Internet WAN link. By observing the results of all tests, an assessment of the likely fault domain is more readily available by inspecting this report. Grafana For an overview of this report please see this explanation Report Panels Speedtest (Download/Upload) This panel show the results of tests to the Ookla speedtest service. The average upload and download speeds (in mbps) are shown. Note that although these results are interesting, they are results from servers that are out on the Internet. The results are going to be subject to the varying conditions on those servers, the Internet network infrastructure itself and your own organizations's Internet pipe. These factors are likely to vary significantly during the day as conditions change. Unless there are other supporting test results, poor speedtest results should not be taken as a definitive indication of an issue on your infrastructure. These results are useful when correlated with other network test results, but their value in isolation is limited. Wireless Connection Rate/Signal TBA DNS Lookup Time (mS) TBA HTTP Server Response Time (mS) TBA DHCP Renewal Time TBA TCP iPerf Rate TBA Ping Targets RTT Avg Times TBA UDP iPerf Throughput TBA iPerf UDP (Jitter/Loss) TBA Poller Status Info TBA Splunk For an overview of this report please see this explanation Report Panels Speedtest (Download/Upload) This panel show the results of tests to the Ookla speedtest service. The average upload and download speeds (in mbps) are shown. Note that although these results are interesting, they are results from servers that are out on the Internet. The results are going to be subject to the varying conditions on those servers, the Internet network infrastructure itself and your own organizations's Internet pipe. These factors are likely to vary significantly during the day as conditions change. Unless there are other supporting test results, poor speedtest results should not be taken as a definitive indication of an issue on your infrastructure. These results are useful when correlated with other network test results, but their value in isolation is limited. Wireless Connection Rate/Signal TBA DNS Lookup Time (mS) TBA HTTP Server Response Time (mS) TBA DHCP Renewal Time TBA TCP iPerf Rate TBA Ping Targets RTT Avg Times TBA UDP iPerf Throughput TBA iPerf UDP (Jitter/Loss) TBA Poller Status Info TBA","title":"Probe Summary"},{"location":"db_probe_summary/#dashboard-01-probe-summary","text":"","title":"Dashboard - 01 - Probe Summary"},{"location":"db_probe_summary/#overview","text":"The probe summary report attempts to provide a high level view of the tests that are run by the probe. By providing summary data in one place, it allows view of correlation between any factors that may be causing performance issues. For instance, if HTTP tests are showing a drop in response times, by looking at other test results, it is possible to see if this issue is limited to just HTTP traffic or maybe other network conditions are the root cause of the issue. If a low physical connection rate on the wireless connection is also observed, the the issue is more likely to be related to a wireless connection issue than a connectivity elsewhere on the network, such as an overloaded Internet WAN link. By observing the results of all tests, an assessment of the likely fault domain is more readily available by inspecting this report.","title":"Overview"},{"location":"db_probe_summary/#grafana","text":"For an overview of this report please see this explanation","title":"Grafana"},{"location":"db_probe_summary/#report-panels","text":"","title":"Report Panels"},{"location":"db_probe_summary/#speedtest-downloadupload","text":"This panel show the results of tests to the Ookla speedtest service. The average upload and download speeds (in mbps) are shown. Note that although these results are interesting, they are results from servers that are out on the Internet. The results are going to be subject to the varying conditions on those servers, the Internet network infrastructure itself and your own organizations's Internet pipe. These factors are likely to vary significantly during the day as conditions change. Unless there are other supporting test results, poor speedtest results should not be taken as a definitive indication of an issue on your infrastructure. These results are useful when correlated with other network test results, but their value in isolation is limited.","title":"Speedtest (Download/Upload)"},{"location":"db_probe_summary/#wireless-connection-ratesignal","text":"TBA","title":"Wireless Connection Rate/Signal"},{"location":"db_probe_summary/#dns-lookup-time-ms","text":"TBA","title":"DNS Lookup Time (mS)"},{"location":"db_probe_summary/#http-server-response-time-ms","text":"TBA","title":"HTTP Server Response Time (mS)"},{"location":"db_probe_summary/#dhcp-renewal-time","text":"TBA","title":"DHCP Renewal Time"},{"location":"db_probe_summary/#tcp-iperf-rate","text":"TBA","title":"TCP iPerf Rate"},{"location":"db_probe_summary/#ping-targets-rtt-avg-times","text":"TBA","title":"Ping Targets RTT Avg Times"},{"location":"db_probe_summary/#udp-iperf-throughput","text":"TBA","title":"UDP iPerf Throughput"},{"location":"db_probe_summary/#iperf-udp-jitterloss","text":"TBA","title":"iPerf UDP (Jitter/Loss)"},{"location":"db_probe_summary/#poller-status-info","text":"TBA","title":"Poller Status Info"},{"location":"db_probe_summary/#splunk","text":"For an overview of this report please see this explanation","title":"Splunk"},{"location":"db_probe_summary/#report-panels_1","text":"","title":"Report Panels"},{"location":"db_probe_summary/#speedtest-downloadupload_1","text":"This panel show the results of tests to the Ookla speedtest service. The average upload and download speeds (in mbps) are shown. Note that although these results are interesting, they are results from servers that are out on the Internet. The results are going to be subject to the varying conditions on those servers, the Internet network infrastructure itself and your own organizations's Internet pipe. These factors are likely to vary significantly during the day as conditions change. Unless there are other supporting test results, poor speedtest results should not be taken as a definitive indication of an issue on your infrastructure. These results are useful when correlated with other network test results, but their value in isolation is limited.","title":"Speedtest (Download/Upload)"},{"location":"db_probe_summary/#wireless-connection-ratesignal_1","text":"TBA","title":"Wireless Connection Rate/Signal"},{"location":"db_probe_summary/#dns-lookup-time-ms_1","text":"TBA","title":"DNS Lookup Time (mS)"},{"location":"db_probe_summary/#http-server-response-time-ms_1","text":"TBA","title":"HTTP Server Response Time (mS)"},{"location":"db_probe_summary/#dhcp-renewal-time_1","text":"TBA","title":"DHCP Renewal Time"},{"location":"db_probe_summary/#tcp-iperf-rate_1","text":"TBA","title":"TCP iPerf Rate"},{"location":"db_probe_summary/#ping-targets-rtt-avg-times_1","text":"TBA","title":"Ping Targets RTT Avg Times"},{"location":"db_probe_summary/#udp-iperf-throughput_1","text":"TBA","title":"UDP iPerf Throughput"},{"location":"db_probe_summary/#iperf-udp-jitterloss_1","text":"TBA","title":"iPerf UDP (Jitter/Loss)"},{"location":"db_probe_summary/#poller-status-info_1","text":"TBA","title":"Poller Status Info"},{"location":"db_smb/","text":"Dashboard - SMB Grafana Help page - TBA Splunk Help page - TBA","title":"SMB"},{"location":"db_smb/#dashboard-smb","text":"","title":"Dashboard - SMB"},{"location":"db_smb/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_smb/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"db_speedtest/","text":"Dashboard - Speedtest Grafana Help page - TBA Splunk Help page - TBA","title":"Speedtest"},{"location":"db_speedtest/#dashboard-speedtest","text":"","title":"Dashboard - Speedtest"},{"location":"db_speedtest/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_speedtest/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"db_tcp_iperf3/","text":"Dashboard - TCP iperf3 Grafana Help page - TBA Splunk Help page - TBA","title":"TCP iperf3"},{"location":"db_tcp_iperf3/#dashboard-tcp-iperf3","text":"","title":"Dashboard - TCP iperf3"},{"location":"db_tcp_iperf3/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_tcp_iperf3/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"db_udp_iperf3/","text":"Dashboard - UDP iperf3 Grafana Help page - TBA Splunk Help page - TBA","title":"UDP iperf3"},{"location":"db_udp_iperf3/#dashboard-udp-iperf3","text":"","title":"Dashboard - UDP iperf3"},{"location":"db_udp_iperf3/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_udp_iperf3/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"db_wireless/","text":"Dashboard - Wireless Detail Grafana Help page - TBA Splunk Help page - TBA","title":"Wireless Detail"},{"location":"db_wireless/#dashboard-wireless-detail","text":"","title":"Dashboard - Wireless Detail"},{"location":"db_wireless/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_wireless/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"faq/","text":"FAQ How can I report a bug / ask a question / make a suggestion for wiperf? Please checkout the discussion section of the wiperf site of GitHub Why do I get a message saying \"Switch : failed wiperf mode\" when I try to switch on to wiperf mode on the WLAN Pi? In almost every instance, this is due to the fact that the configuration file for wiperf has not been configured before trying to switch in to wiperf mode. Check out the required instructions here: Probe configuration When using wiperf on the WLAN Pi, how can I remotely flip between classic and wiper modes via the CLI? Warning : Although it is possible to flip modes remotely (via an SSH session), be aware that you may hit network connectivity issues unless you are very careful. Remember that in classic mode the file /etc/network/interfaces and /etc/wpa_supplicant/wpa_supplicant.conf are used for network connectivity configuration. In wiperf mode, the files /etc/wiperf/conf/etc/network/interfaces and /etc/wiperf/conf/etc/wpa_supplicant/wpa_supplicant.conf are used for network connectivity configuration. To check the current mode of the wiperf, enter the following command: # this files shows the current mode state (i.e. wiperf, wconsole, hotspot or classic) cat /etc/wlanpi-state To check the current mode of wiperf using the wiper switcher script: sudo /usr/bin/wiperf_switcher status To toggle from classic mode to wiperf: sudo /usr/bin/wiperf_switcher on To toggle from wiperf mode to classic: sudo /usr/bin/wiperf_switcher off (Remember, when switching modes, the wlanpi will reset and you will lose comms for around a minute) Why does installation of wiperf fail with the message \"(fail) pip installation of wiperf_poller failed. Exiting.\" ? This is usually due to the fact that the version of python required for wiperf is python version 3.7 or greater. This means that python version 3.7, 3.8...etc are fine but 3.6, 3.5, 3.4, 3.3... etc. will not work. To check the version of python on your probe, enter the CLI command: python -V (note the uppercase 'V'). If you cannot upgrade your version of python using \"apt-get\", then you will need to obtain a more recent image for your probe. How do I use wiperf with a proxy in my network? Please see this advanced configuration note: link My probe only needs to hit internal network targets. How do I stop the DNS check to google.com? Before commencing tests, wiperf will perform a test DNS lookup to ensure that DNS is working OK. By default, the DNS target in /etc/wiperf/config.ini is set to 'google.com'. If your DNS is internal to your network and does not resolve public Internet targets, you can change the section below to point at an internal lookup target (that will pass a lookup!). ; connectivity DNS lookup - site used for initial DNS lookup when assessing if DNS working OK connectivity_lookup: google.com How do I upgrade to the latest version of wiperf? Please see the details in this page: link How do I change the hostname of my probe? Please see the details in this help page: link Why are MCS & Rx Phy rates missing from my reports? In several dashboard reports, the reported MCS values & Rx Phy rate may be blank or permanently zero. This is because these values simply are not reported by many NICs. Sorry, there's not much I can do about this as I don't write the wireless NIC drivers. My probe seems to reboot itself intermittently. Why? Wiperf has a watchdog feature that it uses to try to reset things when it is having connectivity related difficulties. There may be instances when tests are continually failing or wireless connectivity is intermittent due to perhaps being stuck on a remote AP that is sub-optimal from a connectivity perspective. If persistent issues are detected, then wiperf will reboot the probe to try to remediate the issue. This will provide the opportunity to the reset all network connectivity and internal processes. Note that this is a last ditch mechanism. Wiperf will also try bouncing network interfaces to remediate any short-term connectivity issues, which will likely fix many issues without the need for a full reboot. If you observe your probe rebooting on a regular basis (e.g. a couple of times a hour), then check its logs as it is very unhappy about something. How Can I Harden the Probe Security? Please see this note for some suggestions for hardening the probe: link Where do I get the dashboard reports for Splunk and Grafana? Use SFTP/SCP and pull the xml files in /usr/share/wiperf/dashboards from your probe. Or, visit the wiperf GitHub site here How can I fix my probe to only connect to one specific wireless access point for testing? Checkout this note for specific instructions on this configuration. Can I make a feature suggestion? Yes, get along to the GitHub site and post your suggestion in the discussion section of the wiperf GitHub site. It will be added to my \"todo\" list. Can I get some support with wiperf? I try my best to support folks who are having difficulty, but it's a best efforts endeavour. Please make sure you checkout all of the documentation I've provided, but if all else fails, post a question in the discussion section of the wiperf GitHub site. Please be patient... Can I contribute some code for a new feature for wiperf? Please, get in touch before starting work on any code you'd like to submit as a PR. I love feedback and ideas, but each new feature costs me more cycles to support. Let's agree it can be included before submitting. Can I run tests over the Ethernet interface of the WLAN Pi? Yes, from wiperf v2 onwards.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-can-i-report-a-bug-ask-a-question-make-a-suggestion-for-wiperf","text":"Please checkout the discussion section of the wiperf site of GitHub","title":"How can I report a bug / ask a question / make a suggestion for wiperf?"},{"location":"faq/#why-do-i-get-a-message-saying-switch-failed-wiperf-mode-when-i-try-to-switch-on-to-wiperf-mode-on-the-wlan-pi","text":"In almost every instance, this is due to the fact that the configuration file for wiperf has not been configured before trying to switch in to wiperf mode. Check out the required instructions here: Probe configuration","title":"Why do I get a message saying \"Switch : failed wiperf mode\" when I try to switch on to wiperf mode on the WLAN Pi?"},{"location":"faq/#when-using-wiperf-on-the-wlan-pi-how-can-i-remotely-flip-between-classic-and-wiper-modes-via-the-cli","text":"Warning : Although it is possible to flip modes remotely (via an SSH session), be aware that you may hit network connectivity issues unless you are very careful. Remember that in classic mode the file /etc/network/interfaces and /etc/wpa_supplicant/wpa_supplicant.conf are used for network connectivity configuration. In wiperf mode, the files /etc/wiperf/conf/etc/network/interfaces and /etc/wiperf/conf/etc/wpa_supplicant/wpa_supplicant.conf are used for network connectivity configuration. To check the current mode of the wiperf, enter the following command: # this files shows the current mode state (i.e. wiperf, wconsole, hotspot or classic) cat /etc/wlanpi-state To check the current mode of wiperf using the wiper switcher script: sudo /usr/bin/wiperf_switcher status To toggle from classic mode to wiperf: sudo /usr/bin/wiperf_switcher on To toggle from wiperf mode to classic: sudo /usr/bin/wiperf_switcher off (Remember, when switching modes, the wlanpi will reset and you will lose comms for around a minute)","title":"When using wiperf on the WLAN Pi, how can I remotely flip between classic and wiper modes via the CLI?"},{"location":"faq/#why-does-installation-of-wiperf-fail-with-the-message-fail-pip-installation-of-wiperf_poller-failed-exiting","text":"This is usually due to the fact that the version of python required for wiperf is python version 3.7 or greater. This means that python version 3.7, 3.8...etc are fine but 3.6, 3.5, 3.4, 3.3... etc. will not work. To check the version of python on your probe, enter the CLI command: python -V (note the uppercase 'V'). If you cannot upgrade your version of python using \"apt-get\", then you will need to obtain a more recent image for your probe.","title":"Why does installation of wiperf fail with the message \"(fail) pip installation of wiperf_poller failed. Exiting.\" ?"},{"location":"faq/#how-do-i-use-wiperf-with-a-proxy-in-my-network","text":"Please see this advanced configuration note: link","title":"How do I use wiperf with a proxy in my network?"},{"location":"faq/#my-probe-only-needs-to-hit-internal-network-targets-how-do-i-stop-the-dns-check-to-googlecom","text":"Before commencing tests, wiperf will perform a test DNS lookup to ensure that DNS is working OK. By default, the DNS target in /etc/wiperf/config.ini is set to 'google.com'. If your DNS is internal to your network and does not resolve public Internet targets, you can change the section below to point at an internal lookup target (that will pass a lookup!). ; connectivity DNS lookup - site used for initial DNS lookup when assessing if DNS working OK connectivity_lookup: google.com","title":"My probe only needs to hit internal network targets. How do I stop the DNS check to google.com?"},{"location":"faq/#how-do-i-upgrade-to-the-latest-version-of-wiperf","text":"Please see the details in this page: link","title":"How do I upgrade to the latest version of wiperf?"},{"location":"faq/#how-do-i-change-the-hostname-of-my-probe","text":"Please see the details in this help page: link","title":"How do I change the hostname of my probe?"},{"location":"faq/#why-are-mcs-rx-phy-rates-missing-from-my-reports","text":"In several dashboard reports, the reported MCS values & Rx Phy rate may be blank or permanently zero. This is because these values simply are not reported by many NICs. Sorry, there's not much I can do about this as I don't write the wireless NIC drivers.","title":"Why are MCS &amp; Rx Phy rates missing from my reports?"},{"location":"faq/#my-probe-seems-to-reboot-itself-intermittently-why","text":"Wiperf has a watchdog feature that it uses to try to reset things when it is having connectivity related difficulties. There may be instances when tests are continually failing or wireless connectivity is intermittent due to perhaps being stuck on a remote AP that is sub-optimal from a connectivity perspective. If persistent issues are detected, then wiperf will reboot the probe to try to remediate the issue. This will provide the opportunity to the reset all network connectivity and internal processes. Note that this is a last ditch mechanism. Wiperf will also try bouncing network interfaces to remediate any short-term connectivity issues, which will likely fix many issues without the need for a full reboot. If you observe your probe rebooting on a regular basis (e.g. a couple of times a hour), then check its logs as it is very unhappy about something.","title":"My probe seems to reboot itself intermittently. Why?"},{"location":"faq/#how-can-i-harden-the-probe-security","text":"Please see this note for some suggestions for hardening the probe: link","title":"How Can I Harden the Probe Security?"},{"location":"faq/#where-do-i-get-the-dashboard-reports-for-splunk-and-grafana","text":"Use SFTP/SCP and pull the xml files in /usr/share/wiperf/dashboards from your probe. Or, visit the wiperf GitHub site here","title":"Where do I get the dashboard reports for Splunk and Grafana?"},{"location":"faq/#how-can-i-fix-my-probe-to-only-connect-to-one-specific-wireless-access-point-for-testing","text":"Checkout this note for specific instructions on this configuration.","title":"How can I fix my probe to only connect to one specific wireless access point for testing?"},{"location":"faq/#can-i-make-a-feature-suggestion","text":"Yes, get along to the GitHub site and post your suggestion in the discussion section of the wiperf GitHub site. It will be added to my \"todo\" list.","title":"Can I make a feature suggestion?"},{"location":"faq/#can-i-get-some-support-with-wiperf","text":"I try my best to support folks who are having difficulty, but it's a best efforts endeavour. Please make sure you checkout all of the documentation I've provided, but if all else fails, post a question in the discussion section of the wiperf GitHub site. Please be patient...","title":"Can I get some support with wiperf?"},{"location":"faq/#can-i-contribute-some-code-for-a-new-feature-for-wiperf","text":"Please, get in touch before starting work on any code you'd like to submit as a PR. I love feedback and ideas, but each new feature costs me more cycles to support. Let's agree it can be included before submitting.","title":"Can I contribute some code for a new feature for wiperf?"},{"location":"faq/#can-i-run-tests-over-the-ethernet-interface-of-the-wlan-pi","text":"Yes, from wiperf v2 onwards.","title":"Can I run tests over the Ethernet interface of the WLAN Pi?"},{"location":"grafana_configure/","text":"Grafana Configuration Once the Granafa installation is complete, there are two main tasks remaining: Integration of Grafana with InfluxDB Addition of the wiperf dashboards Integration of Grafana with InfluxDB Integration with InfluxDB is required to allow Grafana to send data queries to the InfluxDB database and turn the data in to graphical reports. To do this, Grafana needs to know: the data source is an InfluxDB database where it is (IP address & port) the name of the database within Influx DB (as we previously configured) the access credentials to be used to pull the data out of InfluxDB The screen-shots below show the required steps: Configuration > Data Sources > Add Data Source : Select InfluxDB : Enter the name to be referenced for the connection, the URL, database name, username & password (all highlighted below) - note the InfluxDB values use those configure previously when we set up InfluxDB (the datbase name, username & password settings must match those used in the InfluxDB setup): Once completed, if you hit 'Save and Test' , the database connection test should indicate success if all information has been correctly entered. Adding Wiperf Dashboards Dashboard xml files can be obtained from the /usr/share/wiperf/dashboards folder of the probe. These may be downloaded to your local laptop using a utlity such as SCP or SFTP. Alternatively, the dashboard files may be accessed in the main wiperf repo at : folder Once the dashboard files have been downloaded, they may be imported to Grafana using the following steps: Use the menu option Dashboards > Manage > Import : Hit the 'Upload Json' button and select the dashboard file on your local laptop that you'd like to upload The name of the imported report will be shown as indicated. If you'd like to create or select a folder, this can be done in the 'Folder' field. The database connection previously created as the data source must also be selected to ensure the InfluxDB database can be queried: Once 'Import' is hit, the dashboard will be available in the main Grafana GUI. To find out more about usng Grafana, visit the Grafana getting started pages.","title":"Grafana Configuration"},{"location":"grafana_configure/#grafana-configuration","text":"Once the Granafa installation is complete, there are two main tasks remaining: Integration of Grafana with InfluxDB Addition of the wiperf dashboards","title":"Grafana Configuration"},{"location":"grafana_configure/#integration-of-grafana-with-influxdb","text":"Integration with InfluxDB is required to allow Grafana to send data queries to the InfluxDB database and turn the data in to graphical reports. To do this, Grafana needs to know: the data source is an InfluxDB database where it is (IP address & port) the name of the database within Influx DB (as we previously configured) the access credentials to be used to pull the data out of InfluxDB The screen-shots below show the required steps: Configuration > Data Sources > Add Data Source : Select InfluxDB : Enter the name to be referenced for the connection, the URL, database name, username & password (all highlighted below) - note the InfluxDB values use those configure previously when we set up InfluxDB (the datbase name, username & password settings must match those used in the InfluxDB setup): Once completed, if you hit 'Save and Test' , the database connection test should indicate success if all information has been correctly entered.","title":"Integration of Grafana with InfluxDB"},{"location":"grafana_configure/#adding-wiperf-dashboards","text":"Dashboard xml files can be obtained from the /usr/share/wiperf/dashboards folder of the probe. These may be downloaded to your local laptop using a utlity such as SCP or SFTP. Alternatively, the dashboard files may be accessed in the main wiperf repo at : folder Once the dashboard files have been downloaded, they may be imported to Grafana using the following steps: Use the menu option Dashboards > Manage > Import : Hit the 'Upload Json' button and select the dashboard file on your local laptop that you'd like to upload The name of the imported report will be shown as indicated. If you'd like to create or select a folder, this can be done in the 'Folder' field. The database connection previously created as the data source must also be selected to ensure the InfluxDB database can be queried: Once 'Import' is hit, the dashboard will be available in the main Grafana GUI. To find out more about usng Grafana, visit the Grafana getting started pages.","title":"Adding Wiperf Dashboards"},{"location":"grafana_install/","text":"Grafana Installation Obtaining and installing the Grafana software is straightforward. The following notes provide a high level overview of the steps required. Note that these instructions are for Grafana version 6.7 (other versions may work, but have not been tested): Visit the Grafana 6.7 installation guide at https://grafana.com/docs/grafana/v6.7/ . This provides acces to a wide variety of information about Grafanam including supported OS'es and platform concepts To download the code and to see the commands required for installation on the server CLI, visit the following download page: https://grafana.com/grafana/download Select the required version, Open Source edition and your OS (most likely a Linux variant) Make a copy of the CLI commands provided to download and install the software for your OS SSH to the server that will be used to host Grafana Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start grafana-server Ensure that the service will be started in the server is rebooted with: sudo systemctl enable grafana-server Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status grafana-server (ensure the process is \"active (running)\" ) As a final check, ensure that the Grafana web GUI is available using the URL: http://<server_IP>:3000/ (Note that the default login/pwd is admin/admin)","title":"Grafana Installation"},{"location":"grafana_install/#grafana-installation","text":"Obtaining and installing the Grafana software is straightforward. The following notes provide a high level overview of the steps required. Note that these instructions are for Grafana version 6.7 (other versions may work, but have not been tested): Visit the Grafana 6.7 installation guide at https://grafana.com/docs/grafana/v6.7/ . This provides acces to a wide variety of information about Grafanam including supported OS'es and platform concepts To download the code and to see the commands required for installation on the server CLI, visit the following download page: https://grafana.com/grafana/download Select the required version, Open Source edition and your OS (most likely a Linux variant) Make a copy of the CLI commands provided to download and install the software for your OS SSH to the server that will be used to host Grafana Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start grafana-server Ensure that the service will be started in the server is rebooted with: sudo systemctl enable grafana-server Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status grafana-server (ensure the process is \"active (running)\" ) As a final check, ensure that the Grafana web GUI is available using the URL: http://<server_IP>:3000/ (Note that the default login/pwd is admin/admin)","title":"Grafana Installation"},{"location":"grafana_platform/","text":"Grafana Platform Grafana is an open-source visualization tool that allows us to create reports around the data sent from wiperf probes to InfluxDB. It can integrates with a variety of data sources to query raw data and provides a wide variety of graphical report options - in our case, Grafana integrates with InfluxDB This guide does not cover all installation details of the software package, as these may be obtained from the official Grafana web site: https://grafana.com/docs/grafana/latest/ . Installation instructions are available for all major operating systems. Note that although Windows is supported, if you intend to install Grafana on the same platform as InfuxDB, Windows is not an option as InfluxDB v1.8 does not support Windows. To install Grafana and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that Grafana is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"Grafana Platform"},{"location":"grafana_platform/#grafana-platform","text":"Grafana is an open-source visualization tool that allows us to create reports around the data sent from wiperf probes to InfluxDB. It can integrates with a variety of data sources to query raw data and provides a wide variety of graphical report options - in our case, Grafana integrates with InfluxDB This guide does not cover all installation details of the software package, as these may be obtained from the official Grafana web site: https://grafana.com/docs/grafana/latest/ . Installation instructions are available for all major operating systems. Note that although Windows is supported, if you intend to install Grafana on the same platform as InfuxDB, Windows is not an option as InfluxDB v1.8 does not support Windows. To install Grafana and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that Grafana is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"Grafana Platform"},{"location":"influx_configure/","text":"Influx Configuration Now that we have the InfluxDB software installed, the next step is to create a database in which data from our wiperf probes will be stored. To create the database, we need to execute a series of commands on the CLI of the Influx DB server. Follow the following steps to create the required database: Check the InfluxDB service is running before starting sudo systemctl status influxdb (ensure the process is \"active (running)\" ) Enter the InfluxDB shell using the following command: sudo influx (shell is indicated by the new \">\" prompt) Create an admin user to administer InfluxDB: CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES Exit the InfluxDB shell with the command exit to return to the standard Linux CLI Edit the InfluxDB configuration file (/etc/influxdb/influxdb.conf): sudo nano /etc/influxdb/influxdb.conf uncomment the line # auth-enabled = false in the [http] section and change to auth-enabled = true to enable authentication of access to the database restart the InfluxDB process for the change to take effect: sudo systemctl restart influxdb Enter the InfluxDB shell again using the following command: sudo influx -username admin -password letmein (now using authentication) Create a new database with the following commands: CREATE DATABASE wiperf Check the new database exists using: SHOW DATABASES (the database \"wiperf\" should be shown in the list) Create and assign a user who can write to the wiperf database (i.e. a probe) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" Create and assign a user who can read from the wiperf database (i.e. the Grafana program) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' GRANT read ON \"wiperf\" TO \"grafana\" Exit the InfluxDB shell with the command exit t return to the Linux CLI At this point, the InfluxDB service is ready to receive data from a probe. If you have any probes ready to go, make sure they use the \"wiperf_probe\" user credentials in their configuration file so that they can add their data to the database. If you believe you have a probe that has successfully sent data, you can check the database contents using the following commands in the InfluxDB shell: USE wiperf SHOW SERIES SELECT * FROM \"wiperf-speedtest\" SHOW FIELD KEYS ON \"wiperf\" FROM \"wiperf-speedtest\" To find out more details, please checkout the official getting started guide: https://docs.influxdata.com/influxdb/v1.8/introduction/get-started/ For more information about adding users, check out: https://docs.influxdata.com/influxdb/v1.8/administration/authentication_and_authorization/ Note: You are advised to use your own passwords for the password fields shown in this document to ensure they are secured.","title":"Influx Configuration"},{"location":"influx_configure/#influx-configuration","text":"Now that we have the InfluxDB software installed, the next step is to create a database in which data from our wiperf probes will be stored. To create the database, we need to execute a series of commands on the CLI of the Influx DB server. Follow the following steps to create the required database: Check the InfluxDB service is running before starting sudo systemctl status influxdb (ensure the process is \"active (running)\" ) Enter the InfluxDB shell using the following command: sudo influx (shell is indicated by the new \">\" prompt) Create an admin user to administer InfluxDB: CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES Exit the InfluxDB shell with the command exit to return to the standard Linux CLI Edit the InfluxDB configuration file (/etc/influxdb/influxdb.conf): sudo nano /etc/influxdb/influxdb.conf uncomment the line # auth-enabled = false in the [http] section and change to auth-enabled = true to enable authentication of access to the database restart the InfluxDB process for the change to take effect: sudo systemctl restart influxdb Enter the InfluxDB shell again using the following command: sudo influx -username admin -password letmein (now using authentication) Create a new database with the following commands: CREATE DATABASE wiperf Check the new database exists using: SHOW DATABASES (the database \"wiperf\" should be shown in the list) Create and assign a user who can write to the wiperf database (i.e. a probe) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" Create and assign a user who can read from the wiperf database (i.e. the Grafana program) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' GRANT read ON \"wiperf\" TO \"grafana\" Exit the InfluxDB shell with the command exit t return to the Linux CLI At this point, the InfluxDB service is ready to receive data from a probe. If you have any probes ready to go, make sure they use the \"wiperf_probe\" user credentials in their configuration file so that they can add their data to the database. If you believe you have a probe that has successfully sent data, you can check the database contents using the following commands in the InfluxDB shell: USE wiperf SHOW SERIES SELECT * FROM \"wiperf-speedtest\" SHOW FIELD KEYS ON \"wiperf\" FROM \"wiperf-speedtest\" To find out more details, please checkout the official getting started guide: https://docs.influxdata.com/influxdb/v1.8/introduction/get-started/ For more information about adding users, check out: https://docs.influxdata.com/influxdb/v1.8/administration/authentication_and_authorization/ Note: You are advised to use your own passwords for the password fields shown in this document to ensure they are secured.","title":"Influx Configuration"},{"location":"influx_install/","text":"Influx Installation Obtaining and installing the InfluxDB software is very straightforward. here is a high-level overview of the steps required: On your laptop, open a browser and obtain the required commands to download & install the software by visiting the following web page and selecting the v1.8 download option: https://portal.influxdata.com/downloads/ Copy the install commands provided for your OS Make sure your InfluxDB server has Internet connectivity (as it will need to pull down the required software) SSH to the server that will be used to host InfluxDB On the CLI of your server (your SSH session), paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start influxdb Make sure the InfluxDB service starts after a platform reboot: sudo systemctl enable influxdb Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status influxdb (ensure the process is \"active (running)\" ) The next step is to create a database to drop our incoming data (from wiperf probes) into.","title":"Influx Installation"},{"location":"influx_install/#influx-installation","text":"Obtaining and installing the InfluxDB software is very straightforward. here is a high-level overview of the steps required: On your laptop, open a browser and obtain the required commands to download & install the software by visiting the following web page and selecting the v1.8 download option: https://portal.influxdata.com/downloads/ Copy the install commands provided for your OS Make sure your InfluxDB server has Internet connectivity (as it will need to pull down the required software) SSH to the server that will be used to host InfluxDB On the CLI of your server (your SSH session), paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start influxdb Make sure the InfluxDB service starts after a platform reboot: sudo systemctl enable influxdb Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status influxdb (ensure the process is \"active (running)\" ) The next step is to create a database to drop our incoming data (from wiperf probes) into.","title":"Influx Installation"},{"location":"influx_platform/","text":"InfluxDB Platform InfluxDB is a time-series database that is used to store the network performance data that is collected by wiperf probes. It has many other uses and is used by many organizations as a back-end store for use-cases involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. InfluxDB does not report on network performance report data, but is used as a data repository source for Grafana in our use-case. Details about Grafana are provide later in this documentation. Note that for our use-case, we are using InfluxDB v1.8 (not v2.0). Influx can be installed on a wide variety of Linux-based platforms that can be viewed here . These include Ubuntu, Debian and macOS (no Windows) This guide does not cover all installation details of the software package, as these may be obtained when downloading and installing the software. To install InfluxDB and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high-end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software. Connectivity Planning One area to consider is connectivity between the wiperf probe and the InfluxDB instance. The wiperf probe needs to be able to access the InfluxDB server to send its results data. If the wiperf probe probe is being deployed on a network, how is the performance data generated by the probe going to get back to the InfluxDB server? If the probe is being deployed on a customer network to perform temporary monitoring, it will need to join the wireless network under test (or be plugged in to an ethernet switch port if testing a wired connection). But how is the wiperf probe going to send its data to the InfluxDB server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, potentially bridging wired and wireless networks. In some instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier.) Three topology deployment options are supported: Results data over wireless Results data over Ethernet Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the InfluxDB server. The 3 connectivity options are discussed below. Results Data Over Wireless In this topology the wiperf probe is configured to join an SSID that has the InfluxDB server accessible via its WLAN interface. Typically, the InfluxDB server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the InfluxDB server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of InfluxDB server> Results data over Ethernet If the InfluxDB server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of InfluxDB server> Results data over Zerotier/wireless A simple way of getting the wiperf probe talking with your Splunk server, if it has no direct access, is to use the Zerotier service to create a virtual overlay network via the Internet. In summary, both the InfluxDB server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same virtual network in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the InfluxDB server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees. It\u2019s very easy to use, plus your InfluxDB server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the InfluxDB server and wiperf probe when the client is installed. config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of InfluxDB server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"InfluxDB Platform"},{"location":"influx_platform/#influxdb-platform","text":"InfluxDB is a time-series database that is used to store the network performance data that is collected by wiperf probes. It has many other uses and is used by many organizations as a back-end store for use-cases involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. InfluxDB does not report on network performance report data, but is used as a data repository source for Grafana in our use-case. Details about Grafana are provide later in this documentation. Note that for our use-case, we are using InfluxDB v1.8 (not v2.0). Influx can be installed on a wide variety of Linux-based platforms that can be viewed here . These include Ubuntu, Debian and macOS (no Windows) This guide does not cover all installation details of the software package, as these may be obtained when downloading and installing the software. To install InfluxDB and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high-end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"InfluxDB Platform"},{"location":"influx_platform/#connectivity-planning","text":"One area to consider is connectivity between the wiperf probe and the InfluxDB instance. The wiperf probe needs to be able to access the InfluxDB server to send its results data. If the wiperf probe probe is being deployed on a network, how is the performance data generated by the probe going to get back to the InfluxDB server? If the probe is being deployed on a customer network to perform temporary monitoring, it will need to join the wireless network under test (or be plugged in to an ethernet switch port if testing a wired connection). But how is the wiperf probe going to send its data to the InfluxDB server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, potentially bridging wired and wireless networks. In some instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier.) Three topology deployment options are supported: Results data over wireless Results data over Ethernet Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the InfluxDB server. The 3 connectivity options are discussed below.","title":"Connectivity Planning"},{"location":"influx_platform/#results-data-over-wireless","text":"In this topology the wiperf probe is configured to join an SSID that has the InfluxDB server accessible via its WLAN interface. Typically, the InfluxDB server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the InfluxDB server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of InfluxDB server>","title":"Results Data Over Wireless"},{"location":"influx_platform/#results-data-over-ethernet","text":"If the InfluxDB server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of InfluxDB server>","title":"Results data over Ethernet"},{"location":"influx_platform/#results-data-over-zerotierwireless","text":"A simple way of getting the wiperf probe talking with your Splunk server, if it has no direct access, is to use the Zerotier service to create a virtual overlay network via the Internet. In summary, both the InfluxDB server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same virtual network in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the InfluxDB server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees. It\u2019s very easy to use, plus your InfluxDB server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the InfluxDB server and wiperf probe when the client is installed. config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of InfluxDB server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Results data over Zerotier/wireless"},{"location":"operation/","text":"Overview of Operation Wiperf is an open source utility that runs on a Raspberry Pi or a WLAN Pi hardware device. It provides network probe functionality to gather performance data to give an indication of how a network looks from an end user perspective. It runs a series of tests to gather metrics on network connectivity and performance through the execution of tests such ICMP ping, DNS lookups and iperf. These are fully configurable by editing a local configuration file on the probe device at the time of deployment. Configuration To configure the details of the tests to be run on a probe, a local configuration file on the probe needs to be updated. This will provide information to the probe about items such as the required network connectivity (e.g. wireless/ethernet), IP and credential information for the data server and test details. The configuration file can be updated by accessing the CLI of the probe (usually via SSH) and editing the file /etc/wiperf/config.ini . In addition, configuration of a number of other Linux-OS files (e.g. /etc/network/interfaces ) is also required to connect the probed to the network under test, and run wiperf tests on a regular basis. More details about the probe configuration can be found on this page . Logging Following the completion of the configuration steps, wiperf will run every 5 minutes, performing the configured network tests and sending results data to a reporting server. A number of logs are generated to provide support information about the status and operation of the wiperf process. For more information about the logs created, please visit our troubleshooting page . Reporting Gathering the data with a probe is only half of the story when using wiperf. The gathered data must be sent to a data collection server to allow it to be visualized to allow analysis of network performance. The data server must be an instance of either: Splunk, or InfluxDB with Grafana Splunk The graphic above outlines the collection of network performance data and how this is sent to Splunk by the wiperf probe. The Splunk instance may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.) In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Splunk server for storage and later analysis The data is analyzed by accessing the dashboard (a web GUI) of the Splunk server with a browser ( Note that the Splunk server acts as both the data repository and reporting platform for collected data ) Grafana/Influx The graphic above outlines the collection of network performance data and how this is sent to Influx & Grafana by the wiperf probe. Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data source from which to pull its network performance data. To meet this requirement, a InfluxDB database server is used. Like Grafana, InfluxDB is also an open-source package. ( Note that this setup contrasts with using Splunk, which allows us to use the same package to provide both the data storage and analysis/visualization functions ) For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of InfluxDB as its data source. Grafana & Influx may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.). Note that wiperf only needs to be able to send data to InfluxDB - it requires no communication with the Grafana instance. In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to an InfluxDB server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the InfluxDB server for storage Grafana is configured to use InfluxDB as its data source to allow visualization of performance data Data is analysed by accessing the dashboard (a web GUI) of the Grafana server, which pulls the required dashboard data from InfluxDB.","title":"Operation Overview"},{"location":"operation/#overview-of-operation","text":"Wiperf is an open source utility that runs on a Raspberry Pi or a WLAN Pi hardware device. It provides network probe functionality to gather performance data to give an indication of how a network looks from an end user perspective. It runs a series of tests to gather metrics on network connectivity and performance through the execution of tests such ICMP ping, DNS lookups and iperf. These are fully configurable by editing a local configuration file on the probe device at the time of deployment.","title":"Overview of Operation"},{"location":"operation/#configuration","text":"To configure the details of the tests to be run on a probe, a local configuration file on the probe needs to be updated. This will provide information to the probe about items such as the required network connectivity (e.g. wireless/ethernet), IP and credential information for the data server and test details. The configuration file can be updated by accessing the CLI of the probe (usually via SSH) and editing the file /etc/wiperf/config.ini . In addition, configuration of a number of other Linux-OS files (e.g. /etc/network/interfaces ) is also required to connect the probed to the network under test, and run wiperf tests on a regular basis. More details about the probe configuration can be found on this page .","title":"Configuration"},{"location":"operation/#logging","text":"Following the completion of the configuration steps, wiperf will run every 5 minutes, performing the configured network tests and sending results data to a reporting server. A number of logs are generated to provide support information about the status and operation of the wiperf process. For more information about the logs created, please visit our troubleshooting page .","title":"Logging"},{"location":"operation/#reporting","text":"Gathering the data with a probe is only half of the story when using wiperf. The gathered data must be sent to a data collection server to allow it to be visualized to allow analysis of network performance. The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"Reporting"},{"location":"operation/#splunk","text":"The graphic above outlines the collection of network performance data and how this is sent to Splunk by the wiperf probe. The Splunk instance may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.) In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Splunk server for storage and later analysis The data is analyzed by accessing the dashboard (a web GUI) of the Splunk server with a browser ( Note that the Splunk server acts as both the data repository and reporting platform for collected data )","title":"Splunk"},{"location":"operation/#grafanainflux","text":"The graphic above outlines the collection of network performance data and how this is sent to Influx & Grafana by the wiperf probe. Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data source from which to pull its network performance data. To meet this requirement, a InfluxDB database server is used. Like Grafana, InfluxDB is also an open-source package. ( Note that this setup contrasts with using Splunk, which allows us to use the same package to provide both the data storage and analysis/visualization functions ) For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of InfluxDB as its data source. Grafana & Influx may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.). Note that wiperf only needs to be able to send data to InfluxDB - it requires no communication with the Grafana instance. In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to an InfluxDB server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the InfluxDB server for storage Grafana is configured to use InfluxDB as its data source to allow visualization of performance data Data is analysed by accessing the dashboard (a web GUI) of the Grafana server, which pulls the required dashboard data from InfluxDB.","title":"Grafana/Influx"},{"location":"probe_configure/","text":"Probe Configuration The final step in preparing the probe for deployment is to configure the wiperf software to run the tests we'd like to perform. We also need to tell wiperf where it can find the data server that will provide reporting. The configuration tasks break down as follows: Edit the probe config.ini file to configure tests and data server details Configure a cron job on the probe to run wiperf every 5 mins to perform its network performance tests Configuration File Note: the details in this section apply equally to both the WLAN Pi and RPi probe The operation of wiperf is configured using the file /etc/wiperf/config.ini This needs to be edited prior to running the wiperf software. Network tests are initiated on the WLAN Pi by switching on to wiperf mode. On the RPi, tests are started by configuring a cron job to regularly run the wiperf software (more on this later in this document) By default, the config.ini file does not exist. However, a default template config file ( /etc/wiperf/config.default.ini ) is supplied that can be used as the template to create the config.ini file. Here is the suggested workflow to create the config.ini file: Connect to the CLI of the probe (e.g. via SSH), create a copy of the config template file and edit the newly created configuration file: cd /etc/wiperf # take a copy of the default configuration file sudo cp ./config.default.ini ./config.ini # edit the config file with the required probe settings (ctrl-x to exit the editor) sudo nano ./config.ini By default, the configuration file is set to run all tests except the iperf tests (which may or may not suit your needs). However, there is still an additional minimum configuration that must be applied to successfully run tests. This is outlined in the subsections below. Once you've got your probe going, you're likely going to want to spend a little more time customising the configuration file for your environment. In summary you will need to: Configure the wiperf global mode of operation (wireless or Ethernet) and the interface parameters that determine how the probe is connected to its network Configure the management platform you'll be sending data to Configure the tests you'd like to run Mode/Interface Parameters The probe can be used to perform its tests over its wireless interface, or its ethernet interface. These are known as the 'wireless' or 'ethernet' mode within the config.ini file. In addition, the probe needs to know which interface should ne used to send results data back to the data server. It is possible to perform both tests and send results data over the same interface, or it may be preferable to have tests performed over the wireless interface and return results data over the ethernet interface. The final choice is determined by the environment in to which the probe is deployed. ( Note: if you choose to use Zerotier for management connectivity, the Zerotier interface is also an available option to carry results data back to the data server ) The interfaces available in the probe for ethernet and wireless connectivity will generally be eth0 and wlan0 . However, these names may vary in some platforms. An option to change the actual names of the interfaces of the probe is available if required. The relevant section of the config.ini file is shown below for reference (note that lines that start with a semi-colon (;) are comments and are ignored. Blank lines are also ignored.): [General] ; global test mode: 'wireless' or 'ethernet' ; ; wireless mode: ; - test traffic runs over wireless interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; ethernet mode: ; - test traffic runs over ethernet interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; probe_mode: wireless ; ------------- ethernet mode parameters ------------ ; eth interface name - set this as per the output of an ifconfig command (usually eth0) eth_if: eth0 ; --------------------------------------------------- ; ------------- wireless mode parameters ------------ ; wlan interface name - set this as per the output of an iwconfig command (usually wlan0) wlan_if: wlan0 ; --------------------------------------------------- ; -------------mgt interface parameters ------------ ; interface name over which mgt traffic is sent (i.e. how we get to our management ; server) - options: wlan0, eth0, ztxxxxxx (ZeroTier), lo (local instance of Influx) mgt_if: wlan0 ; --------------------------------------------------- Data Server Parameters Wiperf can send results data to Splunk or InfluxDB (v1.x) data collectors through an exporter module for each collector type. The relevant authentication parameters need to be set for the collector in-use in the following sections ( note that corresponding authentication parameters also need to be configured on the data collector platform also before sending results data - see here for more info: Splunk / InfluxDB ) In summary, the workflow to configure the data server parameters in the probe configuration file is: Set the exporter type (splunk/influxdb) In the appropriate platform section (i.e. Splunk OR InfluxDB) configure the server address of the target data server configure data server port details (if defaults changed) configure data server credential and database information The relevant section of the config.ini file is shown below: ; --------- Common Mgt Platform Params ------- ; set the data exporter type - current options: splunk, influxdb, influxdb2 exporter_type: splunk ; -------------------------------------------- ; -------------- Splunk Config --------------- ; IP address or hostname of Splunk host splunk_host: ; Splunk collector port (8088 by default) splunk_port: 8088 ; Splunk token to access Splunk server created by Splunk (example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d) splunk_token: ;--------------------------------------------- ; -------------- InFlux1 Config --------------- ; IP address or hostname of InfluxDB host influx_host: ; InfluxDb collector port (8086 by default) influx_port: 8086 influx_username: influx_password: influx_database: ;--------------------------------------------- Network Tests Note that all network tests are enabled by default, apart from the iperf3 and SMB tests. If there are some tests you'd like to disable (e.g. if you don't want to run HTTP tests), then you'll need to open up the config.ini file and look through each section for the \"enabled\" parameter for that test and set it to \"no\". For example, to disable the HTTP tests: ; ==================================================================== ; HTTP tests settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [HTTP_test] ; yes = enabled, no = disabled enabled: no For a full description of the configuration file parameters, please review the following page: config.ini reference guide . Running Regular Tests Once the wiperf software has been configured, the final job is to configure a 'cron job' on the probe to run the wiperf testing script every 5 minutes. Cron is a scheduler utility within Linux that will run a software task at configured intervals. ( Note: This step is not required on the WLAN Pi, as the cron job is added automatically when the WLAN Pi is switched in to wiperf mode ) To configure cron (remember, only required on the RPi), on the CLI of the probe, open the cron editor: sudo crontab -e Next, with the editor open, add following line to the open file: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 This command will run the main wiperf script to run the tests configured within config.ini at an interval of 5 minutes. It will also dump all script output to the file /var/log/wiperf_cron.log (this is a good place to look if you hit any issues with wiperf not running as expected) Attention The cron command provided above will run every 5 minutes from the top of the hour (i.e. 0, 5, 10, 15, 20...etc. minutes as the hour.) If you would like to offset the run time to be something other than the top of the hour, simply change the first digit from '0' to another number between 1 and 4. For example, changing the '0' to '1' will run the cron job at 1, 6, 11, 16...etc. minutes past the hour: # run cron at 1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56 minutes past the hour 1-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 (Note: you do not have to wait for the top of the hour for the job to run - it will run as soon as it hits the nearest configured time increment.) Initial Probe Testing Once the cron job has been configured, the case of the RPi, or the WLAN Pi has been put in to wiperf mode, it's time to check if the probe is working as expected. To perform tests, the probe will need to be connected to a network and able to reach the data server. The easiest way to monitor the operation of the probe is to SSH in to the probe and monitor the output of the log file /var/log/wiperf_agent.log . This file is created the first time that wiperf runs. If the file is not created after 5 minutes, then check the log file /var/log/wiperf_cron.log for error messages, as something fundamental is wrong with the installation. To watch the output of /var/log/wiperf_agent.log in real-time and view activity as data is collected every 5 minutes, run the following command on the CLI of the probe (note this command will fail if wiperf has not run correctly and the file does not yet exist): tail -f /var/log/wiperf_agent.log Every 5 minutes, new log output will be seen that looks similar to this: 2020-07-11 11:47:04,214 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,215 - Probe_Log - INFO - Starting logging... 2020-07-11 11:47:04,216 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,240 - Probe_Log - INFO - Checking if we use remote cfg file... 2020-07-11 11:47:04,241 - Probe_Log - INFO - No remote cfg file confgured...using current local ini file. 2020-07-11 11:47:04,242 - Probe_Log - INFO - No lock file found. Creating lock file. 2020-07-11 11:47:04,243 - Probe_Log - INFO - ########## Network connection checks ########## 2020-07-11 11:47:05,245 - Probe_Log - INFO - Checking wireless connection is good...(layer 1 &2) 2020-07-11 11:47:05,246 - Probe_Log - INFO - Checking wireless connection available. 2020-07-11 11:47:05,355 - Probe_Log - INFO - Checking we're connected to the network (layer3) 2020-07-11 11:47:05,356 - Probe_Log - INFO - Checking we have an IP address. 2020-07-11 11:47:05,379 - Probe_Log - INFO - Checking we can do a DNS lookup to google.com 2020-07-11 11:47:05,406 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:05,430 - Probe_Log - INFO - Checked interface route to : 216.58.212.238. Result: 216.58.212.238 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:05,431 - Probe_Log - INFO - Checking we can get to the management platform... 2020-07-11 11:47:05,432 - Probe_Log - INFO - Checking we will send mgt traffic over configured interface 'lo' mode. 2020-07-11 11:47:05,455 - Probe_Log - INFO - Checked interface route to : 127.0.0.1. Result: local 127.0.0.1 dev lo src 127.0.0.1 uid 0 2020-07-11 11:47:05,456 - Probe_Log - INFO - Interface mgt interface route looks good. 2020-07-11 11:47:05,457 - Probe_Log - INFO - Checking port connection to InfluxDB server 127.0.0.1, port: 8086 2020-07-11 11:47:05,484 - Probe_Log - INFO - Port connection to server 127.0.0.1, port: 8086 checked OK. 2020-07-11 11:47:05,485 - Probe_Log - INFO - ########## Wireless Connection ########## 2020-07-11 11:47:05,486 - Probe_Log - INFO - Wireless connection data: SSID:BNL, BSSID:5C:5B:35:C8:4D:C2, Freq:5.5, Center Freq:5.51, Channel: 100, Channel Width: 40, Tx Phy rate:200.0, Rx Phy rate:135.0, Tx MCS: 0, Rx MCS: 0, RSSI:-42.0, Tx retries:187, IP address:192.168.0.48 2020-07-11 11:47:05,486 - Probe_Log - INFO - InfluxDB update: wiperf-network, source=Network Tests 2020-07-11 11:47:05,487 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:05,573 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:05,574 - Probe_Log - INFO - Connection results sent OK. 2020-07-11 11:47:05,595 - Probe_Log - INFO - ########## speedtest ########## 2020-07-11 11:47:05,597 - Probe_Log - INFO - Starting speedtest... 2020-07-11 11:47:06,599 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:06,623 - Probe_Log - INFO - Checked interface route to : 8.8.8.8. Result: 8.8.8.8 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:06,624 - Probe_Log - INFO - Speedtest in progress....please wait. 2020-07-11 11:47:28,761 - Probe_Log - INFO - ping_time: 31, download_rate: 41.56, upload_rate: 9.74, server_name: speedtest-net5.rapidswitch.co.uk:8080 2020-07-11 11:47:28,766 - Probe_Log - INFO - Speedtest ended. 2020-07-11 11:47:28,767 - Probe_Log - INFO - InfluxDB update: wiperf-speedtest, source=Speedtest 2020-07-11 11:47:28,768 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:28,858 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:28,860 - Probe_Log - INFO - Speedtest results sent OK. The output is quite verbose and detailed, but it will provide a good indication of where wiperf is having difficulties. Once wiperf is running with no issues indicated in the logs, then it's time to check for results data on your data server. Hopefully, you'll see performance data being recorded over time as the probe runs its tests and sends the results to the data server. The next step is to deploy the probe .","title":"Probe Configuration"},{"location":"probe_configure/#probe-configuration","text":"The final step in preparing the probe for deployment is to configure the wiperf software to run the tests we'd like to perform. We also need to tell wiperf where it can find the data server that will provide reporting. The configuration tasks break down as follows: Edit the probe config.ini file to configure tests and data server details Configure a cron job on the probe to run wiperf every 5 mins to perform its network performance tests","title":"Probe Configuration"},{"location":"probe_configure/#configuration-file","text":"Note: the details in this section apply equally to both the WLAN Pi and RPi probe The operation of wiperf is configured using the file /etc/wiperf/config.ini This needs to be edited prior to running the wiperf software. Network tests are initiated on the WLAN Pi by switching on to wiperf mode. On the RPi, tests are started by configuring a cron job to regularly run the wiperf software (more on this later in this document) By default, the config.ini file does not exist. However, a default template config file ( /etc/wiperf/config.default.ini ) is supplied that can be used as the template to create the config.ini file. Here is the suggested workflow to create the config.ini file: Connect to the CLI of the probe (e.g. via SSH), create a copy of the config template file and edit the newly created configuration file: cd /etc/wiperf # take a copy of the default configuration file sudo cp ./config.default.ini ./config.ini # edit the config file with the required probe settings (ctrl-x to exit the editor) sudo nano ./config.ini By default, the configuration file is set to run all tests except the iperf tests (which may or may not suit your needs). However, there is still an additional minimum configuration that must be applied to successfully run tests. This is outlined in the subsections below. Once you've got your probe going, you're likely going to want to spend a little more time customising the configuration file for your environment. In summary you will need to: Configure the wiperf global mode of operation (wireless or Ethernet) and the interface parameters that determine how the probe is connected to its network Configure the management platform you'll be sending data to Configure the tests you'd like to run","title":"Configuration File"},{"location":"probe_configure/#modeinterface-parameters","text":"The probe can be used to perform its tests over its wireless interface, or its ethernet interface. These are known as the 'wireless' or 'ethernet' mode within the config.ini file. In addition, the probe needs to know which interface should ne used to send results data back to the data server. It is possible to perform both tests and send results data over the same interface, or it may be preferable to have tests performed over the wireless interface and return results data over the ethernet interface. The final choice is determined by the environment in to which the probe is deployed. ( Note: if you choose to use Zerotier for management connectivity, the Zerotier interface is also an available option to carry results data back to the data server ) The interfaces available in the probe for ethernet and wireless connectivity will generally be eth0 and wlan0 . However, these names may vary in some platforms. An option to change the actual names of the interfaces of the probe is available if required. The relevant section of the config.ini file is shown below for reference (note that lines that start with a semi-colon (;) are comments and are ignored. Blank lines are also ignored.): [General] ; global test mode: 'wireless' or 'ethernet' ; ; wireless mode: ; - test traffic runs over wireless interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; ethernet mode: ; - test traffic runs over ethernet interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; probe_mode: wireless ; ------------- ethernet mode parameters ------------ ; eth interface name - set this as per the output of an ifconfig command (usually eth0) eth_if: eth0 ; --------------------------------------------------- ; ------------- wireless mode parameters ------------ ; wlan interface name - set this as per the output of an iwconfig command (usually wlan0) wlan_if: wlan0 ; --------------------------------------------------- ; -------------mgt interface parameters ------------ ; interface name over which mgt traffic is sent (i.e. how we get to our management ; server) - options: wlan0, eth0, ztxxxxxx (ZeroTier), lo (local instance of Influx) mgt_if: wlan0 ; ---------------------------------------------------","title":"Mode/Interface Parameters"},{"location":"probe_configure/#data-server-parameters","text":"Wiperf can send results data to Splunk or InfluxDB (v1.x) data collectors through an exporter module for each collector type. The relevant authentication parameters need to be set for the collector in-use in the following sections ( note that corresponding authentication parameters also need to be configured on the data collector platform also before sending results data - see here for more info: Splunk / InfluxDB ) In summary, the workflow to configure the data server parameters in the probe configuration file is: Set the exporter type (splunk/influxdb) In the appropriate platform section (i.e. Splunk OR InfluxDB) configure the server address of the target data server configure data server port details (if defaults changed) configure data server credential and database information The relevant section of the config.ini file is shown below: ; --------- Common Mgt Platform Params ------- ; set the data exporter type - current options: splunk, influxdb, influxdb2 exporter_type: splunk ; -------------------------------------------- ; -------------- Splunk Config --------------- ; IP address or hostname of Splunk host splunk_host: ; Splunk collector port (8088 by default) splunk_port: 8088 ; Splunk token to access Splunk server created by Splunk (example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d) splunk_token: ;--------------------------------------------- ; -------------- InFlux1 Config --------------- ; IP address or hostname of InfluxDB host influx_host: ; InfluxDb collector port (8086 by default) influx_port: 8086 influx_username: influx_password: influx_database: ;---------------------------------------------","title":"Data Server Parameters"},{"location":"probe_configure/#network-tests","text":"Note that all network tests are enabled by default, apart from the iperf3 and SMB tests. If there are some tests you'd like to disable (e.g. if you don't want to run HTTP tests), then you'll need to open up the config.ini file and look through each section for the \"enabled\" parameter for that test and set it to \"no\". For example, to disable the HTTP tests: ; ==================================================================== ; HTTP tests settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [HTTP_test] ; yes = enabled, no = disabled enabled: no For a full description of the configuration file parameters, please review the following page: config.ini reference guide .","title":"Network Tests"},{"location":"probe_configure/#running-regular-tests","text":"Once the wiperf software has been configured, the final job is to configure a 'cron job' on the probe to run the wiperf testing script every 5 minutes. Cron is a scheduler utility within Linux that will run a software task at configured intervals. ( Note: This step is not required on the WLAN Pi, as the cron job is added automatically when the WLAN Pi is switched in to wiperf mode ) To configure cron (remember, only required on the RPi), on the CLI of the probe, open the cron editor: sudo crontab -e Next, with the editor open, add following line to the open file: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 This command will run the main wiperf script to run the tests configured within config.ini at an interval of 5 minutes. It will also dump all script output to the file /var/log/wiperf_cron.log (this is a good place to look if you hit any issues with wiperf not running as expected) Attention The cron command provided above will run every 5 minutes from the top of the hour (i.e. 0, 5, 10, 15, 20...etc. minutes as the hour.) If you would like to offset the run time to be something other than the top of the hour, simply change the first digit from '0' to another number between 1 and 4. For example, changing the '0' to '1' will run the cron job at 1, 6, 11, 16...etc. minutes past the hour: # run cron at 1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56 minutes past the hour 1-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 (Note: you do not have to wait for the top of the hour for the job to run - it will run as soon as it hits the nearest configured time increment.)","title":"Running Regular Tests"},{"location":"probe_configure/#initial-probe-testing","text":"Once the cron job has been configured, the case of the RPi, or the WLAN Pi has been put in to wiperf mode, it's time to check if the probe is working as expected. To perform tests, the probe will need to be connected to a network and able to reach the data server. The easiest way to monitor the operation of the probe is to SSH in to the probe and monitor the output of the log file /var/log/wiperf_agent.log . This file is created the first time that wiperf runs. If the file is not created after 5 minutes, then check the log file /var/log/wiperf_cron.log for error messages, as something fundamental is wrong with the installation. To watch the output of /var/log/wiperf_agent.log in real-time and view activity as data is collected every 5 minutes, run the following command on the CLI of the probe (note this command will fail if wiperf has not run correctly and the file does not yet exist): tail -f /var/log/wiperf_agent.log Every 5 minutes, new log output will be seen that looks similar to this: 2020-07-11 11:47:04,214 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,215 - Probe_Log - INFO - Starting logging... 2020-07-11 11:47:04,216 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,240 - Probe_Log - INFO - Checking if we use remote cfg file... 2020-07-11 11:47:04,241 - Probe_Log - INFO - No remote cfg file confgured...using current local ini file. 2020-07-11 11:47:04,242 - Probe_Log - INFO - No lock file found. Creating lock file. 2020-07-11 11:47:04,243 - Probe_Log - INFO - ########## Network connection checks ########## 2020-07-11 11:47:05,245 - Probe_Log - INFO - Checking wireless connection is good...(layer 1 &2) 2020-07-11 11:47:05,246 - Probe_Log - INFO - Checking wireless connection available. 2020-07-11 11:47:05,355 - Probe_Log - INFO - Checking we're connected to the network (layer3) 2020-07-11 11:47:05,356 - Probe_Log - INFO - Checking we have an IP address. 2020-07-11 11:47:05,379 - Probe_Log - INFO - Checking we can do a DNS lookup to google.com 2020-07-11 11:47:05,406 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:05,430 - Probe_Log - INFO - Checked interface route to : 216.58.212.238. Result: 216.58.212.238 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:05,431 - Probe_Log - INFO - Checking we can get to the management platform... 2020-07-11 11:47:05,432 - Probe_Log - INFO - Checking we will send mgt traffic over configured interface 'lo' mode. 2020-07-11 11:47:05,455 - Probe_Log - INFO - Checked interface route to : 127.0.0.1. Result: local 127.0.0.1 dev lo src 127.0.0.1 uid 0 2020-07-11 11:47:05,456 - Probe_Log - INFO - Interface mgt interface route looks good. 2020-07-11 11:47:05,457 - Probe_Log - INFO - Checking port connection to InfluxDB server 127.0.0.1, port: 8086 2020-07-11 11:47:05,484 - Probe_Log - INFO - Port connection to server 127.0.0.1, port: 8086 checked OK. 2020-07-11 11:47:05,485 - Probe_Log - INFO - ########## Wireless Connection ########## 2020-07-11 11:47:05,486 - Probe_Log - INFO - Wireless connection data: SSID:BNL, BSSID:5C:5B:35:C8:4D:C2, Freq:5.5, Center Freq:5.51, Channel: 100, Channel Width: 40, Tx Phy rate:200.0, Rx Phy rate:135.0, Tx MCS: 0, Rx MCS: 0, RSSI:-42.0, Tx retries:187, IP address:192.168.0.48 2020-07-11 11:47:05,486 - Probe_Log - INFO - InfluxDB update: wiperf-network, source=Network Tests 2020-07-11 11:47:05,487 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:05,573 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:05,574 - Probe_Log - INFO - Connection results sent OK. 2020-07-11 11:47:05,595 - Probe_Log - INFO - ########## speedtest ########## 2020-07-11 11:47:05,597 - Probe_Log - INFO - Starting speedtest... 2020-07-11 11:47:06,599 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:06,623 - Probe_Log - INFO - Checked interface route to : 8.8.8.8. Result: 8.8.8.8 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:06,624 - Probe_Log - INFO - Speedtest in progress....please wait. 2020-07-11 11:47:28,761 - Probe_Log - INFO - ping_time: 31, download_rate: 41.56, upload_rate: 9.74, server_name: speedtest-net5.rapidswitch.co.uk:8080 2020-07-11 11:47:28,766 - Probe_Log - INFO - Speedtest ended. 2020-07-11 11:47:28,767 - Probe_Log - INFO - InfluxDB update: wiperf-speedtest, source=Speedtest 2020-07-11 11:47:28,768 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:28,858 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:28,860 - Probe_Log - INFO - Speedtest results sent OK. The output is quite verbose and detailed, but it will provide a good indication of where wiperf is having difficulties. Once wiperf is running with no issues indicated in the logs, then it's time to check for results data on your data server. Hopefully, you'll see performance data being recorded over time as the probe runs its tests and sends the results to the data server. The next step is to deploy the probe .","title":"Initial Probe Testing"},{"location":"probe_deploy/","text":"Probe Deployment Once the probe is configured and tested, it's time to deploy it out in the real world. When deploying, here are a few things to check and remember: Verify that the probe has the network connectivity that you expect once it has been deployed. The following CLI commands will help to check connectivity: Wireless NIC: iwconfig (is the probe joining the wireless network?) IP address: ifconfig (do the interfaces being used have an IP address?) Internet connectivity: ping google.com (can the probe get to the Internet, if that is expected?) Is the probe deployed in the topology you originally intended? If the environment is not as you expected and you need to use a different interface for a particular operation, make sure you have updated config.ini so that wiperf knows where to send traffic (otherwise, you may hit routing issues) Check the output of /var/log/wiperf_agent.log to make sure everything is working with no issues once deployed. If there are hitches, they will generally be highlighted in this file, with a detailed explanation of what has failed. If you run in to any deployment issues, check out the troubleshooting and FAQ sections of this site.","title":"Probe Deployment"},{"location":"probe_deploy/#probe-deployment","text":"Once the probe is configured and tested, it's time to deploy it out in the real world. When deploying, here are a few things to check and remember: Verify that the probe has the network connectivity that you expect once it has been deployed. The following CLI commands will help to check connectivity: Wireless NIC: iwconfig (is the probe joining the wireless network?) IP address: ifconfig (do the interfaces being used have an IP address?) Internet connectivity: ping google.com (can the probe get to the Internet, if that is expected?) Is the probe deployed in the topology you originally intended? If the environment is not as you expected and you need to use a different interface for a particular operation, make sure you have updated config.ini so that wiperf knows where to send traffic (otherwise, you may hit routing issues) Check the output of /var/log/wiperf_agent.log to make sure everything is working with no issues once deployed. If there are hitches, they will generally be highlighted in this file, with a detailed explanation of what has failed. If you run in to any deployment issues, check out the troubleshooting and FAQ sections of this site.","title":"Probe Deployment"},{"location":"probe_install/","text":"Probe Software Installation This section takes a look at how we install various additional required software packages on to our probe. This includes any pre-requisite software packages and the wiperf software itself. WLAN Pi Good news! If you're using a WLAN Pi (v2.x image), you already have the software required (it's part of the WLAN Pi software image). Go to the next section of this documentation site . Note New in V2.1: For SMB test support or to use Librespeed for speedtest, you will need to install a couple of optional extra packages as detailed in these reference documents: SMB test Support Librespeed Support Raspberry Pi The RPi requires a few pre-requisite Linux packages before we can install the wiperf software itself. Note that the probe must be connected to a network (via ethernet or wireless) that has access to the Internet to download the required code. You will need CLI access to the probe to perform the steps detailed below. Package Updates Before we start adding pre-requisite packages, it's always a good idea to update the existing Linux packages on our RPi to make sure we have the \"latest and greatest\". This may take a few minutes to complete as many files may be downloaded & updated, depending on when/if your RPi was last updated: # download & update latest Linux packages sudo apt-get update && sudo apt-get upgrade -y # Reboot if any new packages were installed sudo reboot Pre-requisite Packages Next, we need to install additional Linux packages that may not be included as part of the standard RPi distribution: pip3 iperf3 git curl netcat These are installed with the following CLI commands: sudo apt-get update sudo apt-get install python3-pip iperf3 git curl netcat -y sudo reboot Note New in V2.1: For SMB test support or to use Librespeed for speedtest, you will need to install a couple of optional extra packages as detailed in these reference documents: SMB test Support Librespeed Support wiperf Software Installation To install the wiperf code itself on to the RPi, execute the following CLI command: curl -s https://raw.githubusercontent.com/wifinigel/wiperf/main/setup.sh | sudo bash -s install rpi This will initiate the download and installation of a number of python packages, together with the wiperf code itself. This will take a few minutes to complete. Once installation is complete, our final step is to configure the wiperf probe to perform the tests we'd like to run, and provide details of where the probe needs to send its data (i.e. our data server).","title":"Probe Software Installation"},{"location":"probe_install/#probe-software-installation","text":"This section takes a look at how we install various additional required software packages on to our probe. This includes any pre-requisite software packages and the wiperf software itself.","title":"Probe Software Installation"},{"location":"probe_install/#wlan-pi","text":"Good news! If you're using a WLAN Pi (v2.x image), you already have the software required (it's part of the WLAN Pi software image). Go to the next section of this documentation site . Note New in V2.1: For SMB test support or to use Librespeed for speedtest, you will need to install a couple of optional extra packages as detailed in these reference documents: SMB test Support Librespeed Support","title":"WLAN Pi"},{"location":"probe_install/#raspberry-pi","text":"The RPi requires a few pre-requisite Linux packages before we can install the wiperf software itself. Note that the probe must be connected to a network (via ethernet or wireless) that has access to the Internet to download the required code. You will need CLI access to the probe to perform the steps detailed below.","title":"Raspberry Pi"},{"location":"probe_install/#package-updates","text":"Before we start adding pre-requisite packages, it's always a good idea to update the existing Linux packages on our RPi to make sure we have the \"latest and greatest\". This may take a few minutes to complete as many files may be downloaded & updated, depending on when/if your RPi was last updated: # download & update latest Linux packages sudo apt-get update && sudo apt-get upgrade -y # Reboot if any new packages were installed sudo reboot","title":"Package Updates"},{"location":"probe_install/#pre-requisite-packages","text":"Next, we need to install additional Linux packages that may not be included as part of the standard RPi distribution: pip3 iperf3 git curl netcat These are installed with the following CLI commands: sudo apt-get update sudo apt-get install python3-pip iperf3 git curl netcat -y sudo reboot Note New in V2.1: For SMB test support or to use Librespeed for speedtest, you will need to install a couple of optional extra packages as detailed in these reference documents: SMB test Support Librespeed Support","title":"Pre-requisite Packages"},{"location":"probe_install/#wiperf-software-installation","text":"To install the wiperf code itself on to the RPi, execute the following CLI command: curl -s https://raw.githubusercontent.com/wifinigel/wiperf/main/setup.sh | sudo bash -s install rpi This will initiate the download and installation of a number of python packages, together with the wiperf code itself. This will take a few minutes to complete. Once installation is complete, our final step is to configure the wiperf probe to perform the tests we'd like to run, and provide details of where the probe needs to send its data (i.e. our data server).","title":"wiperf Software Installation"},{"location":"probe_platform/","text":"Probe Platform Wiperf has been primarily designed to work on the NEO2 version of the WLAN Pi platform and the Raspberry Pi platforms. WLAN Pi Wiperf is baked in to the image of the WLAN Pi. It can be activated by switching in to wiperf mode on the WLAN Pi. Find out more details at the official documentation site for the WLAN Pi: https://wlan-pi.github.io/wlanpi-documentation/ Raspberry Pi Wiperf on the RPi has been tested on models that have an internal Wi-Fi NIC: 3b+, 3a+ and 4. It will likely work on most that have an internal NIC, but I don't have the resources or time to try them all. Earlier versions of the RPi that do not have a an internal NIC will need some type of USB wireless adapter, but as support for external wireless NICs is very poor and many tend to be 2.4GHz only, I've not explored this area in detail. Unfortunately, getting a 2 stream 802.11ac NIC going seems very difficult due to the lack of drivers available, so the internal, single stream NIC is often best we can generally do. However, the situation does seem to be improving with more recent kernels, so test out a few NICs and see how you do. Using a single stream NIC has its limitations as speed performance is very limited, but as the main aim of wiperf is to monitor user experience (particularly changes in that experience), then it's good enough for many use-cases where we are mainly interested in changes in relation to the usual baseline. Note: Please use an RPi platform that is used only as a dedicated wiperf probe. Please do not install additional packages other than those recommended. Also, please use only one active Ethernet interface and one active wireless interface (as shown in the notes on this site). Multiple additional live adapters will likely cause operational issues. Other Platforms In essence, wiperf is a series of python scripts & modules, together with a few supporting bash scripts to glue a few things together. It will likely work on other Debian-type systems, so it's worth giving it a go on other systems if you fancy tinkering around on another platform. When using the install script, install using the 'rpi' option. Let me know if you get it going on other platforms, as it will be interesting to share your experiences.","title":"Probe Platform"},{"location":"probe_platform/#probe-platform","text":"Wiperf has been primarily designed to work on the NEO2 version of the WLAN Pi platform and the Raspberry Pi platforms.","title":"Probe Platform"},{"location":"probe_platform/#wlan-pi","text":"Wiperf is baked in to the image of the WLAN Pi. It can be activated by switching in to wiperf mode on the WLAN Pi. Find out more details at the official documentation site for the WLAN Pi: https://wlan-pi.github.io/wlanpi-documentation/","title":"WLAN Pi"},{"location":"probe_platform/#raspberry-pi","text":"Wiperf on the RPi has been tested on models that have an internal Wi-Fi NIC: 3b+, 3a+ and 4. It will likely work on most that have an internal NIC, but I don't have the resources or time to try them all. Earlier versions of the RPi that do not have a an internal NIC will need some type of USB wireless adapter, but as support for external wireless NICs is very poor and many tend to be 2.4GHz only, I've not explored this area in detail. Unfortunately, getting a 2 stream 802.11ac NIC going seems very difficult due to the lack of drivers available, so the internal, single stream NIC is often best we can generally do. However, the situation does seem to be improving with more recent kernels, so test out a few NICs and see how you do. Using a single stream NIC has its limitations as speed performance is very limited, but as the main aim of wiperf is to monitor user experience (particularly changes in that experience), then it's good enough for many use-cases where we are mainly interested in changes in relation to the usual baseline. Note: Please use an RPi platform that is used only as a dedicated wiperf probe. Please do not install additional packages other than those recommended. Also, please use only one active Ethernet interface and one active wireless interface (as shown in the notes on this site). Multiple additional live adapters will likely cause operational issues.","title":"Raspberry Pi"},{"location":"probe_platform/#other-platforms","text":"In essence, wiperf is a series of python scripts & modules, together with a few supporting bash scripts to glue a few things together. It will likely work on other Debian-type systems, so it's worth giving it a go on other systems if you fancy tinkering around on another platform. When using the install script, install using the 'rpi' option. Let me know if you get it going on other platforms, as it will be interesting to share your experiences.","title":"Other Platforms"},{"location":"probe_prepare/","text":"Probe Preparation The wiperf probe needs to have a few pre-requisite activities completed prior to the installation of wiperf code. These vary slightly between the WLAN Pi and RPi platforms, but broadly break down as: Software image preparation Obtain CLI Access Configure the device hostname Configure network connectivity Add pre-requisite software packages. Choose the Instructions for your probe type: Go to WLAN Pi instructions Go to Raspberry Pi instructions WLAN Pi Software Image There is little to do in terms of software image preparation for the WLAN Pi. Visit the WLAN Pi documentation site to find out how to obtain the WLAN Pi image: link . If you install a WLAN Pi image, wiperf will already be installed as part of the image. (Note: all information provided below assumes you are using a 2.0 or later version of the WLAN Pi image ) Probe CLI Access To perform some of the configuration activities required, CLI access to the WLAN Pi is required. The easiest way to achieve this is to SSH to the probe over an OTG connection, or plug the WLAN Pi in to an ethernet network port and SSH to its DHCP assigned IP address (shown on the front panel). Visit the WLAN Pi documentation site for more details of how to gain access to the WLAN Pi: link Hostname Configuration By default, the hostname of your WLAN Pi is : wlanpi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change to a more meaningful hostname, then you will need to SSH to your WLAN Pi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the WLAN Pi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'wlanpi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Next, edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'wlanpi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your WLAN Pi: sudo reboot Network Connectivity Ethernet If the probe is to be connected to Ethernet only, then there is no additional configuration required. By default, if a switch port that can supply a DHCP address is used, the probe will have the required network connectivity. Wireless Configuration (wpa_supplicant.conf) If wiperf is running in wireless mode, when the WLAN Pi is flipped in to wiperf mode, it will need to join the SSID under test to run the configured network tests. We need to provide a configuration (that is only used in wiperf mode) to allow the WLAN Pi to join a WLAN. Edit the following file with the configuration and credentials that will be used by the WLAN Pi to join the SSID under test once it is switched in to wiperf mode: cd /etc/wiperf/conf/etc/wpa_supplicant sudo nano ./wpa_supplicant.conf There are a number of sample configurations included in the default file provided (PSK, PEAP & Open auth). Uncomment the required section and add in the correct SSID & authentication details. (For EAP-TLS, it's time to check-out Google as I've not had opportunity to figure that scenario out...) ( Note: This wireless configuration is only used when the WLAN Pi is flipped in to wiperf mode, not for standard (classic mode) wireless connectivity ) Note If you'd like to fix the AP that the probe associates with, check out this note At this point, the pre-requisite activities for the WLAN Pi are complete. Next, move on to the probe configuration . Raspberry Pi Software Image I would strongly recommend starting with a fresh image using the latest and greatest Raspberry Pi OS (previously called Raspbian): https://www.raspberrypi.org/downloads/raspberry-pi-os/ . I would also recommend that you use the \"headless\", Lite version of Raspberry Pi OS rather than the desktop version (this is mainly as I have not tested with the desktop version and am not sure if there will be any resource or package conflict issues.) Note: A Python version of 3.6 or greater is required to wiperf. The Python version installed as part of the distribution you are using used must be Python 3.6 or higher (check with python3 -V ). Going with a recent RPi image is strongly advised to ensure this requirement is met Note: Please use a dedicated RPi platform that is used only as a probe. Please do not install additional packages other than those recommended. Also, please use only one active Ethernet interface and one active wireless interface. Multiple live adapters will likely cause operational issues. For the development and testing of the wiperf code, version 10 (Buster) was used. You can check the version on your RPi using the cat /etc/os-release command. Here is my sample output: pi@probe7:~$ cat /etc/os-release PRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\" NAME=\"Raspbian GNU/Linux\" VERSION_ID=\"10\" VERSION=\"10 (buster)\" VERSION_CODENAME=buster ID=raspbian ID_LIKE=debian HOME_URL=\"http://www.raspbian.org/\" SUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\" BUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\" Note that you will likely be able to use any recent version, so don't feel compelled to use this exact version. The download page provided above also has links to resources to guide you through how to burn the fresh image on to your SD card. ( You may also like to check out the 'Probe CLI Access' section below to setup SSH access to your headless RPI before booting from your new image ) Once you have burned your image, I'd also recommend you apply all latest updates & give it a reboot for good measure: sudo apt-get update && sudo apt-get upgrade sudo reboot Probe CLI Access You will need CLI access to perform the required configuration steps for wiperf. There are a number of ways of gaining this access that are detailed in this document: https://www.raspberrypi.org/documentation/remote-access/ssh/ (see the section \"Enable SSH on a headless Raspberry Pi\" ). My personal favourite is to enable SSH on a headless RPi by adding an 'ssh' file to the SD card prior to boot-up (see section \"Enable SSH on a headless Raspberry Pi\" in the link above). Default Login Account Password If using a fresh RPI image (which is recommended), remember to update the default 'pi' username with a new password so that your are not running with the default login of : pi/raspberry (user/pwd) Change password : sudo passwd pi Set Country Code If you're starting with a freshly burned image for your RPi, the country code for your internal Wi-Fi adapter needs to be configured before it will activate. To configure the country code. Enter the following on the RPi CLI: sudo raspi-config A textual menu system will open and the following options need to be selected to set the country code: 4. Localization Options > I4 Change WLAN Country > <Select Country> > OK > Finish (Note that if this step is not completed, your wireless adapter will likely not work) Hostname Configuration By default, the hostname of your RPi is : raspberrypi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change this to a more meaningful hostname, then you will need to SSH to your RPi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the RPi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'raspberrypi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Next, edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'raspberrypi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your RPi: sudo reboot Network Connectivity Note: Use the method below to configure network interfaces. DO NOT use the RPI desktop GUI to configure network connectivity (if using the desktop RPi image, which is not recommended anyhow)....it will definitely not work if configured via the GUI network utility. Ethernet If the RPi is to be connected by Ethernet you will need to make some additions to the /etc/network/interfaces file to ensure you have network connectivity. Add the following lines to configure the Ethernet port for DHCP connectivity (unless they already exist in the file): # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp These lines may be added anywhere in the file, using a CLI editor such as nano: sudo nano /etc/network/interfaces Wireless Configuration The RPi needs to be configured to join the wireless network that you'd like to test (assuming you want to test over a wireless connection - omit this step if you are testing wired only). To join a network, we need to configure the wireless interface and provide the network credentials to join the network. To achieve this, we need to edit two files via the CLI of the RPI: /etc/wpa_supplicant/wpa_supplicant.conf /etc/network/interfaces Sample configurations for both files are provided below. /etc/network/interfaces # configure the interfaces file sudo nano /etc/network/interfaces Sample config: # wiperf interface config file # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp # Wireless adapter #1 allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf # wireless-power off # post-up iw dev wlan0 set power_save off Note: The wireless power off commands are commented out in the file above. One of these generally needs to be uncommented to stop the wireless NIC dropping in to power save mode. If you see huge drops in the wireless connection speed in the wireless connection graph, it is being caused by power save mode. Unfortunately, the command to use seems to vary between RPi model and operating system version. When you see the connection speed issue, try uncommenting one of the commands and reboot. If it doesn't fix the issue, try the other command. (see this article for more info ) /etc/wpa_supplicant/wpa_supplicant.conf Editing this file will provide the credentials required to join the wireless network under test: # edit wpa_supplicant.conf file sudo nano /etc/wpa_supplicant/wpa_supplicant.conf Sample config: ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 # Note the country code below will be likey be different, depending on what was set using raspi-config country=GB ap_scan=1 # WPA2 PSK Network sample (highest priority - joined first) network={ ssid=\"enter SSID Name\" psk=\"enter key\" priority=10 } ####################################################################################### # NOTE: to use the templates below, remove the hash symbols at the start of each line ####################################################################################### # WPA2 PSK Network sample (next priority - joined if first priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # psk=\"enter key\" # priority=3 #} # WPA2 PEAP example (next priority - joined if second priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=WPA-EAP # eap=PEAP # anonymous_identity=\"anonymous\" # identity=\"enter your username\" # password=\"enter your password\" # phase2=\"autheap=MSCHAPV2\" # priority=2 #} # Open network example (lowest priority, only joined other 3 networks not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=NONE # priority=1 #} Note that the file includes several samples for a variety of security methods. You will need to uncomment the network section that corresponds to the security method for your environment and comment out all other methods. By default, the PSK method is used (and uncommented), but requires that you enter an SSID and shared key. Test Wireless Connection Once configuration is complete, reboot the RPI: sudo reboot To test if the wireless connection has come up OK, use the following commands to see if wireless interface has joined the wireless network and has an IP address (checkout the info for wlan0 - does it show an ESSID name (your network) and have an IP address?): sudo iwconfig sudo ifconfig Note If you'd like to fix the AP that the probe associates with, check out this note Next, we need to install a few software packages on to the RPi probe .","title":"Probe Preparation"},{"location":"probe_prepare/#probe-preparation","text":"The wiperf probe needs to have a few pre-requisite activities completed prior to the installation of wiperf code. These vary slightly between the WLAN Pi and RPi platforms, but broadly break down as: Software image preparation Obtain CLI Access Configure the device hostname Configure network connectivity Add pre-requisite software packages. Choose the Instructions for your probe type: Go to WLAN Pi instructions Go to Raspberry Pi instructions","title":"Probe Preparation"},{"location":"probe_prepare/#wlan-pi","text":"","title":"WLAN Pi"},{"location":"probe_prepare/#software-image","text":"There is little to do in terms of software image preparation for the WLAN Pi. Visit the WLAN Pi documentation site to find out how to obtain the WLAN Pi image: link . If you install a WLAN Pi image, wiperf will already be installed as part of the image. (Note: all information provided below assumes you are using a 2.0 or later version of the WLAN Pi image )","title":"Software Image"},{"location":"probe_prepare/#probe-cli-access","text":"To perform some of the configuration activities required, CLI access to the WLAN Pi is required. The easiest way to achieve this is to SSH to the probe over an OTG connection, or plug the WLAN Pi in to an ethernet network port and SSH to its DHCP assigned IP address (shown on the front panel). Visit the WLAN Pi documentation site for more details of how to gain access to the WLAN Pi: link","title":"Probe CLI Access"},{"location":"probe_prepare/#hostname-configuration","text":"By default, the hostname of your WLAN Pi is : wlanpi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change to a more meaningful hostname, then you will need to SSH to your WLAN Pi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the WLAN Pi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'wlanpi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Next, edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'wlanpi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your WLAN Pi: sudo reboot","title":"Hostname Configuration"},{"location":"probe_prepare/#network-connectivity","text":"","title":"Network Connectivity"},{"location":"probe_prepare/#ethernet","text":"If the probe is to be connected to Ethernet only, then there is no additional configuration required. By default, if a switch port that can supply a DHCP address is used, the probe will have the required network connectivity.","title":"Ethernet"},{"location":"probe_prepare/#wireless-configuration-wpa_supplicantconf","text":"If wiperf is running in wireless mode, when the WLAN Pi is flipped in to wiperf mode, it will need to join the SSID under test to run the configured network tests. We need to provide a configuration (that is only used in wiperf mode) to allow the WLAN Pi to join a WLAN. Edit the following file with the configuration and credentials that will be used by the WLAN Pi to join the SSID under test once it is switched in to wiperf mode: cd /etc/wiperf/conf/etc/wpa_supplicant sudo nano ./wpa_supplicant.conf There are a number of sample configurations included in the default file provided (PSK, PEAP & Open auth). Uncomment the required section and add in the correct SSID & authentication details. (For EAP-TLS, it's time to check-out Google as I've not had opportunity to figure that scenario out...) ( Note: This wireless configuration is only used when the WLAN Pi is flipped in to wiperf mode, not for standard (classic mode) wireless connectivity ) Note If you'd like to fix the AP that the probe associates with, check out this note At this point, the pre-requisite activities for the WLAN Pi are complete. Next, move on to the probe configuration .","title":"Wireless Configuration (wpa_supplicant.conf)"},{"location":"probe_prepare/#raspberry-pi","text":"","title":"Raspberry Pi"},{"location":"probe_prepare/#software-image_1","text":"I would strongly recommend starting with a fresh image using the latest and greatest Raspberry Pi OS (previously called Raspbian): https://www.raspberrypi.org/downloads/raspberry-pi-os/ . I would also recommend that you use the \"headless\", Lite version of Raspberry Pi OS rather than the desktop version (this is mainly as I have not tested with the desktop version and am not sure if there will be any resource or package conflict issues.) Note: A Python version of 3.6 or greater is required to wiperf. The Python version installed as part of the distribution you are using used must be Python 3.6 or higher (check with python3 -V ). Going with a recent RPi image is strongly advised to ensure this requirement is met Note: Please use a dedicated RPi platform that is used only as a probe. Please do not install additional packages other than those recommended. Also, please use only one active Ethernet interface and one active wireless interface. Multiple live adapters will likely cause operational issues. For the development and testing of the wiperf code, version 10 (Buster) was used. You can check the version on your RPi using the cat /etc/os-release command. Here is my sample output: pi@probe7:~$ cat /etc/os-release PRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\" NAME=\"Raspbian GNU/Linux\" VERSION_ID=\"10\" VERSION=\"10 (buster)\" VERSION_CODENAME=buster ID=raspbian ID_LIKE=debian HOME_URL=\"http://www.raspbian.org/\" SUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\" BUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\" Note that you will likely be able to use any recent version, so don't feel compelled to use this exact version. The download page provided above also has links to resources to guide you through how to burn the fresh image on to your SD card. ( You may also like to check out the 'Probe CLI Access' section below to setup SSH access to your headless RPI before booting from your new image ) Once you have burned your image, I'd also recommend you apply all latest updates & give it a reboot for good measure: sudo apt-get update && sudo apt-get upgrade sudo reboot","title":"Software Image"},{"location":"probe_prepare/#probe-cli-access_1","text":"You will need CLI access to perform the required configuration steps for wiperf. There are a number of ways of gaining this access that are detailed in this document: https://www.raspberrypi.org/documentation/remote-access/ssh/ (see the section \"Enable SSH on a headless Raspberry Pi\" ). My personal favourite is to enable SSH on a headless RPi by adding an 'ssh' file to the SD card prior to boot-up (see section \"Enable SSH on a headless Raspberry Pi\" in the link above).","title":"Probe CLI Access"},{"location":"probe_prepare/#default-login-account-password","text":"If using a fresh RPI image (which is recommended), remember to update the default 'pi' username with a new password so that your are not running with the default login of : pi/raspberry (user/pwd) Change password : sudo passwd pi","title":"Default Login Account Password"},{"location":"probe_prepare/#set-country-code","text":"If you're starting with a freshly burned image for your RPi, the country code for your internal Wi-Fi adapter needs to be configured before it will activate. To configure the country code. Enter the following on the RPi CLI: sudo raspi-config A textual menu system will open and the following options need to be selected to set the country code: 4. Localization Options > I4 Change WLAN Country > <Select Country> > OK > Finish (Note that if this step is not completed, your wireless adapter will likely not work)","title":"Set Country Code"},{"location":"probe_prepare/#hostname-configuration_1","text":"By default, the hostname of your RPi is : raspberrypi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change this to a more meaningful hostname, then you will need to SSH to your RPi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the RPi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'raspberrypi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Next, edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'raspberrypi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your RPi: sudo reboot","title":"Hostname Configuration"},{"location":"probe_prepare/#network-connectivity_1","text":"Note: Use the method below to configure network interfaces. DO NOT use the RPI desktop GUI to configure network connectivity (if using the desktop RPi image, which is not recommended anyhow)....it will definitely not work if configured via the GUI network utility.","title":"Network Connectivity"},{"location":"probe_prepare/#ethernet_1","text":"If the RPi is to be connected by Ethernet you will need to make some additions to the /etc/network/interfaces file to ensure you have network connectivity. Add the following lines to configure the Ethernet port for DHCP connectivity (unless they already exist in the file): # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp These lines may be added anywhere in the file, using a CLI editor such as nano: sudo nano /etc/network/interfaces","title":"Ethernet"},{"location":"probe_prepare/#wireless-configuration","text":"The RPi needs to be configured to join the wireless network that you'd like to test (assuming you want to test over a wireless connection - omit this step if you are testing wired only). To join a network, we need to configure the wireless interface and provide the network credentials to join the network. To achieve this, we need to edit two files via the CLI of the RPI: /etc/wpa_supplicant/wpa_supplicant.conf /etc/network/interfaces Sample configurations for both files are provided below.","title":"Wireless Configuration"},{"location":"probe_prepare/#etcnetworkinterfaces","text":"# configure the interfaces file sudo nano /etc/network/interfaces Sample config: # wiperf interface config file # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp # Wireless adapter #1 allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf # wireless-power off # post-up iw dev wlan0 set power_save off Note: The wireless power off commands are commented out in the file above. One of these generally needs to be uncommented to stop the wireless NIC dropping in to power save mode. If you see huge drops in the wireless connection speed in the wireless connection graph, it is being caused by power save mode. Unfortunately, the command to use seems to vary between RPi model and operating system version. When you see the connection speed issue, try uncommenting one of the commands and reboot. If it doesn't fix the issue, try the other command. (see this article for more info )","title":"/etc/network/interfaces"},{"location":"probe_prepare/#etcwpa_supplicantwpa_supplicantconf","text":"Editing this file will provide the credentials required to join the wireless network under test: # edit wpa_supplicant.conf file sudo nano /etc/wpa_supplicant/wpa_supplicant.conf Sample config: ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 # Note the country code below will be likey be different, depending on what was set using raspi-config country=GB ap_scan=1 # WPA2 PSK Network sample (highest priority - joined first) network={ ssid=\"enter SSID Name\" psk=\"enter key\" priority=10 } ####################################################################################### # NOTE: to use the templates below, remove the hash symbols at the start of each line ####################################################################################### # WPA2 PSK Network sample (next priority - joined if first priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # psk=\"enter key\" # priority=3 #} # WPA2 PEAP example (next priority - joined if second priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=WPA-EAP # eap=PEAP # anonymous_identity=\"anonymous\" # identity=\"enter your username\" # password=\"enter your password\" # phase2=\"autheap=MSCHAPV2\" # priority=2 #} # Open network example (lowest priority, only joined other 3 networks not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=NONE # priority=1 #} Note that the file includes several samples for a variety of security methods. You will need to uncomment the network section that corresponds to the security method for your environment and comment out all other methods. By default, the PSK method is used (and uncommented), but requires that you enter an SSID and shared key.","title":"/etc/wpa_supplicant/wpa_supplicant.conf"},{"location":"probe_prepare/#test-wireless-connection","text":"Once configuration is complete, reboot the RPI: sudo reboot To test if the wireless connection has come up OK, use the following commands to see if wireless interface has joined the wireless network and has an IP address (checkout the info for wlan0 - does it show an ESSID name (your network) and have an IP address?): sudo iwconfig sudo ifconfig Note If you'd like to fix the AP that the probe associates with, check out this note Next, we need to install a few software packages on to the RPi probe .","title":"Test Wireless Connection"},{"location":"probe_upgrade/","text":"Probe Software Upgrade Periodically, new versions of wiperf code may be available to add bug-fixes and new features. The wiperf code must be upgraded from the CLI of the probe. For the WLAN Pi, it is generally recommend to use the version shipped with the WLAN Pi image, but there may be instances when there is a need to upgrade the wiperf code (e.g. in the case of a bug) before the next WLAN Pi image upgrade. The instructions for each probe type are provided below. Instructions are also provided for upgrading to the latest 'dev' release. This code will generally be \"bleeding edge\", so it's best to avoid this option unless directed by the code developers. WLAN Pi Upgrade To Latest Stable Release Attention Make sure your WLAN Pi is in \"Classic\" mode before upgrading (not wiperf mode) To perform an upgrade of wiperf, execute the following commands on the WLAN Pi CLI: # perform the upgrade (probe must be connected to Internet) curl -s https://raw.githubusercontent.com/wifinigel/wiperf/main/setup.sh | sudo bash -s upgrade wlanpi Attention Ensure that you re-create your config.ini file using the new config.default.ini file supplied during the upgrade Upgrade To Latest Dev Release Execute the following commands on the WLAN Pi CLI: # perform the upgrade (probe must be connected to Internet) curl -s https://raw.githubusercontent.com/wifinigel/wiperf/dev/setup.sh | sudo bash -s upgrade wlanpi Raspberry Pi Upgrade To Latest Stable Release Execute the following commands on the RPi CLI: # check the curent installed version and latest available version: sudo /usr/share/wiperf/setup.sh check_ver # perform the upgrade (probe must be connected to Internet) curl -s https://raw.githubusercontent.com/wifinigel/wiperf/main/setup.sh | sudo bash -s upgrade rpi Attention Ensure that you re-create your config.ini file using the new config.default.ini file supplied during the upgrade Upgrade To Latest Dev Release Execute the following commands on the RPi CLI: # perform the upgrade (probe must be connected to Internet) curl -s https://raw.githubusercontent.com/wifinigel/wiperf/dev/setup.sh | sudo bash -s upgrade rpi","title":"Probe Software Upgrade"},{"location":"probe_upgrade/#probe-software-upgrade","text":"Periodically, new versions of wiperf code may be available to add bug-fixes and new features. The wiperf code must be upgraded from the CLI of the probe. For the WLAN Pi, it is generally recommend to use the version shipped with the WLAN Pi image, but there may be instances when there is a need to upgrade the wiperf code (e.g. in the case of a bug) before the next WLAN Pi image upgrade. The instructions for each probe type are provided below. Instructions are also provided for upgrading to the latest 'dev' release. This code will generally be \"bleeding edge\", so it's best to avoid this option unless directed by the code developers.","title":"Probe Software Upgrade"},{"location":"probe_upgrade/#wlan-pi","text":"","title":"WLAN Pi"},{"location":"probe_upgrade/#upgrade-to-latest-stable-release","text":"Attention Make sure your WLAN Pi is in \"Classic\" mode before upgrading (not wiperf mode) To perform an upgrade of wiperf, execute the following commands on the WLAN Pi CLI: # perform the upgrade (probe must be connected to Internet) curl -s https://raw.githubusercontent.com/wifinigel/wiperf/main/setup.sh | sudo bash -s upgrade wlanpi Attention Ensure that you re-create your config.ini file using the new config.default.ini file supplied during the upgrade","title":"Upgrade To Latest Stable Release"},{"location":"probe_upgrade/#upgrade-to-latest-dev-release","text":"Execute the following commands on the WLAN Pi CLI: # perform the upgrade (probe must be connected to Internet) curl -s https://raw.githubusercontent.com/wifinigel/wiperf/dev/setup.sh | sudo bash -s upgrade wlanpi","title":"Upgrade To Latest Dev Release"},{"location":"probe_upgrade/#raspberry-pi","text":"","title":"Raspberry Pi"},{"location":"probe_upgrade/#upgrade-to-latest-stable-release_1","text":"Execute the following commands on the RPi CLI: # check the curent installed version and latest available version: sudo /usr/share/wiperf/setup.sh check_ver # perform the upgrade (probe must be connected to Internet) curl -s https://raw.githubusercontent.com/wifinigel/wiperf/main/setup.sh | sudo bash -s upgrade rpi Attention Ensure that you re-create your config.ini file using the new config.default.ini file supplied during the upgrade","title":"Upgrade To Latest Stable Release"},{"location":"probe_upgrade/#upgrade-to-latest-dev-release_1","text":"Execute the following commands on the RPi CLI: # perform the upgrade (probe must be connected to Internet) curl -s https://raw.githubusercontent.com/wifinigel/wiperf/dev/setup.sh | sudo bash -s upgrade rpi","title":"Upgrade To Latest Dev Release"},{"location":"quickstart/","text":"Quickstart Guide Wiperf is a utility that can be installed on a WLAN Pi or a Raspberry Pi to act as a network probe that runs a series of network performance tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as an ethernet-connected probe. Wiperf is a powerful solution, but it is not trivial to setup and configure. Here is a quickstart guide to outline the component parts and knowledge you'll need to get wiperf going. What it is: an open source engineering tool that runs a set of network tests and reports in to a separate reporting tool (Splunk or Grafana) that is not part of the wiperf project. Wiperf is only a probe that runs a series of network tests and makes data available to other tools. What it isn't: An Enterprise quality/scale network monitoring tool A reporting tool A cheap way of providing Enterprise-wide UX monitoring Skills You'll Need: You will need to be comfortable with the following items to get wiperf going: Using SBC devices like the RPi and WLAN Pi (including burning images & getting on their CLI) Basic Linux administration skills (including editing files on the CLI, running scripts, updating/adding packages) Building a Probe: You'll need to build the probe from scratch, which may include burning images on to an SD card, a variety of Linux operations such as adding new packages and performing administrative tasks such as setting the hostname, network configuration details and the timezone. Once the basic probe is built, you will then download the wiperf code using an automated script and configure its operation using a CLI text editor. Reporting: As stated previously, wiperf is not a reporting platform. It has been designed to send data to a reporting platform (that you build yourself, separately) such as Splunk or Grafana. Details are provided to \"get you going\" with the reporting platform, but these platforms are not part of this project and detailed customization of these platforms and supporting them is outside of the wiperf project. No support will be provided for these platforms besides the basic report templates that are provided on a best-efforts basis. Remember, wiperf is a UX network probe - analyzing the data it provides is your responsibility. Documentation: I have created a wide range of documentation to help you to get your wiperf probe going, together with the basics of setting up a suitable reporting platform. Please take time to read through this documentation before reaching out for support. I recommend that you start here and work sequentially though the documentation provided .","title":"Quickstart"},{"location":"quickstart/#quickstart-guide","text":"Wiperf is a utility that can be installed on a WLAN Pi or a Raspberry Pi to act as a network probe that runs a series of network performance tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as an ethernet-connected probe. Wiperf is a powerful solution, but it is not trivial to setup and configure. Here is a quickstart guide to outline the component parts and knowledge you'll need to get wiperf going. What it is: an open source engineering tool that runs a set of network tests and reports in to a separate reporting tool (Splunk or Grafana) that is not part of the wiperf project. Wiperf is only a probe that runs a series of network tests and makes data available to other tools. What it isn't: An Enterprise quality/scale network monitoring tool A reporting tool A cheap way of providing Enterprise-wide UX monitoring Skills You'll Need: You will need to be comfortable with the following items to get wiperf going: Using SBC devices like the RPi and WLAN Pi (including burning images & getting on their CLI) Basic Linux administration skills (including editing files on the CLI, running scripts, updating/adding packages) Building a Probe: You'll need to build the probe from scratch, which may include burning images on to an SD card, a variety of Linux operations such as adding new packages and performing administrative tasks such as setting the hostname, network configuration details and the timezone. Once the basic probe is built, you will then download the wiperf code using an automated script and configure its operation using a CLI text editor. Reporting: As stated previously, wiperf is not a reporting platform. It has been designed to send data to a reporting platform (that you build yourself, separately) such as Splunk or Grafana. Details are provided to \"get you going\" with the reporting platform, but these platforms are not part of this project and detailed customization of these platforms and supporting them is outside of the wiperf project. No support will be provided for these platforms besides the basic report templates that are provided on a best-efforts basis. Remember, wiperf is a UX network probe - analyzing the data it provides is your responsibility. Documentation: I have created a wide range of documentation to help you to get your wiperf probe going, together with the basics of setting up a suitable reporting platform. Please take time to read through this documentation before reaching out for support. I recommend that you start here and work sequentially though the documentation provided .","title":"Quickstart Guide"},{"location":"reference_doc_caching/","text":"Results Data Caching Note New in V2.1 There were a number of requests from folks for results data to be made available on the local file system of the wiperf probe, in addition to being forwarded to a reporting platform. To meet these requests, results caching has been implemented. This feature is disabled by default, but when enabled all test results are stored on the local file system in either CSV or JSON format. To limit the amount of local file storage consumed, a maximum age limit is configured to age-out older date and prevent the local file system filling up. Note that test results are still sent to the configured reporting platform when caching is enabled. The data files are stored in the local directory /var/cache/wiperf for a configurable period of time ( 3 days by default ). A new day-specific directory is created each day, with a data file for each test type being run (example folder listing below): root@rpi3a:/var/cache/wiperf/2021-01-15# ls -l total 40 -rw-r--r-- 1 root root 98 Jan 15 21:12 wiperf-dhcp.csv -rw-r--r-- 1 root root 279 Jan 15 21:11 wiperf-dns.csv -rw-r--r-- 1 root root 472 Jan 15 21:11 wiperf-http.csv -rw-r--r-- 1 root root 252 Jan 15 21:12 wiperf-iperf3-tcp.csv -rw-r--r-- 1 root root 255 Jan 15 21:12 wiperf-iperf3-udp.csv -rw-r--r-- 1 root root 512 Jan 15 21:11 wiperf-network.csv -rw-r--r-- 1 root root 616 Jan 15 21:11 wiperf-ping.csv -rw-r--r-- 1 root root 634 Jan 15 21:12 wiperf-poll-status.csv -rw-r--r-- 1 root root 451 Jan 15 21:12 wiperf-smb.csv -rw-r--r-- 1 root root 484 Jan 15 21:11 wiperf-speedtest.csv The day-specific folder (and all of its data) is removed once its age exceeds the defined age-out threshold. If only subset of results need to be cached, then they can be filtered using the cache_filter configuration field . Configuration This feature is enabled and configured via the usual wiperf configuration file: /etc/wiperf/config.ini . The relevant section of the file is shown below: ; ==================================================================== ; General settings - any changes to this section should only be ; made when in classic mode (not while in wiperf mode) on the WLAN Pi ; ==================================================================== [General] ; ; !!!!!!!!!!!!!!!!! SNIP !!!!!!!!!!!!!!!!!!!!!!!!!!! ; ; ----------- Caching Parameters ------------- ; Results data may be cached in the local file system ; of the probe for later inspection or retrieval by ; user defined methods. By default, files are stored ; in: /var/cache/wiperf ; ; Enable/disable local file caching (yes or no) cache_enabled: no ; Format of local cache files (csv or json) cache_data_format: csv ; Number of days of data that will be retained ; local cache files cache_retention_period: 3 ; data source filter (e.g. to cache only http & ping data: wiperf-http, wiperf-ping) cache_filter: ;--------------------------------------------- Each of the configuration parameters are detailed in the config.ini reference document . Note that this data cannot be exported to a reporting server, it is only provided for local inspection or remote retrieval by some other user-defined method. Example Output Here is a very brief sample of output data in CSV format for ping tests: root@rpi3a:/var/cache/wiperf/2021-01-15# cat wiperf-ping.csv time,ping_index,ping_host,pkts_tx,pkts_rx,percent_loss,test_time_ms,rtt_min_ms,rtt_avg_ms,rtt_max_ms,rtt_mdev_ms 1610744198240,1,google.com,10,10,0,21,16.4,22.12,35.05,5.25 1610744207526,2,cisco.com,10,10,0,22,132.56,139.05,157.93,7.84 1610744497522,1,google.com,10,10,0,22,17.2,21.42,37.68,5.86 1610744506813,2,cisco.com,10,10,0,22,132.64,155.31,297.98,47.96 1610744798459,1,google.com,10,10,0,22,18.2,22.38,33.54,4.92 1610744807743,2,cisco.com,10,10,0,22,132.23,138.41,158.16,8.22","title":"Results Data Caching"},{"location":"reference_doc_caching/#results-data-caching","text":"Note New in V2.1 There were a number of requests from folks for results data to be made available on the local file system of the wiperf probe, in addition to being forwarded to a reporting platform. To meet these requests, results caching has been implemented. This feature is disabled by default, but when enabled all test results are stored on the local file system in either CSV or JSON format. To limit the amount of local file storage consumed, a maximum age limit is configured to age-out older date and prevent the local file system filling up. Note that test results are still sent to the configured reporting platform when caching is enabled. The data files are stored in the local directory /var/cache/wiperf for a configurable period of time ( 3 days by default ). A new day-specific directory is created each day, with a data file for each test type being run (example folder listing below): root@rpi3a:/var/cache/wiperf/2021-01-15# ls -l total 40 -rw-r--r-- 1 root root 98 Jan 15 21:12 wiperf-dhcp.csv -rw-r--r-- 1 root root 279 Jan 15 21:11 wiperf-dns.csv -rw-r--r-- 1 root root 472 Jan 15 21:11 wiperf-http.csv -rw-r--r-- 1 root root 252 Jan 15 21:12 wiperf-iperf3-tcp.csv -rw-r--r-- 1 root root 255 Jan 15 21:12 wiperf-iperf3-udp.csv -rw-r--r-- 1 root root 512 Jan 15 21:11 wiperf-network.csv -rw-r--r-- 1 root root 616 Jan 15 21:11 wiperf-ping.csv -rw-r--r-- 1 root root 634 Jan 15 21:12 wiperf-poll-status.csv -rw-r--r-- 1 root root 451 Jan 15 21:12 wiperf-smb.csv -rw-r--r-- 1 root root 484 Jan 15 21:11 wiperf-speedtest.csv The day-specific folder (and all of its data) is removed once its age exceeds the defined age-out threshold. If only subset of results need to be cached, then they can be filtered using the cache_filter configuration field .","title":"Results Data Caching"},{"location":"reference_doc_caching/#configuration","text":"This feature is enabled and configured via the usual wiperf configuration file: /etc/wiperf/config.ini . The relevant section of the file is shown below: ; ==================================================================== ; General settings - any changes to this section should only be ; made when in classic mode (not while in wiperf mode) on the WLAN Pi ; ==================================================================== [General] ; ; !!!!!!!!!!!!!!!!! SNIP !!!!!!!!!!!!!!!!!!!!!!!!!!! ; ; ----------- Caching Parameters ------------- ; Results data may be cached in the local file system ; of the probe for later inspection or retrieval by ; user defined methods. By default, files are stored ; in: /var/cache/wiperf ; ; Enable/disable local file caching (yes or no) cache_enabled: no ; Format of local cache files (csv or json) cache_data_format: csv ; Number of days of data that will be retained ; local cache files cache_retention_period: 3 ; data source filter (e.g. to cache only http & ping data: wiperf-http, wiperf-ping) cache_filter: ;--------------------------------------------- Each of the configuration parameters are detailed in the config.ini reference document . Note that this data cannot be exported to a reporting server, it is only provided for local inspection or remote retrieval by some other user-defined method.","title":"Configuration"},{"location":"reference_doc_caching/#example-output","text":"Here is a very brief sample of output data in CSV format for ping tests: root@rpi3a:/var/cache/wiperf/2021-01-15# cat wiperf-ping.csv time,ping_index,ping_host,pkts_tx,pkts_rx,percent_loss,test_time_ms,rtt_min_ms,rtt_avg_ms,rtt_max_ms,rtt_mdev_ms 1610744198240,1,google.com,10,10,0,21,16.4,22.12,35.05,5.25 1610744207526,2,cisco.com,10,10,0,22,132.56,139.05,157.93,7.84 1610744497522,1,google.com,10,10,0,22,17.2,21.42,37.68,5.86 1610744506813,2,cisco.com,10,10,0,22,132.64,155.31,297.98,47.96 1610744798459,1,google.com,10,10,0,22,18.2,22.38,33.54,4.92 1610744807743,2,cisco.com,10,10,0,22,132.23,138.41,158.16,8.22","title":"Example Output"},{"location":"reference_doc_librespeed/","text":"Librespeed Speedtest Note New in V2.1 Attention Make sure you checkout the \"Known Issues\" section at the end of this page In addition to performing a speedtest using the public Ookla service, it is now possible to add support for performing speedtesting to Librespeed servers. One useful point to note is that in addition to testing against public Librespeed servers,it is also possible to test against your own instance of the Librespeed server. This allows testing within your own environment, without hitting the Internet (and associated bottlenecks) Switching between using Ookla and Librespeed is achieved by modifying the provider parameter in the [Speedtest] section of config.ini: ; ==================================================================== ; Speedtest test settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [Speedtest] ; yes = enabled, no = disabled enabled: yes ; ; Speedtest provider valid options: ookla or librespeed provider: librespeed ; ; Ookla: ; The server ID of a specific Ookla server taken from : https://c.speedtest.net/speedtest-servers-static.php ; Note this must be the number (NOT url!) taken from the field id=\"xxxxx\". If not specified, best server used (default) ; ; Librespeed: ; The numeric server ID of the server listed in the avaiable servers seen by running the Librespeed CLI ; command: librespeed-cli --list ; server_id: ; ; Additional args to pass to Librespeed CLI command (e.g. --local-json /etc/wipef/localserver.json --duration 20) - Note: Librespeed only librespeed_args: ; ; If proxy server access is required to run a speedtest, enter the proxy server details here for https & https ; e.g. https_proxy: http://10.1.1.1:8080 ; ; For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) ; e.g. no_proxy: \"mail.local, intranet.local\" http_proxy: https_proxy: no_proxy: ; ; ------------- Advanced settings for Speedtest section, do not change -------------- ; Name used for speedtest file/data group/data source speedtest_data_file: wiperf-speedtest ;------------------------------ Speedtest Section End ------------------------------- In addition to choosing the speedtest provider, it is also possible to choose the specific server ID that is to be used for testing, together with the option of passing additional arguments to the Librespeed test client to control its behaviour. See the inline comments in the config file extract above for more details. Pre-requisites - Librespeed Client The Librespeed speedtest is performed by using the client available from the client GitHub site: client binaries The Librespeed client is not installed by default on the WLAN Pi or RPi, so must be downloaded and installed before it can be selected as an option for use by wiperf. To install the client, you will need to SSH to the probe and perform the following steps on the CLI: RPi To download and install the Librespeed client on the Raspberry Pi, follow these instructions: # RPi librespeed install # go to your home dir cd ~ # get the librespeed-cli binary for RPi wget https://github.com/librespeed/speedtest-cli/releases/download/v1.0.7/librespeed-cli_1.0.7_linux_armv7.tar.gz # extract the files tar xvzf librespeed-cli_1.0.7_linux_armv7.tar.gz # change the owner of the cli utility to root sudo chown root:root librespeed-cli # copy the cli utility to its final destination sudo cp ./librespeed-cli /usr/local/bin/ # verify librespeed cli is ready to go librespeed-cli --version WLAN Pi To download and install the Librespeed client on the WLAN Pi, follow these instructions: # WLAN Pi librespeed install # go to your home dir cd ~ # get the librespeed-cli binary for RPi wget https://github.com/librespeed/speedtest-cli/releases/download/v1.0.7/librespeed-cli_1.0.7_linux_arm64.tar.gz # extract the files tar xvzf librespeed-cli_1.0.7_linux_arm64.tar.gz # change the owner of the cli utility to root sudo chown root:root librespeed-cli # copy the cli utility to its final destination sudo cp ./librespeed-cli /usr/local/bin/ # verify librespeed cli is ready to go librespeed-cli --version Known Issues There is a known issue on some platforms where librespeed will not run without specifying a specific server ID - this seems to be an issue with ping in Go (the language the client is written in). ( This has been seen on the WLAN Pi ) To see a list of servers to pick an ID to test against, run the following command to get a list of available servers: librespeed-cli --list Enter the chosen server numeric ID in the server_id: field of config.ini","title":"Librespeed Speedtest"},{"location":"reference_doc_librespeed/#librespeed-speedtest","text":"Note New in V2.1 Attention Make sure you checkout the \"Known Issues\" section at the end of this page In addition to performing a speedtest using the public Ookla service, it is now possible to add support for performing speedtesting to Librespeed servers. One useful point to note is that in addition to testing against public Librespeed servers,it is also possible to test against your own instance of the Librespeed server. This allows testing within your own environment, without hitting the Internet (and associated bottlenecks) Switching between using Ookla and Librespeed is achieved by modifying the provider parameter in the [Speedtest] section of config.ini: ; ==================================================================== ; Speedtest test settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [Speedtest] ; yes = enabled, no = disabled enabled: yes ; ; Speedtest provider valid options: ookla or librespeed provider: librespeed ; ; Ookla: ; The server ID of a specific Ookla server taken from : https://c.speedtest.net/speedtest-servers-static.php ; Note this must be the number (NOT url!) taken from the field id=\"xxxxx\". If not specified, best server used (default) ; ; Librespeed: ; The numeric server ID of the server listed in the avaiable servers seen by running the Librespeed CLI ; command: librespeed-cli --list ; server_id: ; ; Additional args to pass to Librespeed CLI command (e.g. --local-json /etc/wipef/localserver.json --duration 20) - Note: Librespeed only librespeed_args: ; ; If proxy server access is required to run a speedtest, enter the proxy server details here for https & https ; e.g. https_proxy: http://10.1.1.1:8080 ; ; For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) ; e.g. no_proxy: \"mail.local, intranet.local\" http_proxy: https_proxy: no_proxy: ; ; ------------- Advanced settings for Speedtest section, do not change -------------- ; Name used for speedtest file/data group/data source speedtest_data_file: wiperf-speedtest ;------------------------------ Speedtest Section End ------------------------------- In addition to choosing the speedtest provider, it is also possible to choose the specific server ID that is to be used for testing, together with the option of passing additional arguments to the Librespeed test client to control its behaviour. See the inline comments in the config file extract above for more details.","title":"Librespeed Speedtest"},{"location":"reference_doc_librespeed/#pre-requisites-librespeed-client","text":"The Librespeed speedtest is performed by using the client available from the client GitHub site: client binaries The Librespeed client is not installed by default on the WLAN Pi or RPi, so must be downloaded and installed before it can be selected as an option for use by wiperf. To install the client, you will need to SSH to the probe and perform the following steps on the CLI:","title":"Pre-requisites - Librespeed Client"},{"location":"reference_doc_librespeed/#rpi","text":"To download and install the Librespeed client on the Raspberry Pi, follow these instructions: # RPi librespeed install # go to your home dir cd ~ # get the librespeed-cli binary for RPi wget https://github.com/librespeed/speedtest-cli/releases/download/v1.0.7/librespeed-cli_1.0.7_linux_armv7.tar.gz # extract the files tar xvzf librespeed-cli_1.0.7_linux_armv7.tar.gz # change the owner of the cli utility to root sudo chown root:root librespeed-cli # copy the cli utility to its final destination sudo cp ./librespeed-cli /usr/local/bin/ # verify librespeed cli is ready to go librespeed-cli --version","title":"RPi"},{"location":"reference_doc_librespeed/#wlan-pi","text":"To download and install the Librespeed client on the WLAN Pi, follow these instructions: # WLAN Pi librespeed install # go to your home dir cd ~ # get the librespeed-cli binary for RPi wget https://github.com/librespeed/speedtest-cli/releases/download/v1.0.7/librespeed-cli_1.0.7_linux_arm64.tar.gz # extract the files tar xvzf librespeed-cli_1.0.7_linux_arm64.tar.gz # change the owner of the cli utility to root sudo chown root:root librespeed-cli # copy the cli utility to its final destination sudo cp ./librespeed-cli /usr/local/bin/ # verify librespeed cli is ready to go librespeed-cli --version","title":"WLAN Pi"},{"location":"reference_doc_librespeed/#known-issues","text":"There is a known issue on some platforms where librespeed will not run without specifying a specific server ID - this seems to be an issue with ping in Go (the language the client is written in). ( This has been seen on the WLAN Pi ) To see a list of servers to pick an ID to test against, run the following command to get a list of available servers: librespeed-cli --list Enter the chosen server numeric ID in the server_id: field of config.ini","title":"Known Issues"},{"location":"reference_doc_smb/","text":"SMB/CIFS Test Note New in V2.1 The SMB/CIFS benchmarking test performs a simple copy of a file from a server to the local wiperf host. This may be useful to simulate a standard Windows file copy from a server. Once each test is completed, data is reported that shows the file copied, the transfer time and the data rate achieved. Note that transfers that take longer than one minute will time out, so please consider this when setting up tests. Up to 5 tests may be performed during each wiperf poll cycle. For each test, the following information must be configured: smb_host: name of IP of he remote server smb_username: username for login credentials on remote server smb_password: password for login credentials on remote server smb_path: the shared remote server path smb_filename: the remote file to be copied during the test (See the following reference document for more detailed configuration information: config.ini Pre-requisites As the SMB/CIFS test suite is an optional test module, some prerequisite linux packages may be missing from your wiper probe software (both WLAN Pi and RPi). To ensure that you have the required packages on your wiper probe, please run the following commands: sudo apt-get update sudo apt-get install cifs-utils If you get a message advising that cifs-utils is already installed, do not worry. Configuration Once the pre-requisite packages are installed, as with all other wiperf tests, please configure the relevant section of the wiperf probe /etc/wiperf/config.ini file. Please visit this document for detail information about the configuration options: config.ini ) General SMB Testing (Supplementary Info) If you are not too familiar with using SMB/CIFS file shares and would like to set up and perform your own testing (e.g. in your lab), here are some useful notes on setting up a basic SMB server and client using some test Linux devices. Note: these are unsupported reference notes which may/may not be helpful to you. Server side (Linux server): Install samba: sudo apt-get update && sudo apt-get install samba Create samba account to access share: sudo userdd smbuser sudo smbpasswd -a smbuser Create dir to be shared (adjust this for your user account - 'wlanpi' usr account used in this example) sudo mkdir /home/wlanpi/share sudo chown wlanpi:wlanpi /home/wlanpi/share Edit smb.conf to add share: sudo nano /etc/samba/smb.conf # Add this to end of smb.conf - adjust path for your instance [shared] path = /home/wlanpi/share available = yes valid users = smbuser read only = no create mask = 0755 browsable = yes public = yes writable = yes Restart samba: systemctl restart smbd systemctl status smbd # make sure our firewall doesn't get in the way (if installed) # ** do not leave firewall disabled!!! ** ufw disable Client side (WLAN Pi/RPi): Install cifs-utils: : sudo apt-get update && sudo apt-get install cifs-utils Create local mount point: mkdir /tmp/share Mount the remote volume - adjust IP address for your remote server(s): /sbin/mount.cifs //192.168.0.52/shared /tmp/share -o user=smbuser,pass='smbuser' /sbin/mount.cifs //192.168.0.193/shared /tmp/share -o user=smbuser,pass='smbuser' Copy a file from the remote mount: /bin/cp -f /tmp/share/file.txt ~/. Unmount volume: /bin/umount /tmp/share","title":"SMB/CIFS Test"},{"location":"reference_doc_smb/#smbcifs-test","text":"Note New in V2.1 The SMB/CIFS benchmarking test performs a simple copy of a file from a server to the local wiperf host. This may be useful to simulate a standard Windows file copy from a server. Once each test is completed, data is reported that shows the file copied, the transfer time and the data rate achieved. Note that transfers that take longer than one minute will time out, so please consider this when setting up tests. Up to 5 tests may be performed during each wiperf poll cycle. For each test, the following information must be configured: smb_host: name of IP of he remote server smb_username: username for login credentials on remote server smb_password: password for login credentials on remote server smb_path: the shared remote server path smb_filename: the remote file to be copied during the test (See the following reference document for more detailed configuration information: config.ini","title":"SMB/CIFS Test"},{"location":"reference_doc_smb/#pre-requisites","text":"As the SMB/CIFS test suite is an optional test module, some prerequisite linux packages may be missing from your wiper probe software (both WLAN Pi and RPi). To ensure that you have the required packages on your wiper probe, please run the following commands: sudo apt-get update sudo apt-get install cifs-utils If you get a message advising that cifs-utils is already installed, do not worry.","title":"Pre-requisites"},{"location":"reference_doc_smb/#configuration","text":"Once the pre-requisite packages are installed, as with all other wiperf tests, please configure the relevant section of the wiperf probe /etc/wiperf/config.ini file. Please visit this document for detail information about the configuration options: config.ini )","title":"Configuration"},{"location":"reference_doc_smb/#general-smb-testing-supplementary-info","text":"If you are not too familiar with using SMB/CIFS file shares and would like to set up and perform your own testing (e.g. in your lab), here are some useful notes on setting up a basic SMB server and client using some test Linux devices. Note: these are unsupported reference notes which may/may not be helpful to you. Server side (Linux server): Install samba: sudo apt-get update && sudo apt-get install samba Create samba account to access share: sudo userdd smbuser sudo smbpasswd -a smbuser Create dir to be shared (adjust this for your user account - 'wlanpi' usr account used in this example) sudo mkdir /home/wlanpi/share sudo chown wlanpi:wlanpi /home/wlanpi/share Edit smb.conf to add share: sudo nano /etc/samba/smb.conf # Add this to end of smb.conf - adjust path for your instance [shared] path = /home/wlanpi/share available = yes valid users = smbuser read only = no create mask = 0755 browsable = yes public = yes writable = yes Restart samba: systemctl restart smbd systemctl status smbd # make sure our firewall doesn't get in the way (if installed) # ** do not leave firewall disabled!!! ** ufw disable Client side (WLAN Pi/RPi): Install cifs-utils: : sudo apt-get update && sudo apt-get install cifs-utils Create local mount point: mkdir /tmp/share Mount the remote volume - adjust IP address for your remote server(s): /sbin/mount.cifs //192.168.0.52/shared /tmp/share -o user=smbuser,pass='smbuser' /sbin/mount.cifs //192.168.0.193/shared /tmp/share -o user=smbuser,pass='smbuser' Copy a file from the remote mount: /bin/cp -f /tmp/share/file.txt ~/. Unmount volume: /bin/umount /tmp/share","title":"General SMB Testing (Supplementary Info)"},{"location":"splunk_configure/","text":"Splunk Configuration Now that we have a Splunk server setup, we need to customize it to report our probe data. The steps required are all via the Splunk web GUI and are the same for all OS flavours. Configure Data Input To Splunk We need to tell Splunk how we\u2019ll be sending the data from our probe in to Splunk. We need to configure a data input that will prepare Splunk to receive the data, and to generate an authorization key to be used by the probe when sending its results data. Log In To Splunk The first step is to login to Splunk using the credentials created during the Splunk install. The URL to use is: http://<Splunk_server_IP>:8000 Configure HTTP Event Collector Global Options After login, the following page will be seen: Follow the \u201cSettings > Data > Data Inputs\u201d menu options : Click on the HTTP Event Collector link in the Data Inputs page shown: Click on the \u201cGlobal Settings\u201d button as indicated in the graphic above to reveal the global configuration panel for the HTTP Event Collector: Ensure the panel is configured to look like the example shown above. This should require the following steps: Make sure you hit the All Tokens > Enabled button (this is disabled by default which stops everything working) Default Source Type: Structured > _json Hit: Save to take you back to the HTTP Event Collector page Create a HEC Token After returning to the HTTP Event Collector page, hit the New Token button. This will start a token creation wizard. Enter a name for the token (probe HEC Token) then hit Next > : In the next wizard panel select Source type: Select > Structured > _json : Scroll down to the indexes and make the following selections: Select Allowed Indexes > add all Default Index : main Next, hit the Review > button: The token review panel is now should and should look like the graphic below. Finally hit the Submit> button: A final confirmation message will be provided as shown below: If you return to Settings > Data Input > HTTPS Event Collector , you will now see the token your probe will need to communicate with the Splunk server: At this point, the Splunk server is ready to receive data from the probe. Ensure that your probe has been configured with the correct server IP address, port number and the token we have just created above (copy and paste the \u201cToken Value\u201d in to your probe config.ini file). If using the WLAN Pi, ensure that it is flipped in to wiperf mode. On the RPI, ensure that the required cron job has been configured to start polling. Perform a Test Search After a few minutes, when the probe has run a test cycle, data should start to appear in Splunk. The quickest way to check is to do a general search for data in Splunk and see what is being received. Go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and enter a \u201c*\u201d in the \u201cNew Search\u201d text box. Results data should be seen as shown below: If your search result looks like this (no results found message), then you need to wait a little longer for data to arrive, or there is likely a comms problem between your probe and Splunk: Create a Dashboard Now that we have data arriving at our Splunk server, we need to view the data in an interesting format. Splunk allows us to create reporting dashboards to visualize our data. We will now create a simple dashboard to demonstrate the visualization capabilities. In the probe\u2019s /usr/share/wiperf/dashboards directory, a number of pre-canned dashboard files have been provided to allow a simple copy & paste operation to create a dashboard. These files are also available on the GitHub page of the wiperf project: https://github.com/wifinigel/wiperf/tree/main/dashboards Use an SFTP client to pull the \u201c01 - probe_summary.xml\u201d file from your probe, or open the file on the GitHub page and select \u201cRaw\u201d to copy and paste the code in to a local file on your laptop. In the Splunk GUI, go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and hit the \u201cDashboards\u201d link: Hit the \"Create New Dashboard\" button: In the pop-up panel, enter a dashboard name and hit the \"Create Dashboard\" button: In the \u201cEdit Dashboard\u201d panel that opens, hit the \u201cSource\u201d button: By default, some basic XML configuration will exist in the dashboard definition: Open up the dashboard definition file previously downloaded from your probe or the GitHub site in a text editor. Then simply paste in the code as shown below (make sure the original code was all removed): After hitting the Save button, the dashboard will now be shown: Using the hostname and time period selector above the graphs, different probes and reporting periods may be viewed. The process above may be repeated using each of the xml files found in the dashboards folder to create a series of separate dashboards that focus on different aspects of data available using wiperf.","title":"Splunk Configuration"},{"location":"splunk_configure/#splunk-configuration","text":"Now that we have a Splunk server setup, we need to customize it to report our probe data. The steps required are all via the Splunk web GUI and are the same for all OS flavours.","title":"Splunk Configuration"},{"location":"splunk_configure/#configure-data-input-to-splunk","text":"We need to tell Splunk how we\u2019ll be sending the data from our probe in to Splunk. We need to configure a data input that will prepare Splunk to receive the data, and to generate an authorization key to be used by the probe when sending its results data.","title":"Configure Data Input To Splunk"},{"location":"splunk_configure/#log-in-to-splunk","text":"The first step is to login to Splunk using the credentials created during the Splunk install. The URL to use is: http://<Splunk_server_IP>:8000","title":"Log In To Splunk"},{"location":"splunk_configure/#configure-http-event-collector-global-options","text":"After login, the following page will be seen: Follow the \u201cSettings > Data > Data Inputs\u201d menu options : Click on the HTTP Event Collector link in the Data Inputs page shown: Click on the \u201cGlobal Settings\u201d button as indicated in the graphic above to reveal the global configuration panel for the HTTP Event Collector: Ensure the panel is configured to look like the example shown above. This should require the following steps: Make sure you hit the All Tokens > Enabled button (this is disabled by default which stops everything working) Default Source Type: Structured > _json Hit: Save to take you back to the HTTP Event Collector page","title":"Configure  HTTP Event Collector Global Options"},{"location":"splunk_configure/#create-a-hec-token","text":"After returning to the HTTP Event Collector page, hit the New Token button. This will start a token creation wizard. Enter a name for the token (probe HEC Token) then hit Next > : In the next wizard panel select Source type: Select > Structured > _json : Scroll down to the indexes and make the following selections: Select Allowed Indexes > add all Default Index : main Next, hit the Review > button: The token review panel is now should and should look like the graphic below. Finally hit the Submit> button: A final confirmation message will be provided as shown below: If you return to Settings > Data Input > HTTPS Event Collector , you will now see the token your probe will need to communicate with the Splunk server: At this point, the Splunk server is ready to receive data from the probe. Ensure that your probe has been configured with the correct server IP address, port number and the token we have just created above (copy and paste the \u201cToken Value\u201d in to your probe config.ini file). If using the WLAN Pi, ensure that it is flipped in to wiperf mode. On the RPI, ensure that the required cron job has been configured to start polling.","title":"Create a HEC Token"},{"location":"splunk_configure/#perform-a-test-search","text":"After a few minutes, when the probe has run a test cycle, data should start to appear in Splunk. The quickest way to check is to do a general search for data in Splunk and see what is being received. Go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and enter a \u201c*\u201d in the \u201cNew Search\u201d text box. Results data should be seen as shown below: If your search result looks like this (no results found message), then you need to wait a little longer for data to arrive, or there is likely a comms problem between your probe and Splunk:","title":"Perform a Test Search"},{"location":"splunk_configure/#create-a-dashboard","text":"Now that we have data arriving at our Splunk server, we need to view the data in an interesting format. Splunk allows us to create reporting dashboards to visualize our data. We will now create a simple dashboard to demonstrate the visualization capabilities. In the probe\u2019s /usr/share/wiperf/dashboards directory, a number of pre-canned dashboard files have been provided to allow a simple copy & paste operation to create a dashboard. These files are also available on the GitHub page of the wiperf project: https://github.com/wifinigel/wiperf/tree/main/dashboards Use an SFTP client to pull the \u201c01 - probe_summary.xml\u201d file from your probe, or open the file on the GitHub page and select \u201cRaw\u201d to copy and paste the code in to a local file on your laptop. In the Splunk GUI, go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and hit the \u201cDashboards\u201d link: Hit the \"Create New Dashboard\" button: In the pop-up panel, enter a dashboard name and hit the \"Create Dashboard\" button: In the \u201cEdit Dashboard\u201d panel that opens, hit the \u201cSource\u201d button: By default, some basic XML configuration will exist in the dashboard definition: Open up the dashboard definition file previously downloaded from your probe or the GitHub site in a text editor. Then simply paste in the code as shown below (make sure the original code was all removed): After hitting the Save button, the dashboard will now be shown: Using the hostname and time period selector above the graphs, different probes and reporting periods may be viewed. The process above may be repeated using each of the xml files found in the dashboards folder to create a series of separate dashboards that focus on different aspects of data available using wiperf.","title":"Create a Dashboard"},{"location":"splunk_install/","text":"Splunk Installation Once the software is downloaded, follow the instructions that are appropriate for your OS in the Splunk installation manual: https://docs.splunk.com/Documentation/Splunk/latest/Installation/Chooseyourplatform The installation process for all platforms is very straightforward and is detailed in the official install guides, so will not be covered in detail here. Note When installing the Linux flavour of Splunk, make sure you do not miss the additional step required to ensure that Splunk starts after a server reboot. The following command needs to be executred after the software is installed (but please verify this isn the official installation documents): sudo /opt/splunk/bin/splunk enable boot-start Once installation has been completed, it should be possible to access the web dashboard of Splunk at the URL: http://<Splunk_server_IP>:8000","title":"Splunk Installation"},{"location":"splunk_install/#splunk-installation","text":"Once the software is downloaded, follow the instructions that are appropriate for your OS in the Splunk installation manual: https://docs.splunk.com/Documentation/Splunk/latest/Installation/Chooseyourplatform The installation process for all platforms is very straightforward and is detailed in the official install guides, so will not be covered in detail here. Note When installing the Linux flavour of Splunk, make sure you do not miss the additional step required to ensure that Splunk starts after a server reboot. The following command needs to be executred after the software is installed (but please verify this isn the official installation documents): sudo /opt/splunk/bin/splunk enable boot-start Once installation has been completed, it should be possible to access the web dashboard of Splunk at the URL: http://<Splunk_server_IP>:8000","title":"Splunk Installation"},{"location":"splunk_platform/","text":"Splunk Platform To collect and view the test results data, an instance of Splunk is required (unless you choose to use InfluxDB/Grafana ). Splunk is a flexible data collection and reporting package that can take data sent by the wiperf probe and present it in a nice report format. Splunk can be installed on a wide variety of platforms that can be viewed at : https://www.splunk.com/en_us/download/splunk-enterprise.html This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. Note that a free account sign-up is required when downloading the software from the link listed above. To install Splunk and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . The product being installed is Splunk Enterprise. This is a paid-for product, but it has a free-tier for low data volumes (500Mbytes per day). Install initially with all the licensing defaults and then drop back to the free-tier by selecting Settings > Licensing and selecting the free tier. The free tier is plenty for the low volume rates that the wiperf probe generates when deploying probes at small-scale. Attention If you forget to select the free tier and your trial license expires, you may become locked out of the GUI with a \u201clicense expired\u201d message. If this happens, from the CLI of your Splunk server, find the file \u201cserver.conf\u201d and add the following line to the bottom of the file: [license] active_group = Lite_Free Then, restart the Splunk server and the login issue should be fixed. (The file is /opt/splunk/etc/system/local/server.conf on Linux) Connectivity Planning One area to consider is network connectivity between the wiperf probe and the Splunk instance. The wiperf probe needs to be able to access the Splunk server to send its results data. If the wiperf probe probe is being deployed on a wireless network, how is the results data generated going to get back to the Splunk server? If the probe is being deployed on a customer network to perform temporary monitoring, it will need to join the wireless network under test (or be plugged in to an ethernet switch port if testing a wired connection). But, how is the wiperf probe going to send its data to the Splunk server ? Many environments may not be comfortable with hooking up the wiperf probe to their internal wired network, potentially bridging wired and wireless networks. In some instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier.) Three topology deployment options are supported: Results data over wireless Results data over Ethernet Results data over VPN/wireless The method used is configured on the wiperf probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the Splunk server. The 3 connectivity options are discussed below. Results Data Over Wireless In this topology the wiperf probe is configured to join an SSID that has the Splunk server accessible via its WLAN interface. Typically, the Splunk server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the Splunk server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of Splunk server> Results data over Ethernet If the Splunk server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of Splunk server> Results data over Zerotier/wireless A simple way of getting the wiperf probe talking with your Splunk server, if it has no direct access, is to use the Zerotier service to create a virtual overlay network via the Internet. In summary, both the Splunk server and wiperf probe have a Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same virtual network in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the Splunk server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees. It\u2019s very easy to use, plus your Splunk server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the Splunk server and wiperf probe when the client is installed. config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of Splunk server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Splunk Platform"},{"location":"splunk_platform/#splunk-platform","text":"To collect and view the test results data, an instance of Splunk is required (unless you choose to use InfluxDB/Grafana ). Splunk is a flexible data collection and reporting package that can take data sent by the wiperf probe and present it in a nice report format. Splunk can be installed on a wide variety of platforms that can be viewed at : https://www.splunk.com/en_us/download/splunk-enterprise.html This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. Note that a free account sign-up is required when downloading the software from the link listed above. To install Splunk and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . The product being installed is Splunk Enterprise. This is a paid-for product, but it has a free-tier for low data volumes (500Mbytes per day). Install initially with all the licensing defaults and then drop back to the free-tier by selecting Settings > Licensing and selecting the free tier. The free tier is plenty for the low volume rates that the wiperf probe generates when deploying probes at small-scale. Attention If you forget to select the free tier and your trial license expires, you may become locked out of the GUI with a \u201clicense expired\u201d message. If this happens, from the CLI of your Splunk server, find the file \u201cserver.conf\u201d and add the following line to the bottom of the file: [license] active_group = Lite_Free Then, restart the Splunk server and the login issue should be fixed. (The file is /opt/splunk/etc/system/local/server.conf on Linux)","title":"Splunk Platform"},{"location":"splunk_platform/#connectivity-planning","text":"One area to consider is network connectivity between the wiperf probe and the Splunk instance. The wiperf probe needs to be able to access the Splunk server to send its results data. If the wiperf probe probe is being deployed on a wireless network, how is the results data generated going to get back to the Splunk server? If the probe is being deployed on a customer network to perform temporary monitoring, it will need to join the wireless network under test (or be plugged in to an ethernet switch port if testing a wired connection). But, how is the wiperf probe going to send its data to the Splunk server ? Many environments may not be comfortable with hooking up the wiperf probe to their internal wired network, potentially bridging wired and wireless networks. In some instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier.) Three topology deployment options are supported: Results data over wireless Results data over Ethernet Results data over VPN/wireless The method used is configured on the wiperf probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the Splunk server. The 3 connectivity options are discussed below.","title":"Connectivity Planning"},{"location":"splunk_platform/#results-data-over-wireless","text":"In this topology the wiperf probe is configured to join an SSID that has the Splunk server accessible via its WLAN interface. Typically, the Splunk server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the Splunk server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of Splunk server>","title":"Results Data Over Wireless"},{"location":"splunk_platform/#results-data-over-ethernet","text":"If the Splunk server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of Splunk server>","title":"Results data over Ethernet"},{"location":"splunk_platform/#results-data-over-zerotierwireless","text":"A simple way of getting the wiperf probe talking with your Splunk server, if it has no direct access, is to use the Zerotier service to create a virtual overlay network via the Internet. In summary, both the Splunk server and wiperf probe have a Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same virtual network in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the Splunk server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees. It\u2019s very easy to use, plus your Splunk server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the Splunk server and wiperf probe when the client is installed. config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of Splunk server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Results data over Zerotier/wireless"},{"location":"splunk_software/","text":"Splunk Software To obtain the Splunk software for your data server, get along to the Splunk web site and sign up for an account if you don\u2019t already have one: https://www.splunk.com/en_us/download/splunk-enterprise.html Once you\u2019re logged in to the Splunk site, you\u2019ll have a number of OS options to choose from ( supported platforms can be viewed here ). There are options for Windows, Linux & Mac OS: (Note: the version of Splunk you can download will likely be different to the version shown below - choose the latest version of 8.x) Once you've hit the download button for your OS choice, the Splunk Enterprise software chosen will start to download to your local machine, ready for installation. It\u2019s worth checking the download page to see if there are further download options. If you check the graphic below, you can see there is a \u201cDownload via Command Line (wget)\u201d option, which can be a much easier way to get the code directly on to your server. The options you will see here will vary between OS selections: (Note: the version of Splunk you can download will likely be different to the version shown below - choose the latest version of 8.x)","title":"Splunk Software"},{"location":"splunk_software/#splunk-software","text":"To obtain the Splunk software for your data server, get along to the Splunk web site and sign up for an account if you don\u2019t already have one: https://www.splunk.com/en_us/download/splunk-enterprise.html Once you\u2019re logged in to the Splunk site, you\u2019ll have a number of OS options to choose from ( supported platforms can be viewed here ). There are options for Windows, Linux & Mac OS: (Note: the version of Splunk you can download will likely be different to the version shown below - choose the latest version of 8.x) Once you've hit the download button for your OS choice, the Splunk Enterprise software chosen will start to download to your local machine, ready for installation. It\u2019s worth checking the download page to see if there are further download options. If you check the graphic below, you can see there is a \u201cDownload via Command Line (wget)\u201d option, which can be a much easier way to get the code directly on to your server. The options you will see here will vary between OS selections: (Note: the version of Splunk you can download will likely be different to the version shown below - choose the latest version of 8.x)","title":"Splunk Software"},{"location":"troubleshooting/","text":"Troubleshooting If things seem to be going wrong, here are a few tips to guide you in your diagnosis of the issue. Network Connectivity If you suspect network connectivity issues, your best course of action is to check the status of any interfaces being used by the probe. This can be done by accessing the CLI of the probe and running some of the commands provided below. Once interfaces have been verified, trying to access specific targets via network connectivity checks can also be useful. The following CLI commands will help to check the status of probe interfaces: Wireless NIC: iwconfig (Is the probe joining the wireless network? The SSID to which is is joined should be shown in the \"ESSID\" field) IP address: ifconfig eth0 , ifconfig wlan0 (Are the interfaces up? Do they have an IP address?) Network connectivity to a specific host: ping 192.168.0.254 Internet connectivity: ping google.com (Can the probe get to the Internet, if that is expected?) DNS connectivity: apt-get install dnsutils (install required commands), nslookup google.com Web connectivity: wget https://google.com (check if the required website target can be reached from the probe) iperf3 server connectivity: iperf3 -c 192.168.0.1 -t 10 -i 1 (run 10 sec tcp test to 192.168.0.1 server...alter for your iperf server address) Another useful source of information for connectivity issues in the syslog logging system of the probe. Take a look through the syslog file to see if there are any issues being reported that may be impacting your connectivity: tail -f /var/log/syslog Wiperf Configuration The wiperf configuration file is quite a complex file, so it's well worth checking for typos or critical fields that have been missed. The key fields worth double checking are: probe_mode mgt_if exporter_type (splunk) splunk_host splunk_port splunk_token (influxdb) influx_host influx_port influx_username influx_password influx_database One question to consider when deploying a probe is : Is the probe deployed in the topology you originally intended? If the environment is not as you expected and you need to use a different interface, make sure you have updated config.ini so that wiperf knows where to send test and management traffic (otherwise, you may hit routing issues) Logging Wiperf has extensive logging to help diagnose issues that may be causing operational issues. SSH to the probe and monitor the output of the log file /var/log/wiperf_agent.log . This file is created the first time that wiperf runs. If the file is not created after 5 minutes, then check the log file /var/log/wiperf_cron.log for error messages, as something fundamental is wrong with the installation. To watch the output of /var/log/wiperf_agent.log in real-time and view activity as data is collected every 5 minutes, run the following command on the CLI of the probe: tail -f /var/log/wiperf_agent.log Every 5 minutes, new log output will be seen that look similar to this: 2020-07-11 11:47:04,214 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,215 - Probe_Log - INFO - Starting logging... 2020-07-11 11:47:04,216 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,240 - Probe_Log - INFO - Checking if we use remote cfg file... 2020-07-11 11:47:04,241 - Probe_Log - INFO - No remote cfg file confgured...using current local ini file. 2020-07-11 11:47:04,242 - Probe_Log - INFO - No lock file found. Creating lock file. 2020-07-11 11:47:04,243 - Probe_Log - INFO - ########## Network connection checks ########## 2020-07-11 11:47:05,245 - Probe_Log - INFO - Checking wireless connection is good...(layer 1 &2) 2020-07-11 11:47:05,246 - Probe_Log - INFO - Checking wireless connection available. 2020-07-11 11:47:05,355 - Probe_Log - INFO - Checking we're connected to the network (layer3) 2020-07-11 11:47:05,356 - Probe_Log - INFO - Checking we have an IP address. 2020-07-11 11:47:05,379 - Probe_Log - INFO - Checking we can do a DNS lookup to google.com 2020-07-11 11:47:05,406 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:05,430 - Probe_Log - INFO - Checked interface route to : 216.58.212.238. Result: 216.58.212.238 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:05,431 - Probe_Log - INFO - Checking we can get to the management platform... 2020-07-11 11:47:05,432 - Probe_Log - INFO - Checking we will send mgt traffic over configured interface 'lo' mode. 2020-07-11 11:47:05,455 - Probe_Log - INFO - Checked interface route to : 127.0.0.1. Result: local 127.0.0.1 dev lo src 127.0.0.1 uid 0 2020-07-11 11:47:05,456 - Probe_Log - INFO - Interface mgt interface route looks good. 2020-07-11 11:47:05,457 - Probe_Log - INFO - Checking port connection to InfluxDB server 127.0.0.1, port: 8086 2020-07-11 11:47:05,484 - Probe_Log - INFO - Port connection to server 127.0.0.1, port: 8086 checked OK. 2020-07-11 11:47:05,485 - Probe_Log - INFO - ########## Wireless Connection ########## 2020-07-11 11:47:05,486 - Probe_Log - INFO - Wireless connection data: SSID:BNL, BSSID:5C:5B:35:C8:4D:C2, Freq:5.5, Center Freq:5.51, Channel: 100, Channel Width: 40, Tx Phy rate:200.0, Rx Phy rate:135.0, Tx MCS: 0, Rx MCS: 0, RSSI:-42.0, Tx retries:187, IP address:192.168.0.48 2020-07-11 11:47:05,486 - Probe_Log - INFO - InfluxDB update: wiperf-network, source=Network Tests 2020-07-11 11:47:05,487 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:05,573 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:05,574 - Probe_Log - INFO - Connection results sent OK. 2020-07-11 11:47:05,595 - Probe_Log - INFO - ########## speedtest ########## 2020-07-11 11:47:05,597 - Probe_Log - INFO - Starting speedtest... 2020-07-11 11:47:06,599 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:06,623 - Probe_Log - INFO - Checked interface route to : 8.8.8.8. Result: 8.8.8.8 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:06,624 - Probe_Log - INFO - Speedtest in progress....please wait. 2020-07-11 11:47:28,761 - Probe_Log - INFO - ping_time: 31, download_rate: 41.56, upload_rate: 9.74, server_name: speedtest-net5.rapidswitch.co.uk:8080 2020-07-11 11:47:28,766 - Probe_Log - INFO - Speedtest ended. 2020-07-11 11:47:28,767 - Probe_Log - INFO - InfluxDB update: wiperf-speedtest, source=Speedtest 2020-07-11 11:47:28,768 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:28,858 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:28,860 - Probe_Log - INFO - Speedtest results sent OK. The output is quite verbose and detailed, but it will provide a good indication of where wiperf is having difficulties. Miscellaneous Checks NTP / Time Sync Make sure your probe and reporting server are time synchronized and are showing the same data and time. Check the time and date of the probe using the CLI command date . If your probe and reporting server are in different timezones, check the UTC time on both to make sure you are comparing apples with apples: date -u The WLAN Pi and RPi will generally synchronized to a time source out of the box using a process such as Chrony or NTP, so will not need any specific intervention. However, if they do not have access to the Internet, the synchronization process may be compromised and may need you to manually configure time sources . Hostname If you have changed the probe hostname from its default, make sure you have updated both the /etc/hosts AND the /etc/hostname files with the new name (if done incorrectly, this can cause some very weird issues!)","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"If things seem to be going wrong, here are a few tips to guide you in your diagnosis of the issue.","title":"Troubleshooting"},{"location":"troubleshooting/#network-connectivity","text":"If you suspect network connectivity issues, your best course of action is to check the status of any interfaces being used by the probe. This can be done by accessing the CLI of the probe and running some of the commands provided below. Once interfaces have been verified, trying to access specific targets via network connectivity checks can also be useful. The following CLI commands will help to check the status of probe interfaces: Wireless NIC: iwconfig (Is the probe joining the wireless network? The SSID to which is is joined should be shown in the \"ESSID\" field) IP address: ifconfig eth0 , ifconfig wlan0 (Are the interfaces up? Do they have an IP address?) Network connectivity to a specific host: ping 192.168.0.254 Internet connectivity: ping google.com (Can the probe get to the Internet, if that is expected?) DNS connectivity: apt-get install dnsutils (install required commands), nslookup google.com Web connectivity: wget https://google.com (check if the required website target can be reached from the probe) iperf3 server connectivity: iperf3 -c 192.168.0.1 -t 10 -i 1 (run 10 sec tcp test to 192.168.0.1 server...alter for your iperf server address) Another useful source of information for connectivity issues in the syslog logging system of the probe. Take a look through the syslog file to see if there are any issues being reported that may be impacting your connectivity: tail -f /var/log/syslog","title":"Network Connectivity"},{"location":"troubleshooting/#wiperf-configuration","text":"The wiperf configuration file is quite a complex file, so it's well worth checking for typos or critical fields that have been missed. The key fields worth double checking are: probe_mode mgt_if exporter_type (splunk) splunk_host splunk_port splunk_token (influxdb) influx_host influx_port influx_username influx_password influx_database One question to consider when deploying a probe is : Is the probe deployed in the topology you originally intended? If the environment is not as you expected and you need to use a different interface, make sure you have updated config.ini so that wiperf knows where to send test and management traffic (otherwise, you may hit routing issues)","title":"Wiperf Configuration"},{"location":"troubleshooting/#logging","text":"Wiperf has extensive logging to help diagnose issues that may be causing operational issues. SSH to the probe and monitor the output of the log file /var/log/wiperf_agent.log . This file is created the first time that wiperf runs. If the file is not created after 5 minutes, then check the log file /var/log/wiperf_cron.log for error messages, as something fundamental is wrong with the installation. To watch the output of /var/log/wiperf_agent.log in real-time and view activity as data is collected every 5 minutes, run the following command on the CLI of the probe: tail -f /var/log/wiperf_agent.log Every 5 minutes, new log output will be seen that look similar to this: 2020-07-11 11:47:04,214 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,215 - Probe_Log - INFO - Starting logging... 2020-07-11 11:47:04,216 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,240 - Probe_Log - INFO - Checking if we use remote cfg file... 2020-07-11 11:47:04,241 - Probe_Log - INFO - No remote cfg file confgured...using current local ini file. 2020-07-11 11:47:04,242 - Probe_Log - INFO - No lock file found. Creating lock file. 2020-07-11 11:47:04,243 - Probe_Log - INFO - ########## Network connection checks ########## 2020-07-11 11:47:05,245 - Probe_Log - INFO - Checking wireless connection is good...(layer 1 &2) 2020-07-11 11:47:05,246 - Probe_Log - INFO - Checking wireless connection available. 2020-07-11 11:47:05,355 - Probe_Log - INFO - Checking we're connected to the network (layer3) 2020-07-11 11:47:05,356 - Probe_Log - INFO - Checking we have an IP address. 2020-07-11 11:47:05,379 - Probe_Log - INFO - Checking we can do a DNS lookup to google.com 2020-07-11 11:47:05,406 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:05,430 - Probe_Log - INFO - Checked interface route to : 216.58.212.238. Result: 216.58.212.238 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:05,431 - Probe_Log - INFO - Checking we can get to the management platform... 2020-07-11 11:47:05,432 - Probe_Log - INFO - Checking we will send mgt traffic over configured interface 'lo' mode. 2020-07-11 11:47:05,455 - Probe_Log - INFO - Checked interface route to : 127.0.0.1. Result: local 127.0.0.1 dev lo src 127.0.0.1 uid 0 2020-07-11 11:47:05,456 - Probe_Log - INFO - Interface mgt interface route looks good. 2020-07-11 11:47:05,457 - Probe_Log - INFO - Checking port connection to InfluxDB server 127.0.0.1, port: 8086 2020-07-11 11:47:05,484 - Probe_Log - INFO - Port connection to server 127.0.0.1, port: 8086 checked OK. 2020-07-11 11:47:05,485 - Probe_Log - INFO - ########## Wireless Connection ########## 2020-07-11 11:47:05,486 - Probe_Log - INFO - Wireless connection data: SSID:BNL, BSSID:5C:5B:35:C8:4D:C2, Freq:5.5, Center Freq:5.51, Channel: 100, Channel Width: 40, Tx Phy rate:200.0, Rx Phy rate:135.0, Tx MCS: 0, Rx MCS: 0, RSSI:-42.0, Tx retries:187, IP address:192.168.0.48 2020-07-11 11:47:05,486 - Probe_Log - INFO - InfluxDB update: wiperf-network, source=Network Tests 2020-07-11 11:47:05,487 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:05,573 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:05,574 - Probe_Log - INFO - Connection results sent OK. 2020-07-11 11:47:05,595 - Probe_Log - INFO - ########## speedtest ########## 2020-07-11 11:47:05,597 - Probe_Log - INFO - Starting speedtest... 2020-07-11 11:47:06,599 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:06,623 - Probe_Log - INFO - Checked interface route to : 8.8.8.8. Result: 8.8.8.8 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:06,624 - Probe_Log - INFO - Speedtest in progress....please wait. 2020-07-11 11:47:28,761 - Probe_Log - INFO - ping_time: 31, download_rate: 41.56, upload_rate: 9.74, server_name: speedtest-net5.rapidswitch.co.uk:8080 2020-07-11 11:47:28,766 - Probe_Log - INFO - Speedtest ended. 2020-07-11 11:47:28,767 - Probe_Log - INFO - InfluxDB update: wiperf-speedtest, source=Speedtest 2020-07-11 11:47:28,768 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:28,858 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:28,860 - Probe_Log - INFO - Speedtest results sent OK. The output is quite verbose and detailed, but it will provide a good indication of where wiperf is having difficulties.","title":"Logging"},{"location":"troubleshooting/#miscellaneous-checks","text":"","title":"Miscellaneous Checks"},{"location":"troubleshooting/#ntp-time-sync","text":"Make sure your probe and reporting server are time synchronized and are showing the same data and time. Check the time and date of the probe using the CLI command date . If your probe and reporting server are in different timezones, check the UTC time on both to make sure you are comparing apples with apples: date -u The WLAN Pi and RPi will generally synchronized to a time source out of the box using a process such as Chrony or NTP, so will not need any specific intervention. However, if they do not have access to the Internet, the synchronization process may be compromised and may need you to manually configure time sources .","title":"NTP / Time Sync"},{"location":"troubleshooting/#hostname","text":"If you have changed the probe hostname from its default, make sure you have updated both the /etc/hosts AND the /etc/hostname files with the new name (if done incorrectly, this can cause some very weird issues!)","title":"Hostname"},{"location":"whats_new_v2.1/","text":"What's New in version 2.1 17th January 2021 - Author: Nigel Bowden Version 2.1 of the wiperf probe code introduces a number of bug-fixes and features. These are outlined below: New Features SMB Tests Thanks to the kind contribution of Mario Gingras , wiperf now supports performance testing of SMB/CIFS file transfers. Up to 5 tests may be defined to be run in each wiperf poll cycle. Each test instance will mount a remote volume and then transfer a file from the remote server to the wiperf probe. The transfer rate and time are then reported to the management platform for reporting. A new dashboard is also provided to display the SMB data in report format. If using the SMB tests, note that there are a small number of additional Linux packages that you may need to install to provide the SMB connectivity. Please check out the link provided below for more details. Further information: SMB Tests Reference Document Librespeed Speedtest Support An idea for Librespeed speedtest support by wiperf was submitted by Oscar Leal. This has now been added, meaning that wiperf can now run a speedtest to either Ookla or Librespeed for performance testing. In addition to testing against public Librespeed servers,it is also possible to test against your own instance of the Librespeed server, allowing testing within your own environment, without hitting the Internet (and associated bottlenecks) If using the Librespeed speedtest, note that there an additional Linux package that you will need to install to enable the test to work. Please check out the link provided below for more details. Further information: Librespeed Reference Document Results Spooling Tris Kipling is a keen supporter of the wiperf project and, based on is experience of using wiperf probes, suggested that some mechanism be created to allow a degree of \"survivability\" for instances when communications between the probe and management platform are down. To address this request, a spooling mechanism for results data is now provided. Previously, if the probe was unable to communicate with the management server, no tests would be attempted during the current poll cycle. With the new spooling feature, tests will still be performed by the probe and results data will be locally stored. The data will be forwarded by the probe when communications with the management platform is re-established (and any local data spool files removed). This feature is enabled by default so required no additional configuration. It may be disabled if required. Results Data Caching Due to a couple of requests for local storage of results data, local results caching has been implemented. When this is enabled, all results data is stored in either CSV or JSON format in local storage on the wiperf probe (in addition to being sent to the reporting server). The data files are stored in the local directory /var/cache/wiperf for a configurable period of time (3 days by default). This feature allows for other methods of data inspection or retrieval that users may choose to perform. It is disabled by default. (Note: This is a completely separate feature from the data spooling feature mentioned previously)) Further information: Results Caching Reference Document Error Reporting Diagnosing issues with probe tests may be difficult without remote access to a probe for further investigation. To assist with looking at issues, error messages generated by the probe during its test cycle are now available for reporting back to the reporting platform. This feature is enabled by default, but may be disabled if there are concerns about volumes of management traffic from the probe. There is also a configuration setting to allow the number of error messages per poll cycle to be capped (5 by default). A new \"Probe Health\" dashboard has been provided to allow viewing of error messages. Installing Installation is not required for the WLAN Pi as wiperf is included in the WLAN Pi packages. To upgrade, see the next section. To install on to an RPi (or other platform), see our installation document . Upgrading To upgrade from a previous release of wiperf, please consult these instructions in our upgrade document . Attention Don't forget to also update your management platform dashboard files to make sure you have the latest & greatest that are compatible with this release Splunk link Grafana link When upgrading on the WLAN Pi make sure your WLAN Pi is in classic mode prior to upgrading Make sure you are aware of this advisory about existing ping test results when moving to version 2.1 Several new options have been added to config.ini - see the note below to make sure you don't miss out on them. Attention Ensure that you re-create your config.ini file using the new config.default.ini file supplied during the upgrade Several new options have been added to the config.ini configuration file. Use the new config.default.ini file in /etc/wiperf as a template to create a new copy of config.ini . Wiperf will not run if some of the new configuration sections are missing. Copy across your existing settings in to the new copy of config.ini . For example: # after upgrade cd /etc/wiperf # backup existing config file sudo cp config.ini config.ini.orig # create new config file from template sudo cp config.defaut.ini config.ini # view old config file cat config.ini.orig # edit new file to transpose old config settings sudo nano config.ini Release Notes Added fix for bad read of remote config file when downloaded (thanks Ben Roeder) Fixed bad config variable names for InfluxDB2 in config.py (thanks Konstantin - issue #1) Added SMB tests provided by Mario Gingras Improved route injection for tests when required interface not selected by default routing Fixed issue with incorrect data types in Influx DB for ping tests (some numeric values were created as strings, preventing calculations on their values.) This fixed issue will cause failures for ping test results on existing data. To correct the issue, existing ping test data in the Influx DB must be dropped using the following command in the InfluxDB CLI utility: sudo influx -username admin -password letmein use wiperf drop series from \"wiperf-ping\" Added support for librespeed to speedtest in addition to existing Ookla support. Also made more data points available to reports including latency, jitter & data volume for upload/download Added support for cached data : data may now be cached locally for retrieval and inspection by user-defined means (e.g. SSH to probe & inspect data). Data may be stored in VSC or JSON format. Added result spooling support. If connectivity between the probe and reporting platform is lost, results may be spooled to a local directory and automatically forwarded when connectivity is re-established. Added support for forwarding of probe error messages to reporting platform to allow diagnosis of test related issues on the probe. Fixed DHCP test errors on RPi reported/fixed by Ben Roeder (Issue #39) Fixed re-read of config file from github reported/fixed by Ben Roeder (PR #7) Added new dashboard for SMB tests Updated summary dashboard to support new SMB tests Updated speedtest dashboard to support librespeed and additional data points Added dashboard for probe health, which include probe poll error messages Miscellaneous minor dashboard improvements/fixes","title":"What's New in version 2.1"},{"location":"whats_new_v2.1/#whats-new-in-version-21","text":"17th January 2021 - Author: Nigel Bowden Version 2.1 of the wiperf probe code introduces a number of bug-fixes and features. These are outlined below:","title":"What's New in version 2.1"},{"location":"whats_new_v2.1/#new-features","text":"","title":"New Features"},{"location":"whats_new_v2.1/#smb-tests","text":"Thanks to the kind contribution of Mario Gingras , wiperf now supports performance testing of SMB/CIFS file transfers. Up to 5 tests may be defined to be run in each wiperf poll cycle. Each test instance will mount a remote volume and then transfer a file from the remote server to the wiperf probe. The transfer rate and time are then reported to the management platform for reporting. A new dashboard is also provided to display the SMB data in report format. If using the SMB tests, note that there are a small number of additional Linux packages that you may need to install to provide the SMB connectivity. Please check out the link provided below for more details. Further information: SMB Tests Reference Document","title":"SMB Tests"},{"location":"whats_new_v2.1/#librespeed-speedtest-support","text":"An idea for Librespeed speedtest support by wiperf was submitted by Oscar Leal. This has now been added, meaning that wiperf can now run a speedtest to either Ookla or Librespeed for performance testing. In addition to testing against public Librespeed servers,it is also possible to test against your own instance of the Librespeed server, allowing testing within your own environment, without hitting the Internet (and associated bottlenecks) If using the Librespeed speedtest, note that there an additional Linux package that you will need to install to enable the test to work. Please check out the link provided below for more details. Further information: Librespeed Reference Document","title":"Librespeed Speedtest Support"},{"location":"whats_new_v2.1/#results-spooling","text":"Tris Kipling is a keen supporter of the wiperf project and, based on is experience of using wiperf probes, suggested that some mechanism be created to allow a degree of \"survivability\" for instances when communications between the probe and management platform are down. To address this request, a spooling mechanism for results data is now provided. Previously, if the probe was unable to communicate with the management server, no tests would be attempted during the current poll cycle. With the new spooling feature, tests will still be performed by the probe and results data will be locally stored. The data will be forwarded by the probe when communications with the management platform is re-established (and any local data spool files removed). This feature is enabled by default so required no additional configuration. It may be disabled if required.","title":"Results Spooling"},{"location":"whats_new_v2.1/#results-data-caching","text":"Due to a couple of requests for local storage of results data, local results caching has been implemented. When this is enabled, all results data is stored in either CSV or JSON format in local storage on the wiperf probe (in addition to being sent to the reporting server). The data files are stored in the local directory /var/cache/wiperf for a configurable period of time (3 days by default). This feature allows for other methods of data inspection or retrieval that users may choose to perform. It is disabled by default. (Note: This is a completely separate feature from the data spooling feature mentioned previously)) Further information: Results Caching Reference Document","title":"Results Data Caching"},{"location":"whats_new_v2.1/#error-reporting","text":"Diagnosing issues with probe tests may be difficult without remote access to a probe for further investigation. To assist with looking at issues, error messages generated by the probe during its test cycle are now available for reporting back to the reporting platform. This feature is enabled by default, but may be disabled if there are concerns about volumes of management traffic from the probe. There is also a configuration setting to allow the number of error messages per poll cycle to be capped (5 by default). A new \"Probe Health\" dashboard has been provided to allow viewing of error messages.","title":"Error Reporting"},{"location":"whats_new_v2.1/#installing","text":"Installation is not required for the WLAN Pi as wiperf is included in the WLAN Pi packages. To upgrade, see the next section. To install on to an RPi (or other platform), see our installation document .","title":"Installing"},{"location":"whats_new_v2.1/#upgrading","text":"To upgrade from a previous release of wiperf, please consult these instructions in our upgrade document . Attention Don't forget to also update your management platform dashboard files to make sure you have the latest & greatest that are compatible with this release Splunk link Grafana link When upgrading on the WLAN Pi make sure your WLAN Pi is in classic mode prior to upgrading Make sure you are aware of this advisory about existing ping test results when moving to version 2.1 Several new options have been added to config.ini - see the note below to make sure you don't miss out on them. Attention Ensure that you re-create your config.ini file using the new config.default.ini file supplied during the upgrade Several new options have been added to the config.ini configuration file. Use the new config.default.ini file in /etc/wiperf as a template to create a new copy of config.ini . Wiperf will not run if some of the new configuration sections are missing. Copy across your existing settings in to the new copy of config.ini . For example: # after upgrade cd /etc/wiperf # backup existing config file sudo cp config.ini config.ini.orig # create new config file from template sudo cp config.defaut.ini config.ini # view old config file cat config.ini.orig # edit new file to transpose old config settings sudo nano config.ini","title":"Upgrading"},{"location":"whats_new_v2.1/#release-notes","text":"Added fix for bad read of remote config file when downloaded (thanks Ben Roeder) Fixed bad config variable names for InfluxDB2 in config.py (thanks Konstantin - issue #1) Added SMB tests provided by Mario Gingras Improved route injection for tests when required interface not selected by default routing Fixed issue with incorrect data types in Influx DB for ping tests (some numeric values were created as strings, preventing calculations on their values.) This fixed issue will cause failures for ping test results on existing data. To correct the issue, existing ping test data in the Influx DB must be dropped using the following command in the InfluxDB CLI utility: sudo influx -username admin -password letmein use wiperf drop series from \"wiperf-ping\" Added support for librespeed to speedtest in addition to existing Ookla support. Also made more data points available to reports including latency, jitter & data volume for upload/download Added support for cached data : data may now be cached locally for retrieval and inspection by user-defined means (e.g. SSH to probe & inspect data). Data may be stored in VSC or JSON format. Added result spooling support. If connectivity between the probe and reporting platform is lost, results may be spooled to a local directory and automatically forwarded when connectivity is re-established. Added support for forwarding of probe error messages to reporting platform to allow diagnosis of test related issues on the probe. Fixed DHCP test errors on RPi reported/fixed by Ben Roeder (Issue #39) Fixed re-read of config file from github reported/fixed by Ben Roeder (PR #7) Added new dashboard for SMB tests Updated summary dashboard to support new SMB tests Updated speedtest dashboard to support librespeed and additional data points Added dashboard for probe health, which include probe poll error messages Miscellaneous minor dashboard improvements/fixes","title":"Release Notes"},{"location":"whats_new_v2.2/","text":"What's New in version 2.2 27th January 2021 - Author: Nigel Bowden Version 2.2 of the wiperf probe code follows hot on the heels of v2.1, following several requests from the community for removal of limitations on the number of test targets for several of the tests performed by wiperf. Previously, the Ping, HTTP, DNS and SMB tests were limited to 5 test targets. In v2.2, these limits are removed and the number of test targets is now user defined. Note: Please make sure you review the v2.1 release notes if you're jumping straight from v2.0 to v2.2 Ping Tests The ping tests section of config.ini is now structured as shown below: ; ==================================================================== ; Ping tests settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [Ping_Test] ; yes = enabled, no = disabled enabled: yes ; ; Number of targets we'd like to ping ping_targets_count: 2 ; first host we'd like to ping ping_host1: google.com ; ; second host we'd like to ping ping_host2: cisco.com ; ; add more ping targets to match your \"ping_targets_count\" value ; ping_host3: ; ping_host4: ; ping_host5: ; ping_host6: ;....etc ; ; number of pings to send per test ping_count: 10 ; ; Timeout value for pings (in secs) ping_timeout: 1 ; ; Interval between pings (in secs) ping_interval: 0.2 ; ; ------------ Advanced settings for Ping tests section, do not change ------------- ; Name used for ping_test file/data group/data source ping_data_file: wiperf-ping ;-------------------------------- Ping Section End --------------------------------- Note the new ping_targets_count field which allows the number of ping targets to be user-defined. Note that additional ping_host entries need to be added to correspond with the number of targets specified. Also new for v2.2 are the ping_timeout and ping_interval fields. These allow for the optimization of ping testing and prevent long delays when pinging large numbers, or poorly performing targets. DNS Tests The number of DNS tests that are performed during a poll cycle may now be user-defined, rather than being limited to 5 test entries. A new field within config.ini : dns_targets_count provides the count of the number of tests to be performed. Additionally, as many dns_target entries as required may be added to match up with the number of targets defined. See the extract from the new config.ini template file below showing the new options: ; ==================================================================== ; DNS tests settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [DNS_test] ; yes = enabled, no = disabled enabled: yes ; ; Number of DNS targets we'd like to test dns_targets_count: 2 ; ; First DNS target dns_target1: google.com ; ; Second DNS target dns_target2: cisco.com ; ; add more DNS targets to match your \"dns_targets_count\" value ; dns_target3: ; dns_target4: ; dns_target5: ; dns_target6: ;....etc ; ; ------------ Advanced settings for DNS tests section, do not change -------------- ; Name used for dns_test file/data group/data source dns_data_file: wiperf-dns ;-------------------------------- DNS Section End ---------------------------------- HTTP Tests The number of HTTP tests that are performed during a poll cycle may now be user-defined, rather than being limited to 5 test entries. A new field within config.ini : http_targets_count provides the count of the number of tests to be performed. Additionally, as many http_target entries as required may be added to match up with the number of targets defined. See the extract from the new config.ini template file below showing the new options: ; ==================================================================== ; HTTP tests settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [HTTP_test] ; yes = enabled, no = disabled enabled: yes ; ; Number of HTTP targets we'd like to test http_targets_count: 2 ; ; First HTTP target http_target1: https://google.com ; ; Second HTTP target http_target2: https://cisco.com ; ; add more HTTP targets to match your \"http_targets_count\" value ; http_target3: ; http_target4: ; http_target5: ; http_target6: ;....etc ; ; -------------- Advanced settings for HTTP tests section, do not change ---------------- ; Name used for http_test file/data group/data source http_data_file: wiperf-http ;-------------------------------- HTTP Section End --------------------------------- SMB Tests The number of SMB tests that are performed during a poll cycle may now be user-defined, rather than being limited to 5 test entries. A new field within config.ini : smb_targets_count provides the count of the number of tests to be performed. Additionally, as many SMB entries as required may be added to match up with the number of targets defined. This will require the creation of an smb_host1 , smb_username , smb_password , smb_path and smb_filename field for each target (see examples below) See the extract from the new config.ini template file below showing the new options: ; ==================================================================== ; SMB tests settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [SMB_test] ; yes = enabled, no = disabled enabled: no ; ; Number of SMB targets we'd like to test smb_targets_count: 2 ; ; username and password to be used for all tests if all ; tests use same credentials smb_global_username: smb_global_password: ; ; first host we'd like to test smb_host1: smb_username1: smb_password1: smb_path1: smb_filename1: ; ; second host we'd like to test smb_host2: smb_username2: smb_password2: smb_path2: smb_filename2: ; ; add more SMB targets to match your \"smb_targets_count\" value ;smb_host3: ;smb_username3: ;smb_password3: ;smb_path3: ;smb_filename3: ; ;smb_host4: ;smb_username4: ;smb_password4: ;smb_path4: ;smb_filename4: ; ...etc ; ; ------------ Advanced settings for SMB tests section, do not change -------------- ; Name used for SMB file/data group/data source smb_data_file: wiperf-smb ;-------------------------------- SMB Section End ---------------------------------- Dashboards There are no new dashboards in this release. As the number of test targets is now variable, with no upper limit, it is up to individual users to customize the supplied dashboards for their own use. Going forwards, it is likely I will continue to support the existing 5 instances of each target in any dashboards made available. Caveats Creating a large number of tests may be desirable for certain use-cases, but comes with a few caveats. Remember that each test is performed sequentially (i.e. no test is started until the previous test completes). The total time taken to finish all tests may become quite long for high numbers of tests, or for tests measuring demanding or poorly performing targets (e.g. large file transfers or targets that time-out). By default, the poll cycle defined for wiperf is 5 minutes. This can be modified by re-configuring the system cron job used by wiperf to initiate regular polls. If tests run for longer than the configured poll cycle (e.g. 6 minutes of testing time for a 5 minute poll interval), then a new poll cycle will attempt to start after 5 minutes even though a poll cycle is already in progress. However, the new poll process will detect that a poll cycle is already in progress and exit without performing any tests. This means that you will achieve only half the poll cycles expected, as every 2nd poll cycle will exit without running. The wiperf poller also has a fail-safe mechanism to try to fix issues in the event of many test failures being detected. It has a watchdog function that will count various system errors and will reboot the probe if they become too high. In the event that many tests are configured that fail in high numbers, then you many observe the probe rebooting periodically due to the watchdog counter being exceeded. This is not an error - this is built in by design to try to fix issues such as being stuck to a remote access point or other network level issues that require a full reboot to clear unknown issues. Upgrading To upgrade from a previous release of wiperf, please consult these instructions in our upgrade document . Related Check out this related article: What's New in version 2.1 ?","title":"What's New in V2.2"},{"location":"whats_new_v2.2/#whats-new-in-version-22","text":"27th January 2021 - Author: Nigel Bowden Version 2.2 of the wiperf probe code follows hot on the heels of v2.1, following several requests from the community for removal of limitations on the number of test targets for several of the tests performed by wiperf. Previously, the Ping, HTTP, DNS and SMB tests were limited to 5 test targets. In v2.2, these limits are removed and the number of test targets is now user defined. Note: Please make sure you review the v2.1 release notes if you're jumping straight from v2.0 to v2.2","title":"What's New in version 2.2"},{"location":"whats_new_v2.2/#ping-tests","text":"The ping tests section of config.ini is now structured as shown below: ; ==================================================================== ; Ping tests settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [Ping_Test] ; yes = enabled, no = disabled enabled: yes ; ; Number of targets we'd like to ping ping_targets_count: 2 ; first host we'd like to ping ping_host1: google.com ; ; second host we'd like to ping ping_host2: cisco.com ; ; add more ping targets to match your \"ping_targets_count\" value ; ping_host3: ; ping_host4: ; ping_host5: ; ping_host6: ;....etc ; ; number of pings to send per test ping_count: 10 ; ; Timeout value for pings (in secs) ping_timeout: 1 ; ; Interval between pings (in secs) ping_interval: 0.2 ; ; ------------ Advanced settings for Ping tests section, do not change ------------- ; Name used for ping_test file/data group/data source ping_data_file: wiperf-ping ;-------------------------------- Ping Section End --------------------------------- Note the new ping_targets_count field which allows the number of ping targets to be user-defined. Note that additional ping_host entries need to be added to correspond with the number of targets specified. Also new for v2.2 are the ping_timeout and ping_interval fields. These allow for the optimization of ping testing and prevent long delays when pinging large numbers, or poorly performing targets.","title":"Ping Tests"},{"location":"whats_new_v2.2/#dns-tests","text":"The number of DNS tests that are performed during a poll cycle may now be user-defined, rather than being limited to 5 test entries. A new field within config.ini : dns_targets_count provides the count of the number of tests to be performed. Additionally, as many dns_target entries as required may be added to match up with the number of targets defined. See the extract from the new config.ini template file below showing the new options: ; ==================================================================== ; DNS tests settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [DNS_test] ; yes = enabled, no = disabled enabled: yes ; ; Number of DNS targets we'd like to test dns_targets_count: 2 ; ; First DNS target dns_target1: google.com ; ; Second DNS target dns_target2: cisco.com ; ; add more DNS targets to match your \"dns_targets_count\" value ; dns_target3: ; dns_target4: ; dns_target5: ; dns_target6: ;....etc ; ; ------------ Advanced settings for DNS tests section, do not change -------------- ; Name used for dns_test file/data group/data source dns_data_file: wiperf-dns ;-------------------------------- DNS Section End ----------------------------------","title":"DNS Tests"},{"location":"whats_new_v2.2/#http-tests","text":"The number of HTTP tests that are performed during a poll cycle may now be user-defined, rather than being limited to 5 test entries. A new field within config.ini : http_targets_count provides the count of the number of tests to be performed. Additionally, as many http_target entries as required may be added to match up with the number of targets defined. See the extract from the new config.ini template file below showing the new options: ; ==================================================================== ; HTTP tests settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [HTTP_test] ; yes = enabled, no = disabled enabled: yes ; ; Number of HTTP targets we'd like to test http_targets_count: 2 ; ; First HTTP target http_target1: https://google.com ; ; Second HTTP target http_target2: https://cisco.com ; ; add more HTTP targets to match your \"http_targets_count\" value ; http_target3: ; http_target4: ; http_target5: ; http_target6: ;....etc ; ; -------------- Advanced settings for HTTP tests section, do not change ---------------- ; Name used for http_test file/data group/data source http_data_file: wiperf-http ;-------------------------------- HTTP Section End ---------------------------------","title":"HTTP Tests"},{"location":"whats_new_v2.2/#smb-tests","text":"The number of SMB tests that are performed during a poll cycle may now be user-defined, rather than being limited to 5 test entries. A new field within config.ini : smb_targets_count provides the count of the number of tests to be performed. Additionally, as many SMB entries as required may be added to match up with the number of targets defined. This will require the creation of an smb_host1 , smb_username , smb_password , smb_path and smb_filename field for each target (see examples below) See the extract from the new config.ini template file below showing the new options: ; ==================================================================== ; SMB tests settings ; (Changes made in this section will be used in next test cycle ; and may be made while in Wiperf mode on the WLANPi) ; ==================================================================== [SMB_test] ; yes = enabled, no = disabled enabled: no ; ; Number of SMB targets we'd like to test smb_targets_count: 2 ; ; username and password to be used for all tests if all ; tests use same credentials smb_global_username: smb_global_password: ; ; first host we'd like to test smb_host1: smb_username1: smb_password1: smb_path1: smb_filename1: ; ; second host we'd like to test smb_host2: smb_username2: smb_password2: smb_path2: smb_filename2: ; ; add more SMB targets to match your \"smb_targets_count\" value ;smb_host3: ;smb_username3: ;smb_password3: ;smb_path3: ;smb_filename3: ; ;smb_host4: ;smb_username4: ;smb_password4: ;smb_path4: ;smb_filename4: ; ...etc ; ; ------------ Advanced settings for SMB tests section, do not change -------------- ; Name used for SMB file/data group/data source smb_data_file: wiperf-smb ;-------------------------------- SMB Section End ----------------------------------","title":"SMB Tests"},{"location":"whats_new_v2.2/#dashboards","text":"There are no new dashboards in this release. As the number of test targets is now variable, with no upper limit, it is up to individual users to customize the supplied dashboards for their own use. Going forwards, it is likely I will continue to support the existing 5 instances of each target in any dashboards made available.","title":"Dashboards"},{"location":"whats_new_v2.2/#caveats","text":"Creating a large number of tests may be desirable for certain use-cases, but comes with a few caveats. Remember that each test is performed sequentially (i.e. no test is started until the previous test completes). The total time taken to finish all tests may become quite long for high numbers of tests, or for tests measuring demanding or poorly performing targets (e.g. large file transfers or targets that time-out). By default, the poll cycle defined for wiperf is 5 minutes. This can be modified by re-configuring the system cron job used by wiperf to initiate regular polls. If tests run for longer than the configured poll cycle (e.g. 6 minutes of testing time for a 5 minute poll interval), then a new poll cycle will attempt to start after 5 minutes even though a poll cycle is already in progress. However, the new poll process will detect that a poll cycle is already in progress and exit without performing any tests. This means that you will achieve only half the poll cycles expected, as every 2nd poll cycle will exit without running. The wiperf poller also has a fail-safe mechanism to try to fix issues in the event of many test failures being detected. It has a watchdog function that will count various system errors and will reboot the probe if they become too high. In the event that many tests are configured that fail in high numbers, then you many observe the probe rebooting periodically due to the watchdog counter being exceeded. This is not an error - this is built in by design to try to fix issues such as being stuck to a remote access point or other network level issues that require a full reboot to clear unknown issues.","title":"Caveats"},{"location":"whats_new_v2.2/#upgrading","text":"To upgrade from a previous release of wiperf, please consult these instructions in our upgrade document .","title":"Upgrading"},{"location":"whats_new_v2.2/#related","text":"Check out this related article: What's New in version 2.1 ?","title":"Related"}]}