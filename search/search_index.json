{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"wiperf V2: An Open Source UX Performance Probe Wiperf is a utility that can be installed on a WLAN Pi or a Raspberry Pi to act as a network probe that runs a series of network performance tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as an ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is (usually) no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where issues are being experienced and longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana Data Server The core focus of this project is the probe device that gathers the network performance data in which we are interested. However, the data server is a critical component that allows visualization of that performance data. High-level configuration details will be provided to \"get you going\", but detailed information about the operation of these platforms is beyond the scope of this project. Both of the data servers supported are \"NoSQL\" servers, which means that no data structures have to be pre-defined in database tables. This means we can send our data structures, that contain network performance data, to the server with very little set-up compared to traditional database servers. As long as we have a valid set of credentials for the data server, we can just send JSON-formatted data over HTTPS in whatever structure we choose. A database query language on the data server allows us to retrieve and graph the data collected by the wiperf probe. Splunk Splunk is supported on all popular operating systems and is very easy to set up on your server of choice. It acts as both the data store and visualization platform. Splunk is a commercial, rather than open-source product. The volume of data returned by the probe is very low, so the free tier of Splunk may be used to gather and report on data. For details on how to set up a Splunk server, start at this documentation page: [link][splunk_platform.md] Splunk product web site: https://www.splunk.com/ InfluxDB/Grafana Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data server from which to pull its network performance data. To meet this requirement, the InfluxDB database server is used. Like Grafana, InfluxDB is also an open-source package. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana web site (v6.7): https://grafana.com/ Influx web site (v.1.8): https://www.influxdata.com Workflow to Setup Wiperf The workflow to get Wiperf fully operational consists of a number of steps that break down in to two main areas: Probe setup (the RPi or WLAN Pi device itself) Data server setup (the Splunk or Influx/Grafana server) The data server setup tends to be a task that needs completion only once (or at least very infrequently). Conversely, some or all of the probe setup will need to be completed each time a probe is deployed - this is mainly due to the fact that in each environment in which it is deployed, the connectivity for the probe will vary (e.g. different SSID, different network connection type etc.). Here is an overview of the workflow:: Data server setup: Prepare a server platform Obtain the data server application software Install the data server application(s) Configure the data server application(s) Probe setup: Obtain a probe device (Raspberry Pi or WLAN Pi) Prepare the device for the wiperf software Install the wiperf software Configure the wiperf software Deploy & test the wiperf probe Links: Start here for Splunk: link Start here for InfluxDB/Grafana: link Start here for the probe: link In addition to the setup and deployment of the components, there may also be a requirement to troubleshoot the setup. The following pages provide useful support information: Troubleshooting steps Review known issues / FAQ ] Further Documentation References Configuration file parameters Data points sent by the probe to the data server platform Credits This project has had some great input from a number of people. Here are a few words of thanks to those who have been so generous in helping out. Thanks to Kristian Roberts for his invaluable project input, testing and guidance on Splunk. He kicked this whole thing off and it definitely wouldn't have happened without him. A top bloke. Thanks also to Eric Garnel and James Whitehead for their invaluable help in providing me so many Grafana dashboard files to \"borrow\" from. It was a steep learning curve for me, but the generosity of Eric & James really helped me to get to grips with Grafana. Also thanks to Eric for providing the idea to use InfluxDB as a data source (....even if I did use the wrong version initially! Lol). The code for the MOS score calculation was kindly provided by Mario Gingras. What a great idea...I wish I'd thought of that! Thanks Mario, it's great addition. Caveats This free software is provided for you to use at your own risk. There are no guarantees around its operation, suitability or accuracy of the data that it provides. Please consult the license file shipped with this software. Developer Nigel Bowden (WifiNigel): https://wifinigel.blogspot.com https://github.com/wifinigel","title":"Home"},{"location":"#wiperf-v2-an-open-source-ux-performance-probe","text":"Wiperf is a utility that can be installed on a WLAN Pi or a Raspberry Pi to act as a network probe that runs a series of network performance tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as an ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is (usually) no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where issues are being experienced and longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"wiperf V2: An Open Source UX Performance Probe"},{"location":"#data-server","text":"The core focus of this project is the probe device that gathers the network performance data in which we are interested. However, the data server is a critical component that allows visualization of that performance data. High-level configuration details will be provided to \"get you going\", but detailed information about the operation of these platforms is beyond the scope of this project. Both of the data servers supported are \"NoSQL\" servers, which means that no data structures have to be pre-defined in database tables. This means we can send our data structures, that contain network performance data, to the server with very little set-up compared to traditional database servers. As long as we have a valid set of credentials for the data server, we can just send JSON-formatted data over HTTPS in whatever structure we choose. A database query language on the data server allows us to retrieve and graph the data collected by the wiperf probe.","title":"Data Server"},{"location":"#splunk","text":"Splunk is supported on all popular operating systems and is very easy to set up on your server of choice. It acts as both the data store and visualization platform. Splunk is a commercial, rather than open-source product. The volume of data returned by the probe is very low, so the free tier of Splunk may be used to gather and report on data. For details on how to set up a Splunk server, start at this documentation page: [link][splunk_platform.md] Splunk product web site: https://www.splunk.com/","title":"Splunk"},{"location":"#influxdbgrafana","text":"Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data server from which to pull its network performance data. To meet this requirement, the InfluxDB database server is used. Like Grafana, InfluxDB is also an open-source package. For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of Influx as its data source. Grafana web site (v6.7): https://grafana.com/ Influx web site (v.1.8): https://www.influxdata.com","title":"InfluxDB/Grafana"},{"location":"#workflow-to-setup-wiperf","text":"The workflow to get Wiperf fully operational consists of a number of steps that break down in to two main areas: Probe setup (the RPi or WLAN Pi device itself) Data server setup (the Splunk or Influx/Grafana server) The data server setup tends to be a task that needs completion only once (or at least very infrequently). Conversely, some or all of the probe setup will need to be completed each time a probe is deployed - this is mainly due to the fact that in each environment in which it is deployed, the connectivity for the probe will vary (e.g. different SSID, different network connection type etc.). Here is an overview of the workflow:: Data server setup: Prepare a server platform Obtain the data server application software Install the data server application(s) Configure the data server application(s) Probe setup: Obtain a probe device (Raspberry Pi or WLAN Pi) Prepare the device for the wiperf software Install the wiperf software Configure the wiperf software Deploy & test the wiperf probe Links: Start here for Splunk: link Start here for InfluxDB/Grafana: link Start here for the probe: link In addition to the setup and deployment of the components, there may also be a requirement to troubleshoot the setup. The following pages provide useful support information: Troubleshooting steps Review known issues / FAQ ]","title":"Workflow to Setup Wiperf"},{"location":"#further-documentation-references","text":"Configuration file parameters Data points sent by the probe to the data server platform","title":"Further Documentation References"},{"location":"#credits","text":"This project has had some great input from a number of people. Here are a few words of thanks to those who have been so generous in helping out. Thanks to Kristian Roberts for his invaluable project input, testing and guidance on Splunk. He kicked this whole thing off and it definitely wouldn't have happened without him. A top bloke. Thanks also to Eric Garnel and James Whitehead for their invaluable help in providing me so many Grafana dashboard files to \"borrow\" from. It was a steep learning curve for me, but the generosity of Eric & James really helped me to get to grips with Grafana. Also thanks to Eric for providing the idea to use InfluxDB as a data source (....even if I did use the wrong version initially! Lol). The code for the MOS score calculation was kindly provided by Mario Gingras. What a great idea...I wish I'd thought of that! Thanks Mario, it's great addition.","title":"Credits"},{"location":"#caveats","text":"This free software is provided for you to use at your own risk. There are no guarantees around its operation, suitability or accuracy of the data that it provides. Please consult the license file shipped with this software.","title":"Caveats"},{"location":"#developer","text":"Nigel Bowden (WifiNigel): https://wifinigel.blogspot.com https://github.com/wifinigel","title":"Developer"},{"location":"about/","text":"About Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"About"},{"location":"about/#about","text":"Wiperf is a utility that can be installed on to a WLAN Pi or a Raspberry Pi to act as a network probe running a series of network tests. It is primarily intended to provide an indication of the end-user experience on a wireless network, but may also be used as a standalone ethernet-connected probe. The probe can run the following tests to give an indication of the performance of the network environment into which it has been deployed: Wireless connection health check (if wireless connected) Speedtest (Ookla) iperf3 (TCP & UDP tests) ICMP ping HTTP DNS DHCP Tests may be performed over the wireless or ethernet interface of the probe unit. The results must then be sent back to a Splunk or InfluxDB server (which we'll call the \"data server\") to provide a reporting capability. ( NOTE: There is no graphing/reporting capability on the wiperf probe itself ) Wiperf has been primarily designed to be a tactical tool for engineers to deploy on to a wireless network where perhaps issues are being experienced and some longer term monitoring may be required. It is not designed to replace large-scale commercial offerings that provide wireless and end-user experience monitoring in a far more comprehensive and user-friendly fashion. Tests are run on the wiperf probe at a configured interval (usually 5 minutes) and collected data is sent back to a data server over a network connection between the probe and data server (no connection = no data collection). The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"About"},{"location":"adv_proxy/","text":"Proxy Server If you need to deal with using a proxy on your network, please complete the details of your proxy by completing the following section in your /etc/wiperf/config.ini file: ; If proxy server access is required to run a speedtest, enter the proxy server details here for https & https ; e.g. https_proxy: http://10.1.1.1:8080 ; ; For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) ; e.g. no_proxy: \"mail.local, intranet.local\" http_proxy: https_proxy: no_proxy:","title":"Proxy Server"},{"location":"adv_proxy/#proxy-server","text":"If you need to deal with using a proxy on your network, please complete the details of your proxy by completing the following section in your /etc/wiperf/config.ini file: ; If proxy server access is required to run a speedtest, enter the proxy server details here for https & https ; e.g. https_proxy: http://10.1.1.1:8080 ; ; For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) ; e.g. no_proxy: \"mail.local, intranet.local\" http_proxy: https_proxy: no_proxy:","title":"Proxy Server"},{"location":"adv_remote_cfg/","text":"Remote Configuration Server In wiperf V2, we have added a rudimentary remote configuration server to allow the probe's config.ini file to be changed remotely. The relies on having a private repository in GutHub to store the remote configuration file(s). To help understand how this can work for you, and to understand the limitations of the solution, here is an overview of the process: A private GitHub repo must be created at GitHub - it must be private, otherwise the whole world can read your config files :) (See this doc for details on creating a private repo: https://docs.github.com/en/github/getting-started-with-github/create-a-repo ) An authorization token for the GitHub repo must be created to allow the probe to access it and read its config file Each time test cycle starts (i.e. every 5 mins), wiperf will check its local configuration file config.ini to see if a remote repository is configured If a remote repo is configured, then the wiperf process will check to see if it is time to check its remote config file - it doesn't check every poll cycle, to keep the network traffic overhead low. If it is time to check the config file, wiperf will pull the config file from the GutHub repo and overwrite its local config file with its newly retrieved file. This will be used for the next test cycle. The section of the config.ini file that controls remote repo usage is shown below: ; central configuration server details cfg_url: cfg_username: cfg_password: cfg_token: cfg_refresh_interval: See the following reference guide for an explanation of each field: config.ini reference guide Note : This is an advanced configuration option that requires thorough testing before you deploy your probe. Mis-configuration of your remote config file can cause significant operational issues.","title":"Remote Configuration Server"},{"location":"adv_remote_cfg/#remote-configuration-server","text":"In wiperf V2, we have added a rudimentary remote configuration server to allow the probe's config.ini file to be changed remotely. The relies on having a private repository in GutHub to store the remote configuration file(s). To help understand how this can work for you, and to understand the limitations of the solution, here is an overview of the process: A private GitHub repo must be created at GitHub - it must be private, otherwise the whole world can read your config files :) (See this doc for details on creating a private repo: https://docs.github.com/en/github/getting-started-with-github/create-a-repo ) An authorization token for the GitHub repo must be created to allow the probe to access it and read its config file Each time test cycle starts (i.e. every 5 mins), wiperf will check its local configuration file config.ini to see if a remote repository is configured If a remote repo is configured, then the wiperf process will check to see if it is time to check its remote config file - it doesn't check every poll cycle, to keep the network traffic overhead low. If it is time to check the config file, wiperf will pull the config file from the GutHub repo and overwrite its local config file with its newly retrieved file. This will be used for the next test cycle. The section of the config.ini file that controls remote repo usage is shown below: ; central configuration server details cfg_url: cfg_username: cfg_password: cfg_token: cfg_refresh_interval: See the following reference guide for an explanation of each field: config.ini reference guide Note : This is an advanced configuration option that requires thorough testing before you deploy your probe. Mis-configuration of your remote config file can cause significant operational issues.","title":"Remote Configuration Server"},{"location":"adv_rpi_standalone/","text":"RPi Standalone Probe With Reporting The wiperf project is primarily concerned with the functionality of the probe function that performs network tests to give an indication of what the user network experience looks like. It provides data to either Splunk or InfluxDB/Grafana that provide the reporting function. It is generally recommended that the probe be deployed with a centrally located, remote, reporting server. However, it may be useful to run the probe as a standalone device with the reporting features also installed on the probe. This cannot be done with Splunk, as it cannot run its server software on the RPi. testing has shown that InfluxDB and Grafana can be successfully installed on to an RPi so that the probe and reporting features can all co-exist on one device. This has been tested with an RPi 3B+, but will likely be fine with all subsequent models of RPi. It is not recommended to install Grafana and InfluxDB on to an RPi to act as a centralised server for multiple probes, but providing reporting for a local probe on the same RPi seems to work OK on initial testing. The only caveat is that I would recommend setting a data retention policy as outlined below to ensure the InfluxDB does not consume too much space on the RPi as data is gathered. Note: The steps outlined below are notes taken from initial testing. This is an advanced level topic that requires that you are familiar with Linux commands and the operation of wiperf. Apologies there is very concise information provided, but this really is an advanced level topic that I cannot generally provide support for. ############################# # All steps done on RPi CLI ############################# # update RPi packages (assumes Raspian Buster) sudo apt update sudo apt upgrade sudo reboot # Install wiperf: curl -s https://raw.githubusercontent.com/wifinigel/wiperf/setup.sh | sudo bash -s install rpi # Add cron job (use entry shown) sudo crontab -e # add entry: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 # Add InfluxDB key & repo sudo wget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add - echo \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list sudo apt update sudo apt install influxdb # Once installed, set InfluxDB to start up & start process on boot sudo systemctl unmask influxdb sudo systemctl enable influxdb sudo systemctl start influxdb # Add pre-reqs for Grafan Grafana, get & install pkg sudo apt-get install -y adduser libfontconfig1 wget https://dl.grafana.com/oss/release/grafana-rpi_6.7.4_armhf.deb sudo dpkg -i grafana-rpi_6.7.4_armhf.deb # Once Grafana installed, set for startup & start process sudo systemctl enable grafana-server sudo systemctl start grafana-server # On RPI CLI, prepare Influx DB for data using Influx CLI client: influx > create database wiperf > create retention policy \"wiperf_30_days\" on \"wiperf\" duration 30d replication 1 > CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES > exit # Edit InfluxDB to use login auth & restart processes to activate sudo nano /etc/influxdb/influxdb.conf (uncomment \"# auth-enabled = false\" -> \"auth-enabled = true\") sudo systemctl restart influxdb # On RPI CLI, drop in to Influx CLI client again influx -username admin -password letmein > CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' > GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" > CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' > GRANT read ON \"wiperf\" TO \"grafana\" # Edit wiperf config file to use local mgt i/f, Influx DB & loging credentials sudo nano /etc/wiperf/config.ini mgt_if: lo exporter_type: influxdb influx_username: wiperf_probe influx_password: s3cr3tpwd99 influx_host: 127.0.0.1 # Grafana GUI (these steps done from browser): Browser: http:<ip>:3000 (login admin/admin, changed on first login) In web GUI, add datasource: Configuration > Datasources - Type: InfluxDB - Name: WiperfDB - URL: htttp://127.0.0.1:8086 - Access: Server - Database: wiperf - User: grafana - Password: R34dth3DB Obtain the Grafana dashboard files from the wiperf dashboards folder and place on your browser machine (/usr/share/wiperf/dashboards) In web GUI, import dashboards: + Create > Import > Upload .json file > select dashboard file","title":"RPi Standalone Probe With Reporting"},{"location":"adv_rpi_standalone/#rpi-standalone-probe-with-reporting","text":"The wiperf project is primarily concerned with the functionality of the probe function that performs network tests to give an indication of what the user network experience looks like. It provides data to either Splunk or InfluxDB/Grafana that provide the reporting function. It is generally recommended that the probe be deployed with a centrally located, remote, reporting server. However, it may be useful to run the probe as a standalone device with the reporting features also installed on the probe. This cannot be done with Splunk, as it cannot run its server software on the RPi. testing has shown that InfluxDB and Grafana can be successfully installed on to an RPi so that the probe and reporting features can all co-exist on one device. This has been tested with an RPi 3B+, but will likely be fine with all subsequent models of RPi. It is not recommended to install Grafana and InfluxDB on to an RPi to act as a centralised server for multiple probes, but providing reporting for a local probe on the same RPi seems to work OK on initial testing. The only caveat is that I would recommend setting a data retention policy as outlined below to ensure the InfluxDB does not consume too much space on the RPi as data is gathered. Note: The steps outlined below are notes taken from initial testing. This is an advanced level topic that requires that you are familiar with Linux commands and the operation of wiperf. Apologies there is very concise information provided, but this really is an advanced level topic that I cannot generally provide support for. ############################# # All steps done on RPi CLI ############################# # update RPi packages (assumes Raspian Buster) sudo apt update sudo apt upgrade sudo reboot # Install wiperf: curl -s https://raw.githubusercontent.com/wifinigel/wiperf/setup.sh | sudo bash -s install rpi # Add cron job (use entry shown) sudo crontab -e # add entry: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 # Add InfluxDB key & repo sudo wget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add - echo \"deb https://repos.influxdata.com/debian buster stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list sudo apt update sudo apt install influxdb # Once installed, set InfluxDB to start up & start process on boot sudo systemctl unmask influxdb sudo systemctl enable influxdb sudo systemctl start influxdb # Add pre-reqs for Grafan Grafana, get & install pkg sudo apt-get install -y adduser libfontconfig1 wget https://dl.grafana.com/oss/release/grafana-rpi_6.7.4_armhf.deb sudo dpkg -i grafana-rpi_6.7.4_armhf.deb # Once Grafana installed, set for startup & start process sudo systemctl enable grafana-server sudo systemctl start grafana-server # On RPI CLI, prepare Influx DB for data using Influx CLI client: influx > create database wiperf > create retention policy \"wiperf_30_days\" on \"wiperf\" duration 30d replication 1 > CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES > exit # Edit InfluxDB to use login auth & restart processes to activate sudo nano /etc/influxdb/influxdb.conf (uncomment \"# auth-enabled = false\" -> \"auth-enabled = true\") sudo systemctl restart influxdb # On RPI CLI, drop in to Influx CLI client again influx -username admin -password letmein > CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' > GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" > CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' > GRANT read ON \"wiperf\" TO \"grafana\" # Edit wiperf config file to use local mgt i/f, Influx DB & loging credentials sudo nano /etc/wiperf/config.ini mgt_if: lo exporter_type: influxdb influx_username: wiperf_probe influx_password: s3cr3tpwd99 influx_host: 127.0.0.1 # Grafana GUI (these steps done from browser): Browser: http:<ip>:3000 (login admin/admin, changed on first login) In web GUI, add datasource: Configuration > Datasources - Type: InfluxDB - Name: WiperfDB - URL: htttp://127.0.0.1:8086 - Access: Server - Database: wiperf - User: grafana - Password: R34dth3DB Obtain the Grafana dashboard files from the wiperf dashboards folder and place on your browser machine (/usr/share/wiperf/dashboards) In web GUI, import dashboards: + Create > Import > Upload .json file > select dashboard file","title":"RPi Standalone Probe With Reporting"},{"location":"adv_secure/","text":"Security Hardening WLAN Pi Wiperf employs the following security mechanisms in an attempt to harden the WLAN Pi when deployed in wiperf mode: No forwarding is allowed between interfaces The internal UFW firewall is configured to only allow incoming connectivity on port 22 on the wlan0 & eth0 interfaces RPi If you'd like to harden the RPi when deployed in a network, a quick solution is to install & activate the 'ufw' firewall. This can be configured to stop all incoming connections except those on SSH, which will still allow remote administration. All outgoing traffic from the probe (i.e. network tests and management traffic) will not be disrupted. Install ufw apt-get update apt-get install ufw Add Firewall Rules ufw allow in on eth0 to any port ssh ufw deny in on eth0 ufw allow in on wlan0 to any port ssh ufw deny in on wlan0 Activate Firewall ufw enable Useful Commands # Disable the firewall comletely ufw enable # List fw rules with numbers ufw status numbered # See firewall status ufw status","title":"Security Hardening"},{"location":"adv_secure/#security-hardening","text":"","title":"Security Hardening"},{"location":"adv_secure/#wlan-pi","text":"Wiperf employs the following security mechanisms in an attempt to harden the WLAN Pi when deployed in wiperf mode: No forwarding is allowed between interfaces The internal UFW firewall is configured to only allow incoming connectivity on port 22 on the wlan0 & eth0 interfaces","title":"WLAN Pi"},{"location":"adv_secure/#rpi","text":"If you'd like to harden the RPi when deployed in a network, a quick solution is to install & activate the 'ufw' firewall. This can be configured to stop all incoming connections except those on SSH, which will still allow remote administration. All outgoing traffic from the probe (i.e. network tests and management traffic) will not be disrupted.","title":"RPi"},{"location":"adv_secure/#install-ufw","text":"apt-get update apt-get install ufw","title":"Install ufw"},{"location":"adv_secure/#add-firewall-rules","text":"ufw allow in on eth0 to any port ssh ufw deny in on eth0 ufw allow in on wlan0 to any port ssh ufw deny in on wlan0","title":"Add Firewall Rules"},{"location":"adv_secure/#activate-firewall","text":"ufw enable","title":"Activate Firewall"},{"location":"adv_secure/#useful-commands","text":"# Disable the firewall comletely ufw enable # List fw rules with numbers ufw status numbered # See firewall status ufw status","title":"Useful Commands"},{"location":"config.ini/","text":"config.ini Reference Guide Background The config.ini file controls the operation of the wiperf utility. It has many options available for maximum flexibility, but some may need some clarification. Many options will be fine using the defaults that are supplied with the installed package. However, some will definitely require configuration as they may require values such as IP addresses and port numbers which will vary in each instance where wiperf is used. The config.ini file is located in the directory : /etc/wiperf The file is organised in a number of sections that relate to different areas of operation. Each section begins with a name enclosed is square brackets, like this: [Speedtest] Within each section are a number of configurable parameters that are in the format: parameter: value You may also see some lines that begin with a semi-colon. These are comments and have no effect on the operation of wiperf. You may add, remove or change these as you wish. Here is an example comment: ; wlan interface name set this as per the output of an iwconfig command (usually wlan0) Parameter Reference Guide We'll take a look at each section of the config file and provide some guidance on suitable parameter values: General Section probe_mode eth_if wlan_if mgt_if platform exporter_type splunk_host splunk_port splunk_token influx_host influx_port influx_username influx_password influx_database influx2_host influx2_port influx2_token influx2_bucket influx2_org test_interval test_offset connectivity_lookup location data_format data_dir data_transport debug cfg_url cfg_username cfg_password cfg_token cfg_refresh_interval unit_bouncer Network_Test Section network_data_file Speetest Section enabled server_id http_proxy https_proxy no_proxy speedtest_data_file Ping_Test Section enabled ping_host1 ping_host2 ping_host3 ping_host4 ping_host5 ping_count ping_data_file Iperf3_tcp_test Section enabled server_hostname port duration iperf3_tcp_data_file Iperf3_udp_test Section enabled server_hostname port duration bandwidth iperf3_udp_data_file DNS_test Section enabled dns_target1 dns_target2 dns_target3 dns_target4 dns_target5 dns_data_file HTTP_test Section enabled http_target1 http_target2 http_target3 http_target4 http_target5 http_data_file DHCP_test Section enabled mode dhcp_data_file [General] Section Note: any changes to this section on the WLAN Pi should only be made when it is running in classic mode (not while in wiperf mode). probe_mode The probe may be run in one of two modes: wireless ethernet In 'wireless' mode, all tests are run over the Wi-Fi NIC interface to test wireless connectivity & performance. In 'ethernet' mode, all tests are performed over the wired, ethernet interface. Default setting: probe_mode: wireless top eth_if This parameter contains the name of the ethernet interface on the probe. This will almost always be 'eth0', but is provided in case of new use-cases in the future. You can see the Ethernet interface name by running the 'ifconfig' command from the CLI of the probe. Default setting: eth_if: eth0 top wlan_if This parameter contains the name of the WLAN interface on the probe. This will almost always be 'wlan0', but is provided in case of new use-cases in the future. You can see the WLAN interface name by running the 'ifconfig' command from the CLI of the probe. Default setting: wlan_if: wlan0 top mgt_if When performance tests have been completed, the results data needs to be sent to a reporting server (e.g. Splunk/InfluxDb). This parameter configures the interface over which this management traffic needs to be sent. Getting this parameter correct for your environment is very important to ensure that test results data makes it back to your reporting server. The available options are: wlan0 (the first available WLAN port - usually a USB dongle plugged in to the WLAN Pi, or the internal wireless NIC on the RPi) eth0 (the internal Ethernet port of the probe) zt (Zerotier (the virtual network service) is installed and used to connect to the reporting server) The WANPi is configured to assign a higher cost default route to eth0 by default so that all traffic (tests & test results) will choose the default route provided by wlan0. If eth0 is used as the path to return test results to the reporting server, then a static route is injected in to the probe route table on start-up to ensure correct routing. If this parameter is not correctly set, then results data may not make it back to the reporting server. Default setting: mgt_if: wlan0 top platform (Deprecated) (This setting is now deprecated (and unused) - it has been included for historical reference) Wiperf is supported on both the WLAN Pi and Raspberry Pi platforms. The available options are: wlanpi rpi Default setting: platform: wlanpi top exporter_type Wiperf supports a number of remote data repositories that can be used as targets to store test result data. The available options are: splunk influxdb influxdb2 Default setting: exporter_type: splunk top splunk_host This is the hostname or IP address of the Splunk platform where test result data is sent to. If the hostname of the Splunk server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): splunk_host: top splunk_port The network port used to send updates to the Splunk server. By default this is 8088, but this may be changed within the Splunk application if an alternative port is required for your environment Default setting: splunk_port: 8088 top splunk_token Splunk will only receive HEC updates from devices that are authorised to send it data. Splunk uses tokens to decide if an update is from a valid device. To view available (or create) tokens within Splunk, view the menu option: \"Settings > Data > Data Inputs > HTTP Event Collector\" Here is example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d Default setting (none): splunk_token: top influx_host This is the hostname or IP address of the Influx (v1.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): influx_host: top influx_port The network port used to send updates to the Influx (v1.x) server. By default this is 8086, but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx_port: 8086 top influx_username The username that will be used to access the Influx (v1.x) server DB to post results data. This username must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_username: top influx_password The password that will be used to access the Influx (v1.x) server DB to post results data. This password must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_password: top influx_database The name of the database on the Influx (v1.x) server DB where wiperf will post results data. This database must have been created on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_database: top influx2_host This is the hostname or IP address of the Influx (v2.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your InfluxDb2 server in the Zerotier dashboard for your network) Default setting (none): influx2_host: top influx2_port The network port used to send updates to the Influx (v2.x) server. By default this is 443 (this assumes the cloud service is used), but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx2_port: 443 top influx2_token InfluxDB2 allows the use of authentication tokens when sending results data to the InfluxDB2 server. This provides an easier authentication methods than using a username and password. Once a token has been created on InfluxDB server, it can be used by wiperf to authenticate the results data sent to the InfluxDB2 server Default setting (none): influx2_token: top influx2_bucket Data sent to the InfluxDB2 server from wiperf is stored in a \"bucket\" in the data store. This field is used to configure the bucket to which wiperf should send it's data. Default setting (none): influx2_bucket: top influx2_org The InfluxDB2 server can be partitioned in to a number of organizations, which contain the buckets where data will be stored. Use this field to configure wiperf to send data to the correct organisation on InfluxDB2. Default setting (none): influx2_org: top test_interval (WLAN Pi only) This is the interval (in minutes) at which we would like to run the performance tests. The recommened minimum is 5, which is also the default. (Note: if this setting is too low, scheduled tests may try to run before the previous test sequence has completed, which could cause gaps in your data) Default setting: test_interval: 5 top test_offset (WLAN Pi only) By default test run at the interval specified by the test_interval parameter, which is referenced to the to of the hours (e.g. 5 mins interval will run at 5, 10, 15, 20, 25...etc. mins past the hour). If multiple proes are running, it mau be useful to stagger their start times. By setting test_offset to a value of one, this will offset all test start times by 1 minutes (i.e. 6,11,16,21,26...etc. mins past the hour) The default value is zero which means that the default 5,10,15,20... run pattern will be used. Default setting: test_offset: 0 top connectivity_lookup At the start of each test cycle, a DNS lookup is performed to ensure that DNS is working. By default this is 'google.com' (this was 'bbc.co.uk' on older versions of wiperf). This may be set to any required hostname lookup in instances when the default site may not be available for some reason (e.g. DNS restrictions due to filtering or lack of Internet access) Default setting: connectivity_lookup: google.com top location This is a string that can be added to assist with report filtering, if required. Its default value in an empty string. It could be be used in an expression within your reports to filter units based on a location field (for instance) Default setting: location: top data_format (Not currently operational) wiperf has the capability to output data in a number of formats. The current options are: csv or json However this field is not currently used, as selecting the 'hec' transport mode (the only supported transport currently) over-rides this field. The value in this filed is currently irrelevant, but it s recommended to leave it at the default setting of json Default setting: data_format: json top data_dir This is the directory on the WLAN Pi/RPi where test result data is dumped. Do not change this value from the default . This field is provided for future configuration options if required. Default setting: data_dir: /home/wlanpi/wiperf/data top data_transport The currently supported data transport mode is hec . This is the HTTP Event Collector supported natively within the Splunk server. Other transport modes will be suported in the future, but currently this should be left at the default setting of `hec . (Note: the transport method forwarder is also a valid transport method which provides support for very early versions of this code which used the Splunk Univeral Forwarder. Use of this method is deprecated and will be removed in the near future. Anyone still using the UF should move to using hec ASAP) Default setting: data_transport: hec top debug To enable enhanced logging in the agent.log file, change this setting to \"on\" Default setting: debug: off top cfg_url If using centralized configuration file retrieval, this field specifies the full URL of the config file on the remote repo. (Note that on GitHub this is the URL of the raw file itself) If this field is not set, then centralized configuration retrieval is disabled Default setting (none): cfg_url: top cfg_username If username/pasword credentials are used to retrieve the centralized config, this field specifies the usename to be used. (Note: using an access token is a MUCH better idea...see below) Default setting (none): cfg_username: top cfg_password If username/pasword credentials are used to retrieve the centralized config, this field specifies the password to be used. (Note: using an access token is a MUCH better idea...see below) Default setting (none): cfg_password: top cfg_token If a GitHub authentication token is used to retrieve the centralized config, this field specifies the token to be used. (Note: this is used instead of a username/pwd) Check out this page to find out more about creating access tokens: https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token Default setting (none): cfg_token: top cfg_refresh_interval This field specifies how often (in seconds) the centralized config file should be retrieved . Recommended value: 900 (i.e. 15 mins) Default setting (none): cfg_refresh_interval: top unit_bouncer If you need to bounce (reboot) the unit for some reason on a regular basis, this field can be used to signal to the WLAN Pi each hour at which it must reboot. The field is a comma separated string that lists the hours at which the unit must reboot (in 24-hour format). The number-format and comma separation are important to get right! Note that the reboot is not exactly on the hour, but will occur at the end of the next test cycle that it is within the hour where a reboot is required. It will only happen once per hour. Example: the following config will reboot at midnight, 04:00, 08:00, 12:00, 16:00: unit_bouncer: 00, 06, 12, 18 This parameter is commented out by default as it is obviously not something you necessarilly want to switch on accidentally. Default setting: ; unit_bouncer: 00, 06, 12, 18 top [Network_Test] Section network_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for network tests in remote data repositories (e.g. Splunk, InfluxDB) Default setting: network_data_file: wiperf-network top [Speedtest] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no Speedtest is run. When enabled, a speedtest to the Ookla speedtest service is run each test cycle. Default setting: enabled: yes top server_id If you wish to specify a particular Ookla speedtest server that the test needs to be run against, you can enter its ID here. This must be the (numeric) server ID of a specific Ookla server taken from : https://c.speedtest.net/speedtest-servers-static.php Note this must be the number (NOT url!) taken from the field id=\"xxxxx\". If no value is specified, best server is used (default) Default setting: server_id: top http_proxy https_proxy no_proxy If proxy server access is required to run a speedtest, enter the proxy server details here for https & https e.g. https_proxy: http://10.1.1.1:8080 For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) e.g. no_proxy: \"mail.local, intranet.local\" Default settings: http_proxy: https_proxy: no_proxy: top speedtest_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for Speedtests in the reporting server (e.g. Splunk/InfluxDB) Default setting: speedtest_data_file: wiperf-speedtest top [Ping_Test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no ping tests are run. When enabled, up to 5 entries will be targetted with an ICMP ping and the RRT times recorded Default setting: enabled: yes top ping_host1 IP address or hostname of first ping target. No target details = no test run Default setting: ping_host1: google.com top ping_host2 IP address or hostname of second ping target. No target details = no test run Default setting: ping_host2: cisco.com top ping_host3 IP address or hostname of third ping target. No target details = no test run Default setting: ping_host3: top ping_host4 IP address or hostname of fourth ping target. No target details = no test run Default setting: ping_host4: top ping_host5 IP address or hostname of fifth ping target. No target details = no test run Default setting: ping_host2: top ping_count The number of pings to send for each ping target Default setting: ping_count: 10 top ping_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for ping tests in the reporting server (e.g. Splunk.InfuxDB) Default setting: ping_data_file: wiperf-ping top [Iperf3_tcp_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no tcp iperf3 test is run. When enabled, a tcp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs) Default setting: enabled: yes top server_hostname The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: top port The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top duration The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 10 top iperf3_tcp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for tcp iperf3 tests in Splunk Default setting: iperf3_tcp_data_file: wiperf-iperf3-tcp top [Iperf3_udp_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no udp iperf3 test is run. When enabled, a udp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs), attempting to achieve a data transfer rate of bandwidth bps. Default setting: enabled: yes top server_hostname The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: top port The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top duration The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 10 top bandwidth The data rate that will be attempted for the UDP iperf3 test in bps Default setting: bandwidth: 2000000 top iperf3_udp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for udp iperf3 tests in Splunk Default setting: iperf3_udp_data_file: wiperf-iperf3-udp top [DNS_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no DNS tests are run. When enabled, DNS tests are run for each of the dns_target paramters defined in this section. Any targets that have no value entered will be ignored. Default setting: enabled: yes top dns_target1 Hostname of first DNS target. No target details = no test run Default setting: dns_target1: google.com top dns_target2 Hostname of second DNS target. No target details = no test run Default setting: dns_target2: cisco.com top dns_target3 Hostname of third DNS target. No target details = no test run Default setting: dns_target3: top dns_target4 Hostname of fourth DNS target. No target details = no test run Default setting: dns_target4: top dns_target5 Hostname of fifth DNS target. No target details = no test run Default setting: dns_target5: top dns_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DNS tests in Splunk Default setting: dns_data_file: wiperf-dns top [HTTP_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no HTTP tests are run. When enabled, HTTP tests are run for each of the http_target paramters defined in this section. Any targets that have no value entered will be ignored. Targets must include the full url of each site to be queried (including http:// or https:// element). Valid site address examples: http://bbc.co.uk https://ebay.com A http get will be performed for each target and the result code returned. Default setting: enabled: yes top http_target1 Hostname of first HTTP target. No target details = no test run Default setting: http_target1: https://google.com top http_target2 Hostname of second HTTP target. No target details = no test run Default setting: http_target2: https://cisco.com top http_target3 Hostname of third HTTP target. No target details = no test run Default setting: http_target3: top http_target4 Hostname of fourth HTTP target. No target details = no test run Default setting: http_target4: top http_target5 Hostname of fifth HTTP target. No target details = no test run Default setting: http_target5: top http_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for HTTP tests in Splunk Default setting: http_data_file: wiperf-http top [DHCP_test] Section (Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi) enabled Options: yes or no. If set to no, entire section is ignored and no DHCP test is run. Note that the DHCP test has 2 modes : passive: only a renewal request is sent (no release of IP) active: a release and renew request is performed. Note that the active setting has shown varying degrees of usefulness in esting. In some scenarios (e.g. when connected via ZeroTier), it has caused connectivity issues, hence the passive setting is a better choice. Obviously, the passive setting does not perform such a rigorous DHCP test and is completed much quicker than the active mode. However, it still provides a useful comparative measure of the reponsivemess of DHCP servers. Default setting: enabled: yes top mode (deprecated) Note: This setting has been removed as it caused probe connectivity issues. The probe now only operates in the passive mode. These notes have bene left in for reference for those who used older versions of code or old configuration file. This setting is silently ignored if supplied. Available options: passive active The active settings performs a full release/request and may be disruptve to connectivity - use with caution. The passive setting is the recommended option for most situations. Default setting: mode: passive top dhcp_data_file (Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DHCP tests in Splunk Default setting: dhcp_data_file: wiperf-dhcp top","title":"config.ini Reference Guide"},{"location":"config.ini/#configini-reference-guide","text":"","title":"config.ini Reference Guide"},{"location":"config.ini/#background","text":"The config.ini file controls the operation of the wiperf utility. It has many options available for maximum flexibility, but some may need some clarification. Many options will be fine using the defaults that are supplied with the installed package. However, some will definitely require configuration as they may require values such as IP addresses and port numbers which will vary in each instance where wiperf is used. The config.ini file is located in the directory : /etc/wiperf The file is organised in a number of sections that relate to different areas of operation. Each section begins with a name enclosed is square brackets, like this: [Speedtest] Within each section are a number of configurable parameters that are in the format: parameter: value You may also see some lines that begin with a semi-colon. These are comments and have no effect on the operation of wiperf. You may add, remove or change these as you wish. Here is an example comment: ; wlan interface name set this as per the output of an iwconfig command (usually wlan0)","title":"Background"},{"location":"config.ini/#parameter-reference-guide","text":"We'll take a look at each section of the config file and provide some guidance on suitable parameter values: General Section probe_mode eth_if wlan_if mgt_if platform exporter_type splunk_host splunk_port splunk_token influx_host influx_port influx_username influx_password influx_database influx2_host influx2_port influx2_token influx2_bucket influx2_org test_interval test_offset connectivity_lookup location data_format data_dir data_transport debug cfg_url cfg_username cfg_password cfg_token cfg_refresh_interval unit_bouncer Network_Test Section network_data_file Speetest Section enabled server_id http_proxy https_proxy no_proxy speedtest_data_file Ping_Test Section enabled ping_host1 ping_host2 ping_host3 ping_host4 ping_host5 ping_count ping_data_file Iperf3_tcp_test Section enabled server_hostname port duration iperf3_tcp_data_file Iperf3_udp_test Section enabled server_hostname port duration bandwidth iperf3_udp_data_file DNS_test Section enabled dns_target1 dns_target2 dns_target3 dns_target4 dns_target5 dns_data_file HTTP_test Section enabled http_target1 http_target2 http_target3 http_target4 http_target5 http_data_file DHCP_test Section enabled mode dhcp_data_file","title":"Parameter Reference Guide"},{"location":"config.ini/#general-section","text":"Note: any changes to this section on the WLAN Pi should only be made when it is running in classic mode (not while in wiperf mode).","title":"[General] Section"},{"location":"config.ini/#probe_mode","text":"The probe may be run in one of two modes: wireless ethernet In 'wireless' mode, all tests are run over the Wi-Fi NIC interface to test wireless connectivity & performance. In 'ethernet' mode, all tests are performed over the wired, ethernet interface. Default setting: probe_mode: wireless top","title":"probe_mode"},{"location":"config.ini/#eth_if","text":"This parameter contains the name of the ethernet interface on the probe. This will almost always be 'eth0', but is provided in case of new use-cases in the future. You can see the Ethernet interface name by running the 'ifconfig' command from the CLI of the probe. Default setting: eth_if: eth0 top","title":"eth_if"},{"location":"config.ini/#wlan_if","text":"This parameter contains the name of the WLAN interface on the probe. This will almost always be 'wlan0', but is provided in case of new use-cases in the future. You can see the WLAN interface name by running the 'ifconfig' command from the CLI of the probe. Default setting: wlan_if: wlan0 top","title":"wlan_if"},{"location":"config.ini/#mgt_if","text":"When performance tests have been completed, the results data needs to be sent to a reporting server (e.g. Splunk/InfluxDb). This parameter configures the interface over which this management traffic needs to be sent. Getting this parameter correct for your environment is very important to ensure that test results data makes it back to your reporting server. The available options are: wlan0 (the first available WLAN port - usually a USB dongle plugged in to the WLAN Pi, or the internal wireless NIC on the RPi) eth0 (the internal Ethernet port of the probe) zt (Zerotier (the virtual network service) is installed and used to connect to the reporting server) The WANPi is configured to assign a higher cost default route to eth0 by default so that all traffic (tests & test results) will choose the default route provided by wlan0. If eth0 is used as the path to return test results to the reporting server, then a static route is injected in to the probe route table on start-up to ensure correct routing. If this parameter is not correctly set, then results data may not make it back to the reporting server. Default setting: mgt_if: wlan0 top","title":"mgt_if"},{"location":"config.ini/#platform-deprecated","text":"(This setting is now deprecated (and unused) - it has been included for historical reference) Wiperf is supported on both the WLAN Pi and Raspberry Pi platforms. The available options are: wlanpi rpi Default setting: platform: wlanpi top","title":"platform (Deprecated)"},{"location":"config.ini/#exporter_type","text":"Wiperf supports a number of remote data repositories that can be used as targets to store test result data. The available options are: splunk influxdb influxdb2 Default setting: exporter_type: splunk top","title":"exporter_type"},{"location":"config.ini/#splunk_host","text":"This is the hostname or IP address of the Splunk platform where test result data is sent to. If the hostname of the Splunk server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): splunk_host: top","title":"splunk_host"},{"location":"config.ini/#splunk_port","text":"The network port used to send updates to the Splunk server. By default this is 8088, but this may be changed within the Splunk application if an alternative port is required for your environment Default setting: splunk_port: 8088 top","title":"splunk_port"},{"location":"config.ini/#splunk_token","text":"Splunk will only receive HEC updates from devices that are authorised to send it data. Splunk uses tokens to decide if an update is from a valid device. To view available (or create) tokens within Splunk, view the menu option: \"Settings > Data > Data Inputs > HTTP Event Collector\" Here is example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d Default setting (none): splunk_token: top","title":"splunk_token"},{"location":"config.ini/#influx_host","text":"This is the hostname or IP address of the Influx (v1.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your Splunk server in the Zerotier dashboard for your network) Default setting (none): influx_host: top","title":"influx_host"},{"location":"config.ini/#influx_port","text":"The network port used to send updates to the Influx (v1.x) server. By default this is 8086, but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx_port: 8086 top","title":"influx_port"},{"location":"config.ini/#influx_username","text":"The username that will be used to access the Influx (v1.x) server DB to post results data. This username must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_username: top","title":"influx_username"},{"location":"config.ini/#influx_password","text":"The password that will be used to access the Influx (v1.x) server DB to post results data. This password must be configured on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_password: top","title":"influx_password"},{"location":"config.ini/#influx_database","text":"The name of the database on the Influx (v1.x) server DB where wiperf will post results data. This database must have been created on the InfluxDB server prior to wiper sending results data to the InfluxDB server. Default setting (None): influx_database: top","title":"influx_database"},{"location":"config.ini/#influx2_host","text":"This is the hostname or IP address of the Influx (v2.x) platform where test result data is sent to. If the hostname of the Influx server is used, it must be resolvable by the probe. (Note: If using Zerotier, make sure this is the address of the IP assigned to your InfluxDb2 server in the Zerotier dashboard for your network) Default setting (none): influx2_host: top","title":"influx2_host"},{"location":"config.ini/#influx2_port","text":"The network port used to send updates to the Influx (v2.x) server. By default this is 443 (this assumes the cloud service is used), but this may be changed within the Influx application if an alternative port is required for your environment Default setting: influx2_port: 443 top","title":"influx2_port"},{"location":"config.ini/#influx2_token","text":"InfluxDB2 allows the use of authentication tokens when sending results data to the InfluxDB2 server. This provides an easier authentication methods than using a username and password. Once a token has been created on InfluxDB server, it can be used by wiperf to authenticate the results data sent to the InfluxDB2 server Default setting (none): influx2_token: top","title":"influx2_token"},{"location":"config.ini/#influx2_bucket","text":"Data sent to the InfluxDB2 server from wiperf is stored in a \"bucket\" in the data store. This field is used to configure the bucket to which wiperf should send it's data. Default setting (none): influx2_bucket: top","title":"influx2_bucket"},{"location":"config.ini/#influx2_org","text":"The InfluxDB2 server can be partitioned in to a number of organizations, which contain the buckets where data will be stored. Use this field to configure wiperf to send data to the correct organisation on InfluxDB2. Default setting (none): influx2_org: top","title":"influx2_org"},{"location":"config.ini/#test_interval","text":"(WLAN Pi only) This is the interval (in minutes) at which we would like to run the performance tests. The recommened minimum is 5, which is also the default. (Note: if this setting is too low, scheduled tests may try to run before the previous test sequence has completed, which could cause gaps in your data) Default setting: test_interval: 5 top","title":"test_interval"},{"location":"config.ini/#test_offset","text":"(WLAN Pi only) By default test run at the interval specified by the test_interval parameter, which is referenced to the to of the hours (e.g. 5 mins interval will run at 5, 10, 15, 20, 25...etc. mins past the hour). If multiple proes are running, it mau be useful to stagger their start times. By setting test_offset to a value of one, this will offset all test start times by 1 minutes (i.e. 6,11,16,21,26...etc. mins past the hour) The default value is zero which means that the default 5,10,15,20... run pattern will be used. Default setting: test_offset: 0 top","title":"test_offset"},{"location":"config.ini/#connectivity_lookup","text":"At the start of each test cycle, a DNS lookup is performed to ensure that DNS is working. By default this is 'google.com' (this was 'bbc.co.uk' on older versions of wiperf). This may be set to any required hostname lookup in instances when the default site may not be available for some reason (e.g. DNS restrictions due to filtering or lack of Internet access) Default setting: connectivity_lookup: google.com top","title":"connectivity_lookup"},{"location":"config.ini/#location","text":"This is a string that can be added to assist with report filtering, if required. Its default value in an empty string. It could be be used in an expression within your reports to filter units based on a location field (for instance) Default setting: location: top","title":"location"},{"location":"config.ini/#data_format","text":"(Not currently operational) wiperf has the capability to output data in a number of formats. The current options are: csv or json However this field is not currently used, as selecting the 'hec' transport mode (the only supported transport currently) over-rides this field. The value in this filed is currently irrelevant, but it s recommended to leave it at the default setting of json Default setting: data_format: json top","title":"data_format"},{"location":"config.ini/#data_dir","text":"This is the directory on the WLAN Pi/RPi where test result data is dumped. Do not change this value from the default . This field is provided for future configuration options if required. Default setting: data_dir: /home/wlanpi/wiperf/data top","title":"data_dir"},{"location":"config.ini/#data_transport","text":"The currently supported data transport mode is hec . This is the HTTP Event Collector supported natively within the Splunk server. Other transport modes will be suported in the future, but currently this should be left at the default setting of `hec . (Note: the transport method forwarder is also a valid transport method which provides support for very early versions of this code which used the Splunk Univeral Forwarder. Use of this method is deprecated and will be removed in the near future. Anyone still using the UF should move to using hec ASAP) Default setting: data_transport: hec top","title":"data_transport"},{"location":"config.ini/#debug","text":"To enable enhanced logging in the agent.log file, change this setting to \"on\" Default setting: debug: off top","title":"debug"},{"location":"config.ini/#cfg_url","text":"If using centralized configuration file retrieval, this field specifies the full URL of the config file on the remote repo. (Note that on GitHub this is the URL of the raw file itself) If this field is not set, then centralized configuration retrieval is disabled Default setting (none): cfg_url: top","title":"cfg_url"},{"location":"config.ini/#cfg_username","text":"If username/pasword credentials are used to retrieve the centralized config, this field specifies the usename to be used. (Note: using an access token is a MUCH better idea...see below) Default setting (none): cfg_username: top","title":"cfg_username"},{"location":"config.ini/#cfg_password","text":"If username/pasword credentials are used to retrieve the centralized config, this field specifies the password to be used. (Note: using an access token is a MUCH better idea...see below) Default setting (none): cfg_password: top","title":"cfg_password"},{"location":"config.ini/#cfg_token","text":"If a GitHub authentication token is used to retrieve the centralized config, this field specifies the token to be used. (Note: this is used instead of a username/pwd) Check out this page to find out more about creating access tokens: https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token Default setting (none): cfg_token: top","title":"cfg_token"},{"location":"config.ini/#cfg_refresh_interval","text":"This field specifies how often (in seconds) the centralized config file should be retrieved . Recommended value: 900 (i.e. 15 mins) Default setting (none): cfg_refresh_interval: top","title":"cfg_refresh_interval"},{"location":"config.ini/#unit_bouncer","text":"If you need to bounce (reboot) the unit for some reason on a regular basis, this field can be used to signal to the WLAN Pi each hour at which it must reboot. The field is a comma separated string that lists the hours at which the unit must reboot (in 24-hour format). The number-format and comma separation are important to get right! Note that the reboot is not exactly on the hour, but will occur at the end of the next test cycle that it is within the hour where a reboot is required. It will only happen once per hour. Example: the following config will reboot at midnight, 04:00, 08:00, 12:00, 16:00: unit_bouncer: 00, 06, 12, 18 This parameter is commented out by default as it is obviously not something you necessarilly want to switch on accidentally. Default setting: ; unit_bouncer: 00, 06, 12, 18 top","title":"unit_bouncer"},{"location":"config.ini/#network_test-section","text":"","title":"[Network_Test] Section"},{"location":"config.ini/#network_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for network tests in remote data repositories (e.g. Splunk, InfluxDB) Default setting: network_data_file: wiperf-network top","title":"network_data_file"},{"location":"config.ini/#speedtest-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[Speedtest] Section"},{"location":"config.ini/#enabled","text":"Options: yes or no. If set to no, entire section is ignored and no Speedtest is run. When enabled, a speedtest to the Ookla speedtest service is run each test cycle. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_id","text":"If you wish to specify a particular Ookla speedtest server that the test needs to be run against, you can enter its ID here. This must be the (numeric) server ID of a specific Ookla server taken from : https://c.speedtest.net/speedtest-servers-static.php Note this must be the number (NOT url!) taken from the field id=\"xxxxx\". If no value is specified, best server is used (default) Default setting: server_id: top","title":"server_id"},{"location":"config.ini/#http_proxy","text":"","title":"http_proxy"},{"location":"config.ini/#https_proxy","text":"","title":"https_proxy"},{"location":"config.ini/#no_proxy","text":"If proxy server access is required to run a speedtest, enter the proxy server details here for https & https e.g. https_proxy: http://10.1.1.1:8080 For sites that are not accessed via proxy, use no_proxy (make sure value enclosed in quotes & comma separated for mutiple values) e.g. no_proxy: \"mail.local, intranet.local\" Default settings: http_proxy: https_proxy: no_proxy: top","title":"no_proxy"},{"location":"config.ini/#speedtest_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for Speedtests in the reporting server (e.g. Splunk/InfluxDB) Default setting: speedtest_data_file: wiperf-speedtest top","title":"speedtest_data_file"},{"location":"config.ini/#ping_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[Ping_Test] Section"},{"location":"config.ini/#enabled_1","text":"Options: yes or no. If set to no, entire section is ignored and no ping tests are run. When enabled, up to 5 entries will be targetted with an ICMP ping and the RRT times recorded Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#ping_host1","text":"IP address or hostname of first ping target. No target details = no test run Default setting: ping_host1: google.com top","title":"ping_host1"},{"location":"config.ini/#ping_host2","text":"IP address or hostname of second ping target. No target details = no test run Default setting: ping_host2: cisco.com top","title":"ping_host2"},{"location":"config.ini/#ping_host3","text":"IP address or hostname of third ping target. No target details = no test run Default setting: ping_host3: top","title":"ping_host3"},{"location":"config.ini/#ping_host4","text":"IP address or hostname of fourth ping target. No target details = no test run Default setting: ping_host4: top","title":"ping_host4"},{"location":"config.ini/#ping_host5","text":"IP address or hostname of fifth ping target. No target details = no test run Default setting: ping_host2: top","title":"ping_host5"},{"location":"config.ini/#ping_count","text":"The number of pings to send for each ping target Default setting: ping_count: 10 top","title":"ping_count"},{"location":"config.ini/#ping_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for ping tests in the reporting server (e.g. Splunk.InfuxDB) Default setting: ping_data_file: wiperf-ping top","title":"ping_data_file"},{"location":"config.ini/#iperf3_tcp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[Iperf3_tcp_test] Section"},{"location":"config.ini/#enabled_2","text":"Options: yes or no. If set to no, entire section is ignored and no tcp iperf3 test is run. When enabled, a tcp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs) Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_hostname","text":"The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: top","title":"server_hostname"},{"location":"config.ini/#port","text":"The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top","title":"port"},{"location":"config.ini/#duration","text":"The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 10 top","title":"duration"},{"location":"config.ini/#iperf3_tcp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for tcp iperf3 tests in Splunk Default setting: iperf3_tcp_data_file: wiperf-iperf3-tcp top","title":"iperf3_tcp_data_file"},{"location":"config.ini/#iperf3_udp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[Iperf3_udp_test] Section"},{"location":"config.ini/#enabled_3","text":"Options: yes or no. If set to no, entire section is ignored and no udp iperf3 test is run. When enabled, a udp iperf3 test will be run to the iperf3 server defined in server_hostname to the port network port for the duration period (in secs), attempting to achieve a data transfer rate of bandwidth bps. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#server_hostname_1","text":"The IP address or (resolvable) name of the server running the iperf3 service. Default setting: server_hostname: top","title":"server_hostname"},{"location":"config.ini/#port_1","text":"The network port on the server running iperf3 where the iperf3 service is available (5201 by default). Default setting: port: 5201 top","title":"port"},{"location":"config.ini/#duration_1","text":"The duration (in seconds) that the iperf3 test will be run for Default setting: duration: 10 top","title":"duration"},{"location":"config.ini/#bandwidth","text":"The data rate that will be attempted for the UDP iperf3 test in bps Default setting: bandwidth: 2000000 top","title":"bandwidth"},{"location":"config.ini/#iperf3_udp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for udp iperf3 tests in Splunk Default setting: iperf3_udp_data_file: wiperf-iperf3-udp top","title":"iperf3_udp_data_file"},{"location":"config.ini/#dns_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[DNS_test] Section"},{"location":"config.ini/#enabled_4","text":"Options: yes or no. If set to no, entire section is ignored and no DNS tests are run. When enabled, DNS tests are run for each of the dns_target paramters defined in this section. Any targets that have no value entered will be ignored. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#dns_target1","text":"Hostname of first DNS target. No target details = no test run Default setting: dns_target1: google.com top","title":"dns_target1"},{"location":"config.ini/#dns_target2","text":"Hostname of second DNS target. No target details = no test run Default setting: dns_target2: cisco.com top","title":"dns_target2"},{"location":"config.ini/#dns_target3","text":"Hostname of third DNS target. No target details = no test run Default setting: dns_target3: top","title":"dns_target3"},{"location":"config.ini/#dns_target4","text":"Hostname of fourth DNS target. No target details = no test run Default setting: dns_target4: top","title":"dns_target4"},{"location":"config.ini/#dns_target5","text":"Hostname of fifth DNS target. No target details = no test run Default setting: dns_target5: top","title":"dns_target5"},{"location":"config.ini/#dns_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DNS tests in Splunk Default setting: dns_data_file: wiperf-dns top","title":"dns_data_file"},{"location":"config.ini/#http_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[HTTP_test] Section"},{"location":"config.ini/#enabled_5","text":"Options: yes or no. If set to no, entire section is ignored and no HTTP tests are run. When enabled, HTTP tests are run for each of the http_target paramters defined in this section. Any targets that have no value entered will be ignored. Targets must include the full url of each site to be queried (including http:// or https:// element). Valid site address examples: http://bbc.co.uk https://ebay.com A http get will be performed for each target and the result code returned. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#http_target1","text":"Hostname of first HTTP target. No target details = no test run Default setting: http_target1: https://google.com top","title":"http_target1"},{"location":"config.ini/#http_target2","text":"Hostname of second HTTP target. No target details = no test run Default setting: http_target2: https://cisco.com top","title":"http_target2"},{"location":"config.ini/#http_target3","text":"Hostname of third HTTP target. No target details = no test run Default setting: http_target3: top","title":"http_target3"},{"location":"config.ini/#http_target4","text":"Hostname of fourth HTTP target. No target details = no test run Default setting: http_target4: top","title":"http_target4"},{"location":"config.ini/#http_target5","text":"Hostname of fifth HTTP target. No target details = no test run Default setting: http_target5: top","title":"http_target5"},{"location":"config.ini/#http_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for HTTP tests in Splunk Default setting: http_data_file: wiperf-http top","title":"http_data_file"},{"location":"config.ini/#dhcp_test-section","text":"(Changes made in this section will be used in next test cycle and may be made on the fly while in wiperf mode on the WLAN Pi)","title":"[DHCP_test] Section"},{"location":"config.ini/#enabled_6","text":"Options: yes or no. If set to no, entire section is ignored and no DHCP test is run. Note that the DHCP test has 2 modes : passive: only a renewal request is sent (no release of IP) active: a release and renew request is performed. Note that the active setting has shown varying degrees of usefulness in esting. In some scenarios (e.g. when connected via ZeroTier), it has caused connectivity issues, hence the passive setting is a better choice. Obviously, the passive setting does not perform such a rigorous DHCP test and is completed much quicker than the active mode. However, it still provides a useful comparative measure of the reponsivemess of DHCP servers. Default setting: enabled: yes top","title":"enabled"},{"location":"config.ini/#mode-deprecated","text":"Note: This setting has been removed as it caused probe connectivity issues. The probe now only operates in the passive mode. These notes have bene left in for reference for those who used older versions of code or old configuration file. This setting is silently ignored if supplied. Available options: passive active The active settings performs a full release/request and may be disruptve to connectivity - use with caution. The passive setting is the recommended option for most situations. Default setting: mode: passive top","title":"mode (deprecated)"},{"location":"config.ini/#dhcp_data_file","text":"(Advanced setting, do not change) This the file name for modes where data files are dumped locally and also provides the data source for DHCP tests in Splunk Default setting: dhcp_data_file: wiperf-dhcp top","title":"dhcp_data_file"},{"location":"data_points/","text":"Data Points Reference Guide Background The wiperf probe collects a variety of data points about various aspects of network connectivity and performance. It then makes those data points available to a number of databases via their standard API (e.g. Splunk, InfluxDB etc.). The data collected in all instances is the same, but the format of the data presented to each type of database varies depending on the their API and formatting rules and syntax. This document details the data points collected by the probe. These field names and the data values should be the same no matter which database is used. The probe may collect data for the following network tests, depending upon its configuration: Wireless network connectivity details Speedtest testing results data ICMP ping tests to various destinations DNS lookup tests to various destinations ] HTTP (web) tests to various destinations iperf3 TCP test to a nominated iperf3 server iperf3 UDP test to a nominated iperf3 server DHCP renewal test to test DHCP performance on network to which the WLAN Pi is connected The tests are run each time the wiperf process is triggered (usually every 5 minutes from a local cron job). The tests that are run, together with test configuration parameters are configured in the config.ini file. Here are the data points that may be collected, displayed by test type: Data Points Details Wireless Network Connectivity (Data source: wiperf-network) time : Unix timestamp of time test was performed ssid : The network name of the wireless network to which the wiperf probe is currently connected bssid : The basic service set identifier (i.e. MAC address) of the radio to which the wiperf probe is currently connected freq_ghz : The centre frequency of the channel on which the probe is operating (note this may be different to the primary channel centre freq if a bonded channel is in use) center_freq_ghz : The centre frequency of the primary channel on which the probe is operating channel : The channel number on which the probe is operating channel_width: The channel width (e.g. 20MHz, 40MHz, 80MHz) of the channel on which the probe is operating tx_rate_mbps : The PHY rate at which data is being sent from the probe to the AP (note this is not a throughput rate, just a physical connection rate) rx_rate_mbps : The PHY rate at which data is being sent from the AP to the probe (note this is not a throughput rate, just a physical connection rate) tx_mcs : For HT & VHT connections, this is the the MCS value used by the probe to the AP rx_mcs : For HT & VHT connections, this is the the MCS value used by the AP to the probe signal_level_dbm : The power level of the AP radio signal as observed by the probe (in dBm) tx_retries : The number of transmitted frames that have had to be sent gain (retried) ip_address : The IP address assigned to the probe WLAN NIC Speedtest Results (Data source: wiperf-speedtest) time : Unix timestamp of time test was performed ping_time : The RTT of a ping test to the speedtest server download_rate_mbps : The throughput rate achieved when receiving data from the speedtest server in megabits per second upload_rate_mbps : The throughput rate achieved when sending data to the speedtest server in megabits per second server_name : The name of the speedtest server used for this test Ping Results (Data source: wiperf-ping) time : Unix timestamp of time test was performed ping_index : wiperf runs up to 5 instances of ping test via its configuration file. This index uniquely identifies each instance. ping_host : The IP address or hostname of the target host/site being pinged pkts_tx : The number of ping request packets sent during the ping test pkts_rx : The number of ping response packets received back during the ping test percent_loss : The percentage (%) of packets lost during the test (i.e. how many responses were received compared to requests sent) test_time_ms : How long the ping test took in total rtt_min_ms : The minimum round trip time of all ping tests to this test instance in milliseconds rtt_avg_ms : The average round trip time of all ping tests to this test instance in milliseconds rtt_max_ms : The maximum round trip time of all ping tests to this test instance in milliseconds rtt_mdev_ms : Standard deviation of all ping tests to this test instance (...no, I don't know either...but you'll look cool at dinner parties if you mention it.) DNS Results (Data source: wiperf-ping) time : Unix timestamp of time test was performed dns_index : wiperf runs up to 5 instances of DNS test via its configuration file. This index uniquely identifies each instance. dns_target : The domain name of the target host/site which is the subject of the DNS lookup test lookup_time_ms : The time taken to perform the DNS lookup in milliseconds HTTP Results (Data source: wiperf-http) time : Unix timestamp of time test was performed http_index : wiperf runs up to 5 instances of HTTP test via its configuration file. This index uniquely identifies each instance. http_target : The domain name (or IP address) of the target site which is the subject of the HTTP test http_get_time_ms : The time taken (in mS) to retrieve the html page from the target site in milliseconds http_server_reponse_time_ms: The time taken (in mS) to receive the response headers from the target site. This is a more useful figure in many instances, as it does not include the page load time to is more indicative of the web server RTT. http_status_code : The HTTP status code returned from the target site in this test instance (200 is good, other values have varying meanings: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes ) iperf3 TCP Results (Data source: wiperf-iperf3-tcp) time : Unix timestamp of time test was performed sent_mbps : The transmit throughput achieved (in megabits per seconds) during the TCP iperf test received_mbps : The receive throughput achieved (in megabits per seconds) during the TCP iperf test sent_bytes : The number of bytes (i.e. data volume) sent from the probe to the iperf server during the test received_bytes : The number of bytes (i.e. data volume) received by the probe from the iperf server during the test retransmits : The number of times frames had to be re=transmitted during the test iperf3 UDP Results (Data source: wiperf-iperf3-udp) time : Unix timestamp of time test was performed bytes : The number of bytes transferred from the probe to the iperf server during the test mbps : The throughput achieved (in megabits per second) during the iperf test when sending data to the iperf server jitter_ms : The level of jitter measured (in milliseconds) during the test packets : The number of packets sent from the probe to the iperf server during the test lost_packets : The number of transmitted packets lost during the test lost_percent : The percentage of transmitted packets lost during the test DHCP Test Results (Data source: wiperf-dhcp) time : Unix timestamp of time test was performed renewal_time_ms : The time taken for the probe to renew it's IP address in milliseconds","title":"Data Points Reference Guide"},{"location":"data_points/#data-points-reference-guide","text":"","title":"Data Points Reference Guide"},{"location":"data_points/#background","text":"The wiperf probe collects a variety of data points about various aspects of network connectivity and performance. It then makes those data points available to a number of databases via their standard API (e.g. Splunk, InfluxDB etc.). The data collected in all instances is the same, but the format of the data presented to each type of database varies depending on the their API and formatting rules and syntax. This document details the data points collected by the probe. These field names and the data values should be the same no matter which database is used. The probe may collect data for the following network tests, depending upon its configuration: Wireless network connectivity details Speedtest testing results data ICMP ping tests to various destinations DNS lookup tests to various destinations ] HTTP (web) tests to various destinations iperf3 TCP test to a nominated iperf3 server iperf3 UDP test to a nominated iperf3 server DHCP renewal test to test DHCP performance on network to which the WLAN Pi is connected The tests are run each time the wiperf process is triggered (usually every 5 minutes from a local cron job). The tests that are run, together with test configuration parameters are configured in the config.ini file. Here are the data points that may be collected, displayed by test type:","title":"Background"},{"location":"data_points/#data-points-details","text":"","title":"Data Points Details"},{"location":"data_points/#wireless-network-connectivity","text":"(Data source: wiperf-network) time : Unix timestamp of time test was performed ssid : The network name of the wireless network to which the wiperf probe is currently connected bssid : The basic service set identifier (i.e. MAC address) of the radio to which the wiperf probe is currently connected freq_ghz : The centre frequency of the channel on which the probe is operating (note this may be different to the primary channel centre freq if a bonded channel is in use) center_freq_ghz : The centre frequency of the primary channel on which the probe is operating channel : The channel number on which the probe is operating channel_width: The channel width (e.g. 20MHz, 40MHz, 80MHz) of the channel on which the probe is operating tx_rate_mbps : The PHY rate at which data is being sent from the probe to the AP (note this is not a throughput rate, just a physical connection rate) rx_rate_mbps : The PHY rate at which data is being sent from the AP to the probe (note this is not a throughput rate, just a physical connection rate) tx_mcs : For HT & VHT connections, this is the the MCS value used by the probe to the AP rx_mcs : For HT & VHT connections, this is the the MCS value used by the AP to the probe signal_level_dbm : The power level of the AP radio signal as observed by the probe (in dBm) tx_retries : The number of transmitted frames that have had to be sent gain (retried) ip_address : The IP address assigned to the probe WLAN NIC","title":"Wireless Network Connectivity"},{"location":"data_points/#speedtest-results","text":"(Data source: wiperf-speedtest) time : Unix timestamp of time test was performed ping_time : The RTT of a ping test to the speedtest server download_rate_mbps : The throughput rate achieved when receiving data from the speedtest server in megabits per second upload_rate_mbps : The throughput rate achieved when sending data to the speedtest server in megabits per second server_name : The name of the speedtest server used for this test","title":"Speedtest Results"},{"location":"data_points/#ping-results","text":"(Data source: wiperf-ping) time : Unix timestamp of time test was performed ping_index : wiperf runs up to 5 instances of ping test via its configuration file. This index uniquely identifies each instance. ping_host : The IP address or hostname of the target host/site being pinged pkts_tx : The number of ping request packets sent during the ping test pkts_rx : The number of ping response packets received back during the ping test percent_loss : The percentage (%) of packets lost during the test (i.e. how many responses were received compared to requests sent) test_time_ms : How long the ping test took in total rtt_min_ms : The minimum round trip time of all ping tests to this test instance in milliseconds rtt_avg_ms : The average round trip time of all ping tests to this test instance in milliseconds rtt_max_ms : The maximum round trip time of all ping tests to this test instance in milliseconds rtt_mdev_ms : Standard deviation of all ping tests to this test instance (...no, I don't know either...but you'll look cool at dinner parties if you mention it.)","title":"Ping Results"},{"location":"data_points/#dns-results","text":"(Data source: wiperf-ping) time : Unix timestamp of time test was performed dns_index : wiperf runs up to 5 instances of DNS test via its configuration file. This index uniquely identifies each instance. dns_target : The domain name of the target host/site which is the subject of the DNS lookup test lookup_time_ms : The time taken to perform the DNS lookup in milliseconds","title":"DNS Results"},{"location":"data_points/#http-results","text":"(Data source: wiperf-http) time : Unix timestamp of time test was performed http_index : wiperf runs up to 5 instances of HTTP test via its configuration file. This index uniquely identifies each instance. http_target : The domain name (or IP address) of the target site which is the subject of the HTTP test http_get_time_ms : The time taken (in mS) to retrieve the html page from the target site in milliseconds http_server_reponse_time_ms: The time taken (in mS) to receive the response headers from the target site. This is a more useful figure in many instances, as it does not include the page load time to is more indicative of the web server RTT. http_status_code : The HTTP status code returned from the target site in this test instance (200 is good, other values have varying meanings: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes )","title":"HTTP Results"},{"location":"data_points/#iperf3-tcp-results","text":"(Data source: wiperf-iperf3-tcp) time : Unix timestamp of time test was performed sent_mbps : The transmit throughput achieved (in megabits per seconds) during the TCP iperf test received_mbps : The receive throughput achieved (in megabits per seconds) during the TCP iperf test sent_bytes : The number of bytes (i.e. data volume) sent from the probe to the iperf server during the test received_bytes : The number of bytes (i.e. data volume) received by the probe from the iperf server during the test retransmits : The number of times frames had to be re=transmitted during the test","title":"iperf3 TCP Results"},{"location":"data_points/#iperf3-udp-results","text":"(Data source: wiperf-iperf3-udp) time : Unix timestamp of time test was performed bytes : The number of bytes transferred from the probe to the iperf server during the test mbps : The throughput achieved (in megabits per second) during the iperf test when sending data to the iperf server jitter_ms : The level of jitter measured (in milliseconds) during the test packets : The number of packets sent from the probe to the iperf server during the test lost_packets : The number of transmitted packets lost during the test lost_percent : The percentage of transmitted packets lost during the test","title":"iperf3 UDP Results"},{"location":"data_points/#dhcp-test-results","text":"(Data source: wiperf-dhcp) time : Unix timestamp of time test was performed renewal_time_ms : The time taken for the probe to renew it's IP address in milliseconds","title":"DHCP Test Results"},{"location":"db_http/","text":"Dashboard - HTTP Grafana Help page - TBA Splunk Help page - TBA","title":"HTTP"},{"location":"db_http/#dashboard-http","text":"","title":"Dashboard - HTTP"},{"location":"db_http/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_http/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"db_probe_summary/","text":"Dashboard - Probe Summary Grafana Help page - TBA Splunk Help page - TBA","title":"Probe Summary"},{"location":"db_probe_summary/#dashboard-probe-summary","text":"","title":"Dashboard - Probe Summary"},{"location":"db_probe_summary/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_probe_summary/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"db_speedtest/","text":"Dashboard - Speedtest Grafana Help page - TBA Splunk Help page - TBA","title":"Speedtest"},{"location":"db_speedtest/#dashboard-speedtest","text":"","title":"Dashboard - Speedtest"},{"location":"db_speedtest/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_speedtest/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"db_tcp_iperf3/","text":"Dashboard - TCP iperf3 Grafana Help page - TBA Splunk Help page - TBA","title":"TCP iperf3"},{"location":"db_tcp_iperf3/#dashboard-tcp-iperf3","text":"","title":"Dashboard - TCP iperf3"},{"location":"db_tcp_iperf3/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_tcp_iperf3/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"db_udp_iperf3/","text":"Dashboard - UDP iperf3 Grafana Help page - TBA Splunk Help page - TBA","title":"UDP iperf3"},{"location":"db_udp_iperf3/#dashboard-udp-iperf3","text":"","title":"Dashboard - UDP iperf3"},{"location":"db_udp_iperf3/#grafana","text":"Help page - TBA","title":"Grafana"},{"location":"db_udp_iperf3/#splunk","text":"Help page - TBA","title":"Splunk"},{"location":"faq/","text":"FAQ How do I use wiperf with a proxy in my network? Please see this advanced configuration note: link My probe only needs to hit internal network targets. How do I stop the DNS check to google.com? Before commencing tests, wiperf will perform a test DNS lookup to ensure that DNS is working OK. By default, the DNS target in /etc/wiperf/config.ini is set to 'google.com'. If your DNS is internal to your network and does not resolve public Internet targets, yo can change the section below to point at an internal target. ; connectivity DNS lookup - site used for initial DNS lookup when assessing if DNS working OK connectivity_lookup: google.com How do I change the hostname of my probe? Please see the details in this help page: link Why are MCS & Rx Phy rates missing from my reports? In several dashboard reports, the reported MCS values & Rx Phy rate may be blank. This is because these values simply are not reported by many NICs. Sorry, there's not much I can do about this as I don't write the wireless NIC drivers. My probe seems to reboot itself intermittently. Why? Wiperf has a watchdog feature that it uses to try to reset things when it is having connectivity related difficulties. There may be instances when tests are continually failing or wireless connectivity is intermittent due to perhaps being stuck on a remote AP that is sub-optimal from a connectivity perspective. If persistent issues are detected, then wiperf will reboot the probe to try to remediate the issue. This will provide the opportunity to the reset all network connectivity and internal processes. Note that this is a last ditch mechanism. Wiperf will try bouncing network interfaces to remediate any short-term connectivity issues, which will likely fix many issues without the need for a full reboot. If you observe your probe rebooting on a regular basis (e.g. a couple of times a hour), then check its logs as it is very unhappy about something. How Can I Harden the Probe Security? Please see this note for some suggestions for hardening the probe: link Where do I get the dashboard reports for Splunk and Grafana? Use SFTP/SCP and pull the xml files in /usr/share/wiperf/dashboards from your probe. Can I make a feature suggestion? Yes, get along to the GitHub site and post your suggestion in the 'Issues' section: https://github.com/wifinigel/wiperf/issues. It will be added to my \"todo\" list. Can I run tests over the Ethernet interface of the WLAN Pi? Yes, from wiperf v2.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-do-i-use-wiperf-with-a-proxy-in-my-network","text":"Please see this advanced configuration note: link","title":"How do I use wiperf with a proxy in my network?"},{"location":"faq/#my-probe-only-needs-to-hit-internal-network-targets-how-do-i-stop-the-dns-check-to-googlecom","text":"Before commencing tests, wiperf will perform a test DNS lookup to ensure that DNS is working OK. By default, the DNS target in /etc/wiperf/config.ini is set to 'google.com'. If your DNS is internal to your network and does not resolve public Internet targets, yo can change the section below to point at an internal target. ; connectivity DNS lookup - site used for initial DNS lookup when assessing if DNS working OK connectivity_lookup: google.com","title":"My probe only needs to hit internal network targets. How do I stop the DNS check to google.com?"},{"location":"faq/#how-do-i-change-the-hostname-of-my-probe","text":"Please see the details in this help page: link","title":"How do I change the hostname of my probe?"},{"location":"faq/#why-are-mcs-rx-phy-rates-missing-from-my-reports","text":"In several dashboard reports, the reported MCS values & Rx Phy rate may be blank. This is because these values simply are not reported by many NICs. Sorry, there's not much I can do about this as I don't write the wireless NIC drivers.","title":"Why are MCS &amp; Rx Phy rates missing from my reports?"},{"location":"faq/#my-probe-seems-to-reboot-itself-intermittently-why","text":"Wiperf has a watchdog feature that it uses to try to reset things when it is having connectivity related difficulties. There may be instances when tests are continually failing or wireless connectivity is intermittent due to perhaps being stuck on a remote AP that is sub-optimal from a connectivity perspective. If persistent issues are detected, then wiperf will reboot the probe to try to remediate the issue. This will provide the opportunity to the reset all network connectivity and internal processes. Note that this is a last ditch mechanism. Wiperf will try bouncing network interfaces to remediate any short-term connectivity issues, which will likely fix many issues without the need for a full reboot. If you observe your probe rebooting on a regular basis (e.g. a couple of times a hour), then check its logs as it is very unhappy about something.","title":"My probe seems to reboot itself intermittently. Why?"},{"location":"faq/#how-can-i-harden-the-probe-security","text":"Please see this note for some suggestions for hardening the probe: link","title":"How Can I Harden the Probe Security?"},{"location":"faq/#where-do-i-get-the-dashboard-reports-for-splunk-and-grafana","text":"Use SFTP/SCP and pull the xml files in /usr/share/wiperf/dashboards from your probe.","title":"Where do I get the dashboard reports for Splunk and Grafana?"},{"location":"faq/#can-i-make-a-feature-suggestion","text":"Yes, get along to the GitHub site and post your suggestion in the 'Issues' section: https://github.com/wifinigel/wiperf/issues. It will be added to my \"todo\" list.","title":"Can I make a feature suggestion?"},{"location":"faq/#can-i-run-tests-over-the-ethernet-interface-of-the-wlan-pi","text":"Yes, from wiperf v2.","title":"Can I run tests over the Ethernet interface of the WLAN Pi?"},{"location":"grafana_configure/","text":"Grafana Configuration Once the Granafa installation is complete, there are two main tasks remaining: Integration of Grafana with InfluxDB Addition of the wiperf dashboards Integration of Grafana with InfluxDB Integration with InfluxDB is required to allow Grafana to send data queries to the InfluxDB database and turn the data in to graphical reports. To do this, Grafana needs to know: the data source is an InfluxDB database where it is (IP address & port) the name of the database within Influx DB (as we previously configured) the access credentials to be used to pull the data out of InfluxDB The screen-shots below show the required steps: Configuration > Data Sources > Add Data Source : Select InfluxDB : Enter the name to be referenced for the connection, the URL, database name, username & password (all highlighted below) - note the InfluxDB values use those configure previously when we set up InfluxDB (the datbase name, username & password settings must match those used in the InfluxDB setup): Once completed, if you hit 'Save and Test' , the database connection test should indicate success if all information has been correctly entered. Adding Wiperf Dashboards Dashboard xml files can be obtained from the /usr/share/wiperf/dashboards folder of the probe. These may be downloaded to your local laptop using a utlity such as SCP or SFTP. Alternatively, the dashboard files may be accessed in the main wiperf repo at : folder Once the dashboard files have been downloaded, they may be imported to Grafana using the following steps: Use the menu option Dashboards > Manage > Import : Hit the 'Upload Json' button and select the dashboard file on your local laptop that you'd like to upload The name of the imported report will be shown as indicated. If you'd like to create or select a folder, this can be done in the 'Folder' field. The database connection previously created as the data source must also be selected to ensure the InfluxDB database can be queried: Once 'Import' is hit, the dashboard will be available in the main Grafana GUI. To find out more about usng Grafana, visit the Grafana getting started pages.","title":"Grafana Configuration"},{"location":"grafana_configure/#grafana-configuration","text":"Once the Granafa installation is complete, there are two main tasks remaining: Integration of Grafana with InfluxDB Addition of the wiperf dashboards","title":"Grafana Configuration"},{"location":"grafana_configure/#integration-of-grafana-with-influxdb","text":"Integration with InfluxDB is required to allow Grafana to send data queries to the InfluxDB database and turn the data in to graphical reports. To do this, Grafana needs to know: the data source is an InfluxDB database where it is (IP address & port) the name of the database within Influx DB (as we previously configured) the access credentials to be used to pull the data out of InfluxDB The screen-shots below show the required steps: Configuration > Data Sources > Add Data Source : Select InfluxDB : Enter the name to be referenced for the connection, the URL, database name, username & password (all highlighted below) - note the InfluxDB values use those configure previously when we set up InfluxDB (the datbase name, username & password settings must match those used in the InfluxDB setup): Once completed, if you hit 'Save and Test' , the database connection test should indicate success if all information has been correctly entered.","title":"Integration of Grafana with InfluxDB"},{"location":"grafana_configure/#adding-wiperf-dashboards","text":"Dashboard xml files can be obtained from the /usr/share/wiperf/dashboards folder of the probe. These may be downloaded to your local laptop using a utlity such as SCP or SFTP. Alternatively, the dashboard files may be accessed in the main wiperf repo at : folder Once the dashboard files have been downloaded, they may be imported to Grafana using the following steps: Use the menu option Dashboards > Manage > Import : Hit the 'Upload Json' button and select the dashboard file on your local laptop that you'd like to upload The name of the imported report will be shown as indicated. If you'd like to create or select a folder, this can be done in the 'Folder' field. The database connection previously created as the data source must also be selected to ensure the InfluxDB database can be queried: Once 'Import' is hit, the dashboard will be available in the main Grafana GUI. To find out more about usng Grafana, visit the Grafana getting started pages.","title":"Adding Wiperf Dashboards"},{"location":"grafana_install/","text":"Grafana Installation Obtaining and installing the Grafana software is straightforward. The following notes provide a high level overview of the steps required. Note that these instructions are for Grafana version 6.7 (other versions may work, but have not been tested): Visit the Grafana 6.7 installation guide at https://grafana.com/docs/grafana/v6.7/ . This provides acces to a wide variety of information about Grafanam including supported OS'es and platform concepts To download the code and to see the commands required for installation on the server CLI, visit the following download page: https://grafana.com/grafana/download Select the required version, Open Source edition and your OS (most likely a Linux variant) Make a copy of the CLI commands provided to download and install the software for your OS SSH to the server that will be used to host Grafana Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start grafana Ensure that the service will be started in the server is rebooted with: sudo systemctl enable grafana Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status grafana (ensure the process is \"active (running)\" ) As a final check, ensure that the Grafana web GUI is available using the URL: http://<server_IP>:3000/","title":"Grafana Installation"},{"location":"grafana_install/#grafana-installation","text":"Obtaining and installing the Grafana software is straightforward. The following notes provide a high level overview of the steps required. Note that these instructions are for Grafana version 6.7 (other versions may work, but have not been tested): Visit the Grafana 6.7 installation guide at https://grafana.com/docs/grafana/v6.7/ . This provides acces to a wide variety of information about Grafanam including supported OS'es and platform concepts To download the code and to see the commands required for installation on the server CLI, visit the following download page: https://grafana.com/grafana/download Select the required version, Open Source edition and your OS (most likely a Linux variant) Make a copy of the CLI commands provided to download and install the software for your OS SSH to the server that will be used to host Grafana Make sure your server has Internet connectivity (as it will need to pull down the required software) On the CLI of your server, paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start grafana Ensure that the service will be started in the server is rebooted with: sudo systemctl enable grafana Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status grafana (ensure the process is \"active (running)\" ) As a final check, ensure that the Grafana web GUI is available using the URL: http://<server_IP>:3000/","title":"Grafana Installation"},{"location":"grafana_platform/","text":"Grafana Platform Grafana is an open-source visualization tool that allows us to create reports around the data sent from wiperf probes to InfluxDB. It can integrates with a variety of data sources to query raw data and provides a wide variety of graphical report options - in our case, Grafana integrates with InfluxDB This guide does not cover all installation details of the software package, as these may be obtained from the official Grafana web site: https://grafana.com/docs/grafana/latest/ . Installation instructions are available for all major operating systems. Note that although Windows is supported, if you intend to install Grafana on the same platform as InfuxDB, Windows is not an option as InfluxDB v1.8 does not support Windows. To install Grafana and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"Grafana Platform"},{"location":"grafana_platform/#grafana-platform","text":"Grafana is an open-source visualization tool that allows us to create reports around the data sent from wiperf probes to InfluxDB. It can integrates with a variety of data sources to query raw data and provides a wide variety of graphical report options - in our case, Grafana integrates with InfluxDB This guide does not cover all installation details of the software package, as these may be obtained from the official Grafana web site: https://grafana.com/docs/grafana/latest/ . Installation instructions are available for all major operating systems. Note that although Windows is supported, if you intend to install Grafana on the same platform as InfuxDB, Windows is not an option as InfluxDB v1.8 does not support Windows. To install Grafana and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"Grafana Platform"},{"location":"influx_configure/","text":"Influx Configuration Now that we have the InfluxDB software installed, the next step is to create a database in which data from our wiperf probes will be stored. To create the database, we need to execute a series of commands on the CLI of the Influx DB server. Follow the following steps to create the required database: Check the InfluxDB service is running before starting sudo systemctl status influxdb (ensure the process is \"active (running)\" ) Enter the InfluxDB shell using the following command: sudo influx (shell is indicated by the new \">\" prompt) Create an admin user to administer InfluxDB: CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES Exit the InfluxDB shell with the command exit to return to the standard Linux CLI Edit the InfluxDB configuration file (/etc/influxdb/influxdb.conf): sudo nano /etc/influxdb/influxdb.conf uncomment the line # auth-enabled = false in the [http] section and change to auth-enabled = true to enable authentication of access to the database restart the InfluxDB process for the change to take effect: sudo systemctl restart influxdb Enter the InfluxDB shell again using the following command: sudo influx -username admin -password letmein (now using authentication) Create a new database with the following commands: CREATE DATABASE wiperf Check the new database exists using: SHOW DATABASES (the database \"wiperf\" should be shown in the list) Create and assign a user who can write to the wiperf database (i.e. a probe) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" Create and assign a user who can read from the wiperf database (i.e. the Grafana program) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' GRANT read ON \"wiperf\" TO \"grafana\" Exit the InfluxDB shell with the command exit t return to the Linux CLI At this point, the InfluxDB service is ready to receive data from a probe. If you have any probes ready to go, make sure they use the \"wiperf_probe\" user credentials in their configuration file so that they can add their data to the database. If you believe you have a probe that has successfully sent data, you can check the database contents using the following commands in the InfluxDB shell: USE wiperf SHOW SERIES SELECT * FROM \"wiperf-speedtest\" SHOW FIELD KEYS ON \"wiperf\" FROM \"wiperf-speedtest\" To find out more details, please checkout the official getting started guide: https://docs.influxdata.com/influxdb/v1.8/introduction/get-started/ For more information about adding users, check out: https://docs.influxdata.com/influxdb/v1.8/administration/authentication_and_authorization/ Note: You are advised to use your own passwords for the password fields shown in this document to ensure they are secured.","title":"Influx Configuration"},{"location":"influx_configure/#influx-configuration","text":"Now that we have the InfluxDB software installed, the next step is to create a database in which data from our wiperf probes will be stored. To create the database, we need to execute a series of commands on the CLI of the Influx DB server. Follow the following steps to create the required database: Check the InfluxDB service is running before starting sudo systemctl status influxdb (ensure the process is \"active (running)\" ) Enter the InfluxDB shell using the following command: sudo influx (shell is indicated by the new \">\" prompt) Create an admin user to administer InfluxDB: CREATE USER admin WITH PASSWORD 'letmein' WITH ALL PRIVILEGES Exit the InfluxDB shell with the command exit to return to the standard Linux CLI Edit the InfluxDB configuration file (/etc/influxdb/influxdb.conf): sudo nano /etc/influxdb/influxdb.conf uncomment the line # auth-enabled = false in the [http] section and change to auth-enabled = true to enable authentication of access to the database restart the InfluxDB process for the change to take effect: sudo systemctl restart influxdb Enter the InfluxDB shell again using the following command: sudo influx -username admin -password letmein (now using authentication) Create a new database with the following commands: CREATE DATABASE wiperf Check the new database exists using: SHOW DATABASES (the database \"wiperf\" should be shown in the list) Create and assign a user who can write to the wiperf database (i.e. a probe) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"wiperf_probe\" WITH PASSWORD 's3cr3tpwd99' GRANT WRITE ON \"wiperf\" TO \"wiperf_probe\" Create and assign a user who can read from the wiperf database (i.e. the Grafana program) using the following CLI commands (note the single and double quotes are signifcant for the user & pwd fields): CREATE USER \"grafana\" WITH PASSWORD 'R34dth3DB' GRANT read ON \"wiperf\" TO \"grafana\" Exit the InfluxDB shell with the command exit t return to the Linux CLI At this point, the InfluxDB service is ready to receive data from a probe. If you have any probes ready to go, make sure they use the \"wiperf_probe\" user credentials in their configuration file so that they can add their data to the database. If you believe you have a probe that has successfully sent data, you can check the database contents using the following commands in the InfluxDB shell: USE wiperf SHOW SERIES SELECT * FROM \"wiperf-speedtest\" SHOW FIELD KEYS ON \"wiperf\" FROM \"wiperf-speedtest\" To find out more details, please checkout the official getting started guide: https://docs.influxdata.com/influxdb/v1.8/introduction/get-started/ For more information about adding users, check out: https://docs.influxdata.com/influxdb/v1.8/administration/authentication_and_authorization/ Note: You are advised to use your own passwords for the password fields shown in this document to ensure they are secured.","title":"Influx Configuration"},{"location":"influx_install/","text":"Influx Installation Obtaining and installing the InfluxDB software is very straightforward. here is a high-level overview of the steps required: On your laptop, open a browser and obtain the required commands to download & install the software by visiting the following web page and selecting the v1.8 download option: https://portal.influxdata.com/downloads/ Copy the install commands provided for your OS Make sure your InfluxDB server has Internet connectivity (as it will need to pull down the required software) SSH to the server that will be used to host InfluxDB On the CLI of your server (your SSH session), paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start influxdb Make sure the InfluxDB service starts after a platform reboot: sudo systemctl enable influxdb Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status influxdb (ensure the process is \"active (running)\" ) The next step is to create a database to drop our incoming data (from wiperf probes) into.","title":"Influx Installation"},{"location":"influx_install/#influx-installation","text":"Obtaining and installing the InfluxDB software is very straightforward. here is a high-level overview of the steps required: On your laptop, open a browser and obtain the required commands to download & install the software by visiting the following web page and selecting the v1.8 download option: https://portal.influxdata.com/downloads/ Copy the install commands provided for your OS Make sure your InfluxDB server has Internet connectivity (as it will need to pull down the required software) SSH to the server that will be used to host InfluxDB On the CLI of your server (your SSH session), paste in the copied commands to kick-off the software download & install Once installation is complete, start the InfluxDB processes with the server CLI command: sudo systemctl start influxdb Make sure the InfluxDB service starts after a platform reboot: sudo systemctl enable influxdb Check the software is installed and running by executing the following command on the server CLI: sudo systemctl status influxdb (ensure the process is \"active (running)\" ) The next step is to create a database to drop our incoming data (from wiperf probes) into.","title":"Influx Installation"},{"location":"influx_platform/","text":"InfluxDB Platform InfluxDB is a time-series database that is used to store the network performance data that is collected by wiperf probes. It has many other uses and is used by many organizations as a back-end store for use-cases involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. InfluxDB does not report on network performance report data, but is used as a data repository source for Grafana in our use-case. Details about Grafana are provide later in this documentation. Note that for our use-case, we are using InfluxDB v1.8 (not v2.0). Influx can be installed on a wide variety of Linux-based platforms that can be viewed here . These include Ubuntu, Debian and macOS (no Windows) This guide does not cover all installation details of the software package, as these may be obtained when downloading and installing the software. To install InfluxDB and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high-end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software. Connectivity Planning One area to consider is connectivity between the wiperf probe and the InfluxDB instance. The wiperf probe needs to be able to access the InfluxDB server to send its results data. If the wiperf probe probe is being deployed on a network, how is the performance data generated by the probe going to get back to the InfluxDB server? If the probe is being deployed on a customer network to perform temporary monitoring, it will need to join the wireless network under test (or be plugged in to an ethernet switch port if testing a wired connection). But how is the wiperf probe going to send its data to the InfluxDB server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, potentially bridging wired and wireless networks. In some instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier.) Three topology deployment options are supported: Results data over wireless Results data over Ethernet Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the InfluxDB server. The 3 connectivity options are discussed below. Results Data Over Wireless In this topology the wiperf probe is configured to join an SSID that has the InfluxDB server accessible via its WLAN interface. Typically, the InfluxDB server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the InfluxDB server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of InfluxDB server> Results data over Ethernet If the InfluxDB server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of InfluxDB server> Results data over Zerotier/wireless A simple way of getting the wiperf probe talking with your Splunk server, if it has no direct access, is to use the Zerotier service to create a virtual overlay network via the Internet. In summary, both the InfluxDB server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same virtual network in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the InfluxDB server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees. It\u2019s very easy to use, plus your InfluxDB server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the InfluxDB server and wiperf probe when the client is installed. config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of InfluxDB server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"InfluxDB Platform"},{"location":"influx_platform/#influxdb-platform","text":"InfluxDB is a time-series database that is used to store the network performance data that is collected by wiperf probes. It has many other uses and is used by many organizations as a back-end store for use-cases involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. InfluxDB does not report on network performance report data, but is used as a data repository source for Grafana in our use-case. Details about Grafana are provide later in this documentation. Note that for our use-case, we are using InfluxDB v1.8 (not v2.0). Influx can be installed on a wide variety of Linux-based platforms that can be viewed here . These include Ubuntu, Debian and macOS (no Windows) This guide does not cover all installation details of the software package, as these may be obtained when downloading and installing the software. To install InfluxDB and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high-end server. If you'd like to look into server requirements further, then check out this page . Note that InfluxDB is an open-source product. There is no cost for downloading and installing your own instance of the software.","title":"InfluxDB Platform"},{"location":"influx_platform/#connectivity-planning","text":"One area to consider is connectivity between the wiperf probe and the InfluxDB instance. The wiperf probe needs to be able to access the InfluxDB server to send its results data. If the wiperf probe probe is being deployed on a network, how is the performance data generated by the probe going to get back to the InfluxDB server? If the probe is being deployed on a customer network to perform temporary monitoring, it will need to join the wireless network under test (or be plugged in to an ethernet switch port if testing a wired connection). But how is the wiperf probe going to send its data to the InfluxDB server ? Many environments may not be comfortable with hooking up the wiperf probe to their wired network, potentially bridging wired and wireless networks. In some instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier.) Three topology deployment options are supported: Results data over wireless Results data over Ethernet Results data over VPN/wireless The method used is configured on the wiperf probe probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the InfluxDB server. The 3 connectivity options are discussed below.","title":"Connectivity Planning"},{"location":"influx_platform/#results-data-over-wireless","text":"In this topology the wiperf probe is configured to join an SSID that has the InfluxDB server accessible via its WLAN interface. Typically, the InfluxDB server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the InfluxDB server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of InfluxDB server>","title":"Results Data Over Wireless"},{"location":"influx_platform/#results-data-over-ethernet","text":"If the InfluxDB server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of InfluxDB server>","title":"Results data over Ethernet"},{"location":"influx_platform/#results-data-over-zerotierwireless","text":"A simple way of getting the wiperf probe talking with your Splunk server, if it has no direct access, is to use the Zerotier service to create a virtual overlay network via the Internet. In summary, both the InfluxDB server and wiperf probe have the Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same virtual network in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the InfluxDB server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees. It\u2019s very easy to use, plus your InfluxDB server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the InfluxDB server and wiperf probe when the client is installed. config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of InfluxDB server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Results data over Zerotier/wireless"},{"location":"operation/","text":"Overview of Operation Wiperf is an open source utility that runs on a Raspberry Pi or a WLAN Pi hardware device. It provides network probe functionality to gather performance data to give an indication of how a network looks from an end user perspective. It runs a series of tests to gather metrics on network connectivity and performance through the execution of tests such ICMP ping, DNS lookups and iperf. These are fully configurable by editing a local configuration file on the probe device at the time of deployment. Configuration To configure the details of the tests to be run on a probe, a local configuration file on the probe needs to be updated. This will provide information to the probe about items such as the required network connectivity (e.g. wireless/ethernet), IP and credential information for the data server and test details. The configuration file can be updated by accessing the CLI of the probe (usually via SSH) and editing the file /etc/wiperf/config.ini . In addition, configuration is also required to connect the probed to the network under test, and run wiperf tests on a regular basis. More details about the probe configuration can be found on this page . Logging Following the completion of the configuration steps, wiperf will run every 5 minutes, performing the configured network tests and sending results data to the data server. A number of logs are generated to provide support information around the installation and operation of the wiperf process. For more information about the logs created, please visit our troubleshooting page . Reporting Gathering the data with a probe is only half of the story when using wiperf. The gathered data must be sent to a data collection server to allow it to be visualized to allow analysis of network performance. The data server must be an instance of either: Splunk, or InfluxDB with Grafana Splunk The graphic above outlines the collection of network performance data and how this is sent to Splunk by the wiperf probe. The Splunk instance may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.) In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Splunk server for storage and later analysis The data is analyzed by accessing the dashboard (a web GUI) of the Splunk server with a browser ( Note that the Splunk server acts as both the data repository and reporting platform for collected data ) Grafana/Influx The graphic above outlines the collection of network performance data and how this is sent to Influx & Grafana by the wiperf probe. Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data source from which to pull its network performance data. To meet this requirement, a InfluxDB database server is used. Like Grafana, InfluxDB is also an open-source package. ( Note that this setup contrasts with using Splunk, which allows us to use the same package to provide both the data storage and analysis/visualization functions ) For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of InfluxDB as its data source. Grafana & Influx may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.). Note that wiperf only needs to be able to send data to InfluxDB - it requires no communication with the Grafana instance. In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to an InfluxDB server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the InfluxDB server for storage Grafana is configured to use InfluxDB as its data source to allow visualization of performance data Data is analysed by accessing the dashboard (a web GUI) of the Grafana server, which pulls the required dashboard data from InfluxDB.","title":"Operation Overview"},{"location":"operation/#overview-of-operation","text":"Wiperf is an open source utility that runs on a Raspberry Pi or a WLAN Pi hardware device. It provides network probe functionality to gather performance data to give an indication of how a network looks from an end user perspective. It runs a series of tests to gather metrics on network connectivity and performance through the execution of tests such ICMP ping, DNS lookups and iperf. These are fully configurable by editing a local configuration file on the probe device at the time of deployment.","title":"Overview of Operation"},{"location":"operation/#configuration","text":"To configure the details of the tests to be run on a probe, a local configuration file on the probe needs to be updated. This will provide information to the probe about items such as the required network connectivity (e.g. wireless/ethernet), IP and credential information for the data server and test details. The configuration file can be updated by accessing the CLI of the probe (usually via SSH) and editing the file /etc/wiperf/config.ini . In addition, configuration is also required to connect the probed to the network under test, and run wiperf tests on a regular basis. More details about the probe configuration can be found on this page .","title":"Configuration"},{"location":"operation/#logging","text":"Following the completion of the configuration steps, wiperf will run every 5 minutes, performing the configured network tests and sending results data to the data server. A number of logs are generated to provide support information around the installation and operation of the wiperf process. For more information about the logs created, please visit our troubleshooting page .","title":"Logging"},{"location":"operation/#reporting","text":"Gathering the data with a probe is only half of the story when using wiperf. The gathered data must be sent to a data collection server to allow it to be visualized to allow analysis of network performance. The data server must be an instance of either: Splunk, or InfluxDB with Grafana","title":"Reporting"},{"location":"operation/#splunk","text":"The graphic above outlines the collection of network performance data and how this is sent to Splunk by the wiperf probe. The Splunk instance may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.) In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to a Splunk server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the Splunk server for storage and later analysis The data is analyzed by accessing the dashboard (a web GUI) of the Splunk server with a browser ( Note that the Splunk server acts as both the data repository and reporting platform for collected data )","title":"Splunk"},{"location":"operation/#grafanainflux","text":"The graphic above outlines the collection of network performance data and how this is sent to Influx & Grafana by the wiperf probe. Grafana is a popular open-source data visualization tool. It is used to graph the performance data collected by wiperf. However, Grafana needs a data source from which to pull its network performance data. To meet this requirement, a InfluxDB database server is used. Like Grafana, InfluxDB is also an open-source package. ( Note that this setup contrasts with using Splunk, which allows us to use the same package to provide both the data storage and analysis/visualization functions ) For small-scale instances, Grafana & Influx may be installed on the same server platform and Grafana configured to use the local instance of InfluxDB as its data source. Grafana & Influx may be provided anywhere that is convenient (e.g. on a server locally, via VPN, cloud etc.). Note that wiperf only needs to be able to send data to InfluxDB - it requires no communication with the Grafana instance. In summary, the steps for data collection are as follows: A wiperf probe (i.e. a WLAN Pi or Raspberry Pi) is configured to perform tests and send results to an InfluxDB server The probe performs the configured tests (e.g. speedtest, iperf, http etc.) The results of each test are sent over https to the InfluxDB server for storage Grafana is configured to use InfluxDB as its data source to allow visualization of performance data Data is analysed by accessing the dashboard (a web GUI) of the Grafana server, which pulls the required dashboard data from InfluxDB.","title":"Grafana/Influx"},{"location":"probe_configure/","text":"Probe Configuration The final step in getting our probe ready to deploy is to configure the wiperf software to perform the tests we'd like to perform, and to tell wiperf where it can find the data server that will provide reporting. The configuration tasks break down as follows: Edit the probe config.ini file to configure tests and data server details Add a cron job on the probe to run wiperf every 5 mins to perform its tests Configuration File Note: the details in this section apply to both the WLAN Pi and RPi probe The operation of wiperf is configured using the file /etc/wiperf/config.ini This needs to be edited prior to running the wiperf software to perform tests. (Tests are initiated on the WLAN Pi by switching on to wiperf mode. On the RPi, tests are started by configuring a cron job - more on this later in this document) Prior to the first use of wiperf, the config.ini file does not exist. However, a default template config file ( /etc/wiperf/config.default.ini ) is supplied that can be used as the template to create the config.ini file. Here is the suggested workflow to create the config.ini file: Connect to the CLI of the probe (e.g. via SSH), create a copy of the config template file and edit the newly created config: cd /etc/wiperf # take a copy of the default configuration file cp ./config.default.ini ./config.ini # edit the config file with the required probe settings (ctrl-x to exit the editor) sudo nano ./config.ini By default, the configuration file is set to run all tests (which may or may not suit your needs). However, there is a minimum configuration that must be applied to successfully run tests. The minimum configuration parameters you need to configure (just to get you going) are outlined in the subsections below. Once you've got your probe going, you're likely going to want to spend a little more time customising the file for your environment. In summary you need to: Configure the wiperf global mode of operation (wireless or Ethernet) and the interface parameters that determine how the probe is connected to its network Configure the management platform you'll be sending data to Configure the tests you'd like to run Mode/Interface Parameters The probe can be used to perform its tests over its wireless interface, or its ethernet interface. These are known as 'wireless' or 'ethernet' mode in the config.ini file. In addition, the probe needs to know which interface is used to send results data back to the data server. It is possible to perform tests and send results data over the same interface, or it may be preferable to have tests performed over the wireless interface and return results over the ethernet interface. The final choice is determined by the environment in to which the probe is deployed. (Note: if you choose to use Zerotier for management connectivity, the Zerotier interface is also an option available). The interfaces available in the probe for ethernet and wireless connectivity will generally be eth0 and wlan0 . However, these may vary in some platforms, there the option to change the actual names of the interfaces of the probe is available if required. The relevant section of the config.ini file is shown below for reference (note that lines that start with a semi-colon (;) are comments and are ignored. Blank lines are also ignored.): [General] ; global test mode: 'wireless' or 'ethernet' ; ; wireless mode: ; - test traffic runs over wireless interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; ethernet mode: ; - test traffic runs over ethernet interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; probe_mode: wireless ; ------------- ethernet mode parameters ------------ ; eth interface name - set this as per the output of an ifconfig command (usually eth0) eth_if: eth0 ; --------------------------------------------------- ; ------------- wireless mode parameters ------------ ; wlan interface name - set this as per the output of an iwconfig command (usually wlan0) wlan_if: wlan0 ; --------------------------------------------------- ; -------------mgt interface parameters ------------ ; interface name over which mgt traffic is sent (i.e. how we get to our management ; server) - options: wlan0, eth0, ztxxxxxx (ZeroTier), lo (local instance of Influx) mgt_if: wlan0 ; --------------------------------------------------- Data Server Parameters Wiperf can send results data to Splunk and InfluxDB (v1.x) data collectors through an exporter module for each collector type. The relevant authentication parameters need to be set for the collector in-use in the following sections (note these also need to be configured on the data collector platform also before sending results data - see here for more info: Splunk / InfluxDB ) In summary, the workflow to configure the data server parameters in the probe configuration file is to: Set the exporter type (splunk/influxdb) configure the server address of the target data server configure data server port details (if defaults changed) configure data server credential and database information The relevant section of the config.ini file is shown below: ; --------- Common Mgt Platform Params ------- ; set the data exporter type - current options: splunk, influxdb, influxdb2 exporter_type: splunk ; -------------------------------------------- ; -------------- Splunk Config --------------- ; IP address or hostname of Splunk host splunk_host: ; Splunk collector port (8088 by default) splunk_port: 8088 ; Splunk token to access Splunk server created by Splunk (example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d) splunk_token: ;--------------------------------------------- ; -------------- InFlux1 Config --------------- ; IP address or hostname of InfluxDB host influx_host: ; InfluxDb collector port (8086 by default) influx_port: 8086 influx_username: influx_password: influx_database: ;--------------------------------------------- Network Tests Note that all network tests are enabled by default. If there are some tests you'd like to disable (e.g. if you don't have an iperf3 server set up), then you'll need to open up the config.ini file and look through each section for the \"enabled\" parameter for that test and set it to \"no\". For example, to disable the iperf tcp test: [Iperf3_tcp_test] ; yes = enabled, no = disabled enabled: no For a full description of the configuration file parameters, please review the following page: config.ini reference guide . Running Regular Tests Once the wiperf software has been configured, the final job is to configure a 'cron job' on the probe to run the software every 5 minutes. Cron is a scheduler utility within Linux that will run a software task at configured intervals. ( Note: This step is not required on the WLAN Pi, as the cron job is added automatically when the WLAN Pi is switched in to wiperf mode ) To configure cron, on the CLI of the probe, open the cron editor: sudo crontab -e Next, with the editor open, add following line to the open file: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 This command will run the main wiperf script to run the tests configured within config.ini at an interval of 5 minutes. It will also dump all script output to the file /var/log/wiperf_cron.log (this is a good place to look if you hit any issues with wiperf not running as expected) Initial Probe Testing Once the cron job has been configured, the case of the RPi, or the WLAN Pi has been put in to wiperf mode, it's time to check if the probe is working as expected. To perform tests, the probe will need to be connected to a network and able to reach the data server. The easiest way to monitor the operation of the probe is to SSH in to the probe and monitor the output of the log file /var/log/wiperf_agent.log . This file is created the first time that wiperf runs. If the file is not created after 5 minutes, then check the log file /var/log/wiperf_cron.log for error messages, as something fundamental is wrong with the installation. To watch the output of /var/log/wiperf_agent.log in real-time and view activity as data is collected every 5 minutes, run the following command on the CLI of the probe: tail -f /var/log/wiperf_agent.log Every 5 minutes, new log output will be seen that look similar to this: 2020-07-11 11:47:04,214 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,215 - Probe_Log - INFO - Starting logging... 2020-07-11 11:47:04,216 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,240 - Probe_Log - INFO - Checking if we use remote cfg file... 2020-07-11 11:47:04,241 - Probe_Log - INFO - No remote cfg file confgured...using current local ini file. 2020-07-11 11:47:04,242 - Probe_Log - INFO - No lock file found. Creating lock file. 2020-07-11 11:47:04,243 - Probe_Log - INFO - ########## Network connection checks ########## 2020-07-11 11:47:05,245 - Probe_Log - INFO - Checking wireless connection is good...(layer 1 &2) 2020-07-11 11:47:05,246 - Probe_Log - INFO - Checking wireless connection available. 2020-07-11 11:47:05,355 - Probe_Log - INFO - Checking we're connected to the network (layer3) 2020-07-11 11:47:05,356 - Probe_Log - INFO - Checking we have an IP address. 2020-07-11 11:47:05,379 - Probe_Log - INFO - Checking we can do a DNS lookup to google.com 2020-07-11 11:47:05,406 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:05,430 - Probe_Log - INFO - Checked interface route to : 216.58.212.238. Result: 216.58.212.238 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:05,431 - Probe_Log - INFO - Checking we can get to the management platform... 2020-07-11 11:47:05,432 - Probe_Log - INFO - Checking we will send mgt traffic over configured interface 'lo' mode. 2020-07-11 11:47:05,455 - Probe_Log - INFO - Checked interface route to : 127.0.0.1. Result: local 127.0.0.1 dev lo src 127.0.0.1 uid 0 2020-07-11 11:47:05,456 - Probe_Log - INFO - Interface mgt interface route looks good. 2020-07-11 11:47:05,457 - Probe_Log - INFO - Checking port connection to InfluxDB server 127.0.0.1, port: 8086 2020-07-11 11:47:05,484 - Probe_Log - INFO - Port connection to server 127.0.0.1, port: 8086 checked OK. 2020-07-11 11:47:05,485 - Probe_Log - INFO - ########## Wireless Connection ########## 2020-07-11 11:47:05,486 - Probe_Log - INFO - Wireless connection data: SSID:BNL, BSSID:5C:5B:35:C8:4D:C2, Freq:5.5, Center Freq:5.51, Channel: 100, Channel Width: 40, Tx Phy rate:200.0, Rx Phy rate:135.0, Tx MCS: 0, Rx MCS: 0, RSSI:-42.0, Tx retries:187, IP address:192.168.0.48 2020-07-11 11:47:05,486 - Probe_Log - INFO - InfluxDB update: wiperf-network, source=Network Tests 2020-07-11 11:47:05,487 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:05,573 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:05,574 - Probe_Log - INFO - Connection results sent OK. 2020-07-11 11:47:05,595 - Probe_Log - INFO - ########## speedtest ########## 2020-07-11 11:47:05,597 - Probe_Log - INFO - Starting speedtest... 2020-07-11 11:47:06,599 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:06,623 - Probe_Log - INFO - Checked interface route to : 8.8.8.8. Result: 8.8.8.8 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:06,624 - Probe_Log - INFO - Speedtest in progress....please wait. 2020-07-11 11:47:28,761 - Probe_Log - INFO - ping_time: 31, download_rate: 41.56, upload_rate: 9.74, server_name: speedtest-net5.rapidswitch.co.uk:8080 2020-07-11 11:47:28,766 - Probe_Log - INFO - Speedtest ended. 2020-07-11 11:47:28,767 - Probe_Log - INFO - InfluxDB update: wiperf-speedtest, source=Speedtest 2020-07-11 11:47:28,768 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:28,858 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:28,860 - Probe_Log - INFO - Speedtest results sent OK. The output is quite verbose and detailed, but it will provide a good indication of where wiperf is having difficulties. Once wiperf is running with no issues indicated in the logs, then it's time to check for results data on your data server. Hopefully, you'll see performance data being recorded over time as the probe runs its tests and sends the results to the data server.","title":"Probe Configuration"},{"location":"probe_configure/#probe-configuration","text":"The final step in getting our probe ready to deploy is to configure the wiperf software to perform the tests we'd like to perform, and to tell wiperf where it can find the data server that will provide reporting. The configuration tasks break down as follows: Edit the probe config.ini file to configure tests and data server details Add a cron job on the probe to run wiperf every 5 mins to perform its tests","title":"Probe Configuration"},{"location":"probe_configure/#configuration-file","text":"Note: the details in this section apply to both the WLAN Pi and RPi probe The operation of wiperf is configured using the file /etc/wiperf/config.ini This needs to be edited prior to running the wiperf software to perform tests. (Tests are initiated on the WLAN Pi by switching on to wiperf mode. On the RPi, tests are started by configuring a cron job - more on this later in this document) Prior to the first use of wiperf, the config.ini file does not exist. However, a default template config file ( /etc/wiperf/config.default.ini ) is supplied that can be used as the template to create the config.ini file. Here is the suggested workflow to create the config.ini file: Connect to the CLI of the probe (e.g. via SSH), create a copy of the config template file and edit the newly created config: cd /etc/wiperf # take a copy of the default configuration file cp ./config.default.ini ./config.ini # edit the config file with the required probe settings (ctrl-x to exit the editor) sudo nano ./config.ini By default, the configuration file is set to run all tests (which may or may not suit your needs). However, there is a minimum configuration that must be applied to successfully run tests. The minimum configuration parameters you need to configure (just to get you going) are outlined in the subsections below. Once you've got your probe going, you're likely going to want to spend a little more time customising the file for your environment. In summary you need to: Configure the wiperf global mode of operation (wireless or Ethernet) and the interface parameters that determine how the probe is connected to its network Configure the management platform you'll be sending data to Configure the tests you'd like to run","title":"Configuration File"},{"location":"probe_configure/#modeinterface-parameters","text":"The probe can be used to perform its tests over its wireless interface, or its ethernet interface. These are known as 'wireless' or 'ethernet' mode in the config.ini file. In addition, the probe needs to know which interface is used to send results data back to the data server. It is possible to perform tests and send results data over the same interface, or it may be preferable to have tests performed over the wireless interface and return results over the ethernet interface. The final choice is determined by the environment in to which the probe is deployed. (Note: if you choose to use Zerotier for management connectivity, the Zerotier interface is also an option available). The interfaces available in the probe for ethernet and wireless connectivity will generally be eth0 and wlan0 . However, these may vary in some platforms, there the option to change the actual names of the interfaces of the probe is available if required. The relevant section of the config.ini file is shown below for reference (note that lines that start with a semi-colon (;) are comments and are ignored. Blank lines are also ignored.): [General] ; global test mode: 'wireless' or 'ethernet' ; ; wireless mode: ; - test traffic runs over wireless interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; ethernet mode: ; - test traffic runs over ethernet interface ; - management traffic (i.e. result data) sent over interface specified in mgt_if parameter ; probe_mode: wireless ; ------------- ethernet mode parameters ------------ ; eth interface name - set this as per the output of an ifconfig command (usually eth0) eth_if: eth0 ; --------------------------------------------------- ; ------------- wireless mode parameters ------------ ; wlan interface name - set this as per the output of an iwconfig command (usually wlan0) wlan_if: wlan0 ; --------------------------------------------------- ; -------------mgt interface parameters ------------ ; interface name over which mgt traffic is sent (i.e. how we get to our management ; server) - options: wlan0, eth0, ztxxxxxx (ZeroTier), lo (local instance of Influx) mgt_if: wlan0 ; ---------------------------------------------------","title":"Mode/Interface Parameters"},{"location":"probe_configure/#data-server-parameters","text":"Wiperf can send results data to Splunk and InfluxDB (v1.x) data collectors through an exporter module for each collector type. The relevant authentication parameters need to be set for the collector in-use in the following sections (note these also need to be configured on the data collector platform also before sending results data - see here for more info: Splunk / InfluxDB ) In summary, the workflow to configure the data server parameters in the probe configuration file is to: Set the exporter type (splunk/influxdb) configure the server address of the target data server configure data server port details (if defaults changed) configure data server credential and database information The relevant section of the config.ini file is shown below: ; --------- Common Mgt Platform Params ------- ; set the data exporter type - current options: splunk, influxdb, influxdb2 exporter_type: splunk ; -------------------------------------------- ; -------------- Splunk Config --------------- ; IP address or hostname of Splunk host splunk_host: ; Splunk collector port (8088 by default) splunk_port: 8088 ; Splunk token to access Splunk server created by Splunk (example token: 84adb9ca-071c-48ad-8aa1-b1903c60310d) splunk_token: ;--------------------------------------------- ; -------------- InFlux1 Config --------------- ; IP address or hostname of InfluxDB host influx_host: ; InfluxDb collector port (8086 by default) influx_port: 8086 influx_username: influx_password: influx_database: ;---------------------------------------------","title":"Data Server Parameters"},{"location":"probe_configure/#network-tests","text":"Note that all network tests are enabled by default. If there are some tests you'd like to disable (e.g. if you don't have an iperf3 server set up), then you'll need to open up the config.ini file and look through each section for the \"enabled\" parameter for that test and set it to \"no\". For example, to disable the iperf tcp test: [Iperf3_tcp_test] ; yes = enabled, no = disabled enabled: no For a full description of the configuration file parameters, please review the following page: config.ini reference guide .","title":"Network Tests"},{"location":"probe_configure/#running-regular-tests","text":"Once the wiperf software has been configured, the final job is to configure a 'cron job' on the probe to run the software every 5 minutes. Cron is a scheduler utility within Linux that will run a software task at configured intervals. ( Note: This step is not required on the WLAN Pi, as the cron job is added automatically when the WLAN Pi is switched in to wiperf mode ) To configure cron, on the CLI of the probe, open the cron editor: sudo crontab -e Next, with the editor open, add following line to the open file: 0-59/5 * * * * /usr/bin/python3 /usr/share/wiperf/wiperf_run.py > /var/log/wiperf_cron.log 2>&1 This command will run the main wiperf script to run the tests configured within config.ini at an interval of 5 minutes. It will also dump all script output to the file /var/log/wiperf_cron.log (this is a good place to look if you hit any issues with wiperf not running as expected)","title":"Running Regular Tests"},{"location":"probe_configure/#initial-probe-testing","text":"Once the cron job has been configured, the case of the RPi, or the WLAN Pi has been put in to wiperf mode, it's time to check if the probe is working as expected. To perform tests, the probe will need to be connected to a network and able to reach the data server. The easiest way to monitor the operation of the probe is to SSH in to the probe and monitor the output of the log file /var/log/wiperf_agent.log . This file is created the first time that wiperf runs. If the file is not created after 5 minutes, then check the log file /var/log/wiperf_cron.log for error messages, as something fundamental is wrong with the installation. To watch the output of /var/log/wiperf_agent.log in real-time and view activity as data is collected every 5 minutes, run the following command on the CLI of the probe: tail -f /var/log/wiperf_agent.log Every 5 minutes, new log output will be seen that look similar to this: 2020-07-11 11:47:04,214 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,215 - Probe_Log - INFO - Starting logging... 2020-07-11 11:47:04,216 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,240 - Probe_Log - INFO - Checking if we use remote cfg file... 2020-07-11 11:47:04,241 - Probe_Log - INFO - No remote cfg file confgured...using current local ini file. 2020-07-11 11:47:04,242 - Probe_Log - INFO - No lock file found. Creating lock file. 2020-07-11 11:47:04,243 - Probe_Log - INFO - ########## Network connection checks ########## 2020-07-11 11:47:05,245 - Probe_Log - INFO - Checking wireless connection is good...(layer 1 &2) 2020-07-11 11:47:05,246 - Probe_Log - INFO - Checking wireless connection available. 2020-07-11 11:47:05,355 - Probe_Log - INFO - Checking we're connected to the network (layer3) 2020-07-11 11:47:05,356 - Probe_Log - INFO - Checking we have an IP address. 2020-07-11 11:47:05,379 - Probe_Log - INFO - Checking we can do a DNS lookup to google.com 2020-07-11 11:47:05,406 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:05,430 - Probe_Log - INFO - Checked interface route to : 216.58.212.238. Result: 216.58.212.238 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:05,431 - Probe_Log - INFO - Checking we can get to the management platform... 2020-07-11 11:47:05,432 - Probe_Log - INFO - Checking we will send mgt traffic over configured interface 'lo' mode. 2020-07-11 11:47:05,455 - Probe_Log - INFO - Checked interface route to : 127.0.0.1. Result: local 127.0.0.1 dev lo src 127.0.0.1 uid 0 2020-07-11 11:47:05,456 - Probe_Log - INFO - Interface mgt interface route looks good. 2020-07-11 11:47:05,457 - Probe_Log - INFO - Checking port connection to InfluxDB server 127.0.0.1, port: 8086 2020-07-11 11:47:05,484 - Probe_Log - INFO - Port connection to server 127.0.0.1, port: 8086 checked OK. 2020-07-11 11:47:05,485 - Probe_Log - INFO - ########## Wireless Connection ########## 2020-07-11 11:47:05,486 - Probe_Log - INFO - Wireless connection data: SSID:BNL, BSSID:5C:5B:35:C8:4D:C2, Freq:5.5, Center Freq:5.51, Channel: 100, Channel Width: 40, Tx Phy rate:200.0, Rx Phy rate:135.0, Tx MCS: 0, Rx MCS: 0, RSSI:-42.0, Tx retries:187, IP address:192.168.0.48 2020-07-11 11:47:05,486 - Probe_Log - INFO - InfluxDB update: wiperf-network, source=Network Tests 2020-07-11 11:47:05,487 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:05,573 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:05,574 - Probe_Log - INFO - Connection results sent OK. 2020-07-11 11:47:05,595 - Probe_Log - INFO - ########## speedtest ########## 2020-07-11 11:47:05,597 - Probe_Log - INFO - Starting speedtest... 2020-07-11 11:47:06,599 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:06,623 - Probe_Log - INFO - Checked interface route to : 8.8.8.8. Result: 8.8.8.8 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:06,624 - Probe_Log - INFO - Speedtest in progress....please wait. 2020-07-11 11:47:28,761 - Probe_Log - INFO - ping_time: 31, download_rate: 41.56, upload_rate: 9.74, server_name: speedtest-net5.rapidswitch.co.uk:8080 2020-07-11 11:47:28,766 - Probe_Log - INFO - Speedtest ended. 2020-07-11 11:47:28,767 - Probe_Log - INFO - InfluxDB update: wiperf-speedtest, source=Speedtest 2020-07-11 11:47:28,768 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:28,858 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:28,860 - Probe_Log - INFO - Speedtest results sent OK. The output is quite verbose and detailed, but it will provide a good indication of where wiperf is having difficulties. Once wiperf is running with no issues indicated in the logs, then it's time to check for results data on your data server. Hopefully, you'll see performance data being recorded over time as the probe runs its tests and sends the results to the data server.","title":"Initial Probe Testing"},{"location":"probe_deploy/","text":"Probe Deployment Once the probe is configured and tested, it's time to deploy it out in the real world. When deploying, here are a few thinks to check and remember: Verify that the probe has the network connectivity that you expect once it has been deployed. The following CLI commands will help to check connectivity: Wireless NIC: iwconfig (is the probe joining the wireless network?) IP address: ifconfig (do the interfaces being used have an IP address?) Internet connectivity: ping google.com (can the probe get to the Internet, if that is expected?) Is the probe deployed in the topology you originally intended? If the environment is not as you expected and you need to use a different interface, make sure you have updated config.ini so that wiperf knows where to send traffic (otherwise, you may hit routing issues) Check the output of /var/log/wiperf_agent.log to make sure everything is working with no issues once deployed. If there are hitches, they will generally be highlighted in this file, with a detailed explanation of what has failed.","title":"Probe Deployment"},{"location":"probe_deploy/#probe-deployment","text":"Once the probe is configured and tested, it's time to deploy it out in the real world. When deploying, here are a few thinks to check and remember: Verify that the probe has the network connectivity that you expect once it has been deployed. The following CLI commands will help to check connectivity: Wireless NIC: iwconfig (is the probe joining the wireless network?) IP address: ifconfig (do the interfaces being used have an IP address?) Internet connectivity: ping google.com (can the probe get to the Internet, if that is expected?) Is the probe deployed in the topology you originally intended? If the environment is not as you expected and you need to use a different interface, make sure you have updated config.ini so that wiperf knows where to send traffic (otherwise, you may hit routing issues) Check the output of /var/log/wiperf_agent.log to make sure everything is working with no issues once deployed. If there are hitches, they will generally be highlighted in this file, with a detailed explanation of what has failed.","title":"Probe Deployment"},{"location":"probe_install/","text":"Probe Software Installation This section takes a look at how we install various additional required software packages on to our probe. This includes any pre-requisite software packages and the wiperf software itself. WLAN Pi Good news! If you're using a WLAN Pi (v2.x image), you already have the software you require - it's part of the WLAN Pi software image. Go to the next section of this documentation site . Raspberry Pi The RPi requires a few pre-requisite Linux packages before we can install the wiperf software itself. Note that the probe must be connected to a network (via ethernet or wireless) that has access to the Internet to download the required code. You will need CLI access to the probe to perform the steps detailed below. Package Updates Before we start adding pre-requisite packages, it's always a good idea to update the existing Linux packages on our RPi to make sure we have the \"latest and greatest\". This may take a few minutes to complete as many files may be downloaded & updated, depending on when/if your RPi was last updated: sudo apt-get update && sudo apt-get upgrade -y sudo reboot Pre-requisite Packages Next, we need to install additional Linux packages that are not included as part of the standard RPi distribution: pip3, iperf3 and git. These are installed as follows: sudo apt-get update sudo apt-get install python3-pip iperf3 git -y sudo reboot wiperf Software Installation To install the wiperf code itself on to the RPi, execute the following command: curl -s https://raw.githubusercontent.com/wifinigel/wiperf/setup.sh | sudo bash -s install rpi This will initiate the download and installation of a number of python packages, together with the wiperf code itself. This will take a few minutes to complete. Once installation is complete, our final step is to configure the wiperf probe to perform the tests we'd like to perform, and provide details of were the probe needs to send its data (i.e. our data server).","title":"Probe Software Installation"},{"location":"probe_install/#probe-software-installation","text":"This section takes a look at how we install various additional required software packages on to our probe. This includes any pre-requisite software packages and the wiperf software itself.","title":"Probe Software Installation"},{"location":"probe_install/#wlan-pi","text":"Good news! If you're using a WLAN Pi (v2.x image), you already have the software you require - it's part of the WLAN Pi software image. Go to the next section of this documentation site .","title":"WLAN Pi"},{"location":"probe_install/#raspberry-pi","text":"The RPi requires a few pre-requisite Linux packages before we can install the wiperf software itself. Note that the probe must be connected to a network (via ethernet or wireless) that has access to the Internet to download the required code. You will need CLI access to the probe to perform the steps detailed below.","title":"Raspberry Pi"},{"location":"probe_install/#package-updates","text":"Before we start adding pre-requisite packages, it's always a good idea to update the existing Linux packages on our RPi to make sure we have the \"latest and greatest\". This may take a few minutes to complete as many files may be downloaded & updated, depending on when/if your RPi was last updated: sudo apt-get update && sudo apt-get upgrade -y sudo reboot","title":"Package Updates"},{"location":"probe_install/#pre-requisite-packages","text":"Next, we need to install additional Linux packages that are not included as part of the standard RPi distribution: pip3, iperf3 and git. These are installed as follows: sudo apt-get update sudo apt-get install python3-pip iperf3 git -y sudo reboot","title":"Pre-requisite Packages"},{"location":"probe_install/#wiperf-software-installation","text":"To install the wiperf code itself on to the RPi, execute the following command: curl -s https://raw.githubusercontent.com/wifinigel/wiperf/setup.sh | sudo bash -s install rpi This will initiate the download and installation of a number of python packages, together with the wiperf code itself. This will take a few minutes to complete. Once installation is complete, our final step is to configure the wiperf probe to perform the tests we'd like to perform, and provide details of were the probe needs to send its data (i.e. our data server).","title":"wiperf Software Installation"},{"location":"probe_platform/","text":"Probe Platform Wiperf has been primarily designed to work on the NEO2 version of the WLAN Pi platform and the Raspberry Pi. WLAN Pi Wiperf is baked in to the image of the WLAN Pi. It can be activated by switching in to wiperf mode on the WLAN Pi. Find out more details at the official documentation site for the WLAN Pi: https://wlan-pi.github.io/wlanpi-documentation/ Raspberry Pi Wiperf on the RPi has been tested on models that have an internal Wi-Fi NIC: 3b+, 3a+ and 4. It will likely work on most that have an internal NIC, but I don't have the resources or time to try them all. Earlier versions of the RPi that do not have a an internal NIC will need some type of USB wireless adapter, but as support for external wireless NICs is very poor and many tend to be 2.4GHz only, I've not explored this area in detail. Unfortunately, getting a 2 stream 802.11ac NIC going seems nigh-on impossible due to the lack of drivers available, so the internal, single stream NIC is the best we can generally do. Using a single stream NIC has its limitations as speed performance is very limited, but as the main aim of wiperf is to monitor user experience (particularly changes in that experience), then it's good enough for many use-cases where we are mainly interested in changes in relation to the usual baseline. Other Platforms In essence, wiperf is a series of python scripts & modules, together with a few supporting bash scripts to glue a few things together. It will likely work on other Debian-type systems, so it's worth giving it a go on other systems if you fancy tinkering around on another platform. When using the install script, install using the 'rpi' option. Let me know if you get it going on other platforms, as it will be interesting to share your experiences.","title":"Probe Platform"},{"location":"probe_platform/#probe-platform","text":"Wiperf has been primarily designed to work on the NEO2 version of the WLAN Pi platform and the Raspberry Pi.","title":"Probe Platform"},{"location":"probe_platform/#wlan-pi","text":"Wiperf is baked in to the image of the WLAN Pi. It can be activated by switching in to wiperf mode on the WLAN Pi. Find out more details at the official documentation site for the WLAN Pi: https://wlan-pi.github.io/wlanpi-documentation/","title":"WLAN Pi"},{"location":"probe_platform/#raspberry-pi","text":"Wiperf on the RPi has been tested on models that have an internal Wi-Fi NIC: 3b+, 3a+ and 4. It will likely work on most that have an internal NIC, but I don't have the resources or time to try them all. Earlier versions of the RPi that do not have a an internal NIC will need some type of USB wireless adapter, but as support for external wireless NICs is very poor and many tend to be 2.4GHz only, I've not explored this area in detail. Unfortunately, getting a 2 stream 802.11ac NIC going seems nigh-on impossible due to the lack of drivers available, so the internal, single stream NIC is the best we can generally do. Using a single stream NIC has its limitations as speed performance is very limited, but as the main aim of wiperf is to monitor user experience (particularly changes in that experience), then it's good enough for many use-cases where we are mainly interested in changes in relation to the usual baseline.","title":"Raspberry Pi"},{"location":"probe_platform/#other-platforms","text":"In essence, wiperf is a series of python scripts & modules, together with a few supporting bash scripts to glue a few things together. It will likely work on other Debian-type systems, so it's worth giving it a go on other systems if you fancy tinkering around on another platform. When using the install script, install using the 'rpi' option. Let me know if you get it going on other platforms, as it will be interesting to share your experiences.","title":"Other Platforms"},{"location":"probe_prepare/","text":"Probe Preparation The wiperf probe needs to have a few pre-requisite activities completed prior to the installation of the wiperf code. These vary slightly between the WLAN Pi and RPi platforms, but broadly break down as: Software image preparation CLI Access Configure the device hostname Configure network connectivity Add pre-requisite software packages. WLAN Pi Software Image There is little to do in terms of software image preparation for the WLAN Pi. Visit the WLAN Pi documentation site to find out how to obtain the WLAN Pi image: https://wlan-pi.github.io/wlanpi-documentation/ . If you install the a WLAN Pi image, wiperf will already be installed as part of the image. (Note: all information provided below assumes you are using a 2.0 or later version of the WLAN Pi image) Probe CLI Access To perform some of the configuration activities required, CLI access to the WLAN Pi is required. The easiest way to achieve this is to SSH to the probe over an OTG connection, or plug the WLAN Pi in to an ethernet network port and SSH to its DHCP assigned IP address (shown on the front panel). Visit the WLAN Pi documentation site for more details: https://wlan-pi.github.io/wlanpi-documentation/ Hostname Configuration By default, the hostname of your WLAN Pi is : wlanpi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change to a more meaningful hostname, then you will need to SSH to your WLAN Pi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the WLAN Pi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'wlanpi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Alternatively, you may also use the following CLI command to achieve the same result: sudo hostnamectl set-hostname <name> Whichever method is used to update the hostname file, next edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'wlanpi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your WLAN Pi: sudo reboot Network Connectivity Ethernet If the probe is to be connected by Ethernet only, then there is no additional configuration required. By default, if a switch port that can supply a DHCP address is used, then the probe will have the required network connection. Wireless Configuration (wpa_supplicant.conf) If wiperf is running in wireless mode, when the WLAN Pi is flipped in to wiperf mode, it will need to join the SSID under test to run the configured tests. We need to provide a configuration (that is only used in wiperf mode) to allow the WLAN Pi to join a WLAN. Edit the following file with the configuration and credentials that will be used by the WLAN Pi to join the SSID under test once it is switched in to wiperf mode: cd /etc/wiperf/conf/etc/wpa_supplicant sudo nano ./wpa_supplicant.conf There are a number of sample configurations included in the default file provided (PSK, PEAP & Open auth). Uncomment the required section and add in the correct SSID & authentication details. (For EAP-TLS, it's time to check-out Google as I've not had opportunity to figure that scenario out...) (Note: This configuration is only used when the WLAN Pi is flipped in to wiperf mode, not for standard (classic mode) connectivity) Raspberry Pi Software Image I would strongly recommend starting with a fresh image using the latest and greatest Raspberry Pi OS (previously called Raspbian): https://www.raspberrypi.org/downloads/raspberry-pi-os/ . For the development and testing of the wiperf code, version 10 (Buster) was used. You can check the version on your RPi using the cat /etc/os0-release command. Here is my sample output: pi@probe7:~$ cat /etc/os-release PRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\" NAME=\"Raspbian GNU/Linux\" VERSION_ID=\"10\" VERSION=\"10 (buster)\" VERSION_CODENAME=buster ID=raspbian ID_LIKE=debian HOME_URL=\"http://www.raspbian.org/\" SUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\" BUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\" Note that you will likely be able to use any recent version, so don't feel compelled to use this exact version. The download page provided above also has links to resources to guide you through burning the image on to your SD card. (You may also like to check out the 'Probe CLI Access' section below to setup SSH access to your headless RPI before booting from your new image) Once you have burned your image, I'd also recommend you apply all latest updates & give it a reboot for good measure: sudo apt-get update && sudo apt-get upgrade sudo reboot Probe CLI Access You will need CLI access to perform the required configuration steps for wiperf. There are a number of ways of gaining this access that are detailed in this document: https://www.raspberrypi.org/documentation/remote-access/ssh/ . My personal favourite is to enable SSH on a headless RPi by adding an 'ssh' file to the SD card prior to boot-up. Default Login Account Password If using a fresh RPI image (which is recommended), remember to either update the default 'pi' username with a new password so that your are not running with the default login of : pi/raspberry (user/pwd) Change password : sudo passwd pi Hostname Configuration By default, the hostname of your RPi is : pi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change this to a more meaningful hostname, then you will need to SSH to your WLAN Pi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the RPi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'pi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Alternatively, you may also use the following CLI command to achieve the same result: sudo hostnamectl set-hostname <name> Whichever method is used to update the hostname file, next edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'pi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your RPi: sudo reboot Network Connectivity Ethernet If the probe is to be connected by Ethernet you will need to make some additions to the /etc/network/interfaces file to ensure you have network connectivity. Add the following lines to configure the Ethernet port for DHCP connectivity: # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp These lines may be added anywhere in the file, using a CLI editor such as nano: sudo nano /etc/network/interfaces Wireless Configuration The RPi needs to be configured to join the wireless network that you'd like to test. To join a network, we need to configure the wireless interface and provide the network credentials to join the network. To achieve this, we need to edit two files on the CLI of the RPI: sudo nano /etc/wpa_supplicant/wpa_supplicant.conf sudo nano /etc/network/interfaces Sample configurations for both files are provided below. /etc/network/interfaces # wiperf interface config file # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp # Wireless adapter #1 allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf # wireless-power off # post-up iw dev wlan0 set power_save off # Local loopback auto lo iface lo inet loopback Note: The wireless power off commands are commented out in the file above. One of these needs to be uncommented to stop the wireless NC dropping in to power save mode. If you see huge drops in the wireless connection speed in the wireless connection graph, it is being caused by power save mode. Unfortunately, the command to use seems to vary between RPi model and operating system version. When you see the connection speed issue, try uncommenting one of the commands and reboot. If it doesn't fix the issue, try the other command. (see this article for more info ) /etc/wpa_supplicant/wpa_supplicant.conf ap_scan=1 # WPA2 PSK Network sample (highest priority - joined first) network={ ssid=\"enter SSID Name\" psk=\"enter key\" priority=10 } ####################################################################################### # NOTE: to use the templates below, remove the hash symbols at the start of each line ####################################################################################### # WPA2 PSK Network sample (next priority - joined if first priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # psk=\"enter key\" # priority=3 #} # WPA2 PEAP example (next priority - joined if second priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=WPA-EAP # eap=PEAP # anonymous_identity=\"anonymous\" # identity=\"enter your username\" # password=\"enter your password\" # phase2=\"autheap=MSCHAPV2\" # priority=2 #} # Open network example (lowest priority, only joined other 3 networks not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=NONE # priority=1 #} Note that the file includes several samples for a variety of security methods. You will need to uncomment the security mthod for your environment and comment out all other methods. By default, the PSK method is used, bit requires that you enter an SSID and shared key. Test Wireless Connection Once configuration is complete, reboot the RPI. To test if the wireless connection has come up OK, use the following commands to see if wireless interface has joined the wireless network and has an IP address: iwconfig ifconfig","title":"Probe Preparation"},{"location":"probe_prepare/#probe-preparation","text":"The wiperf probe needs to have a few pre-requisite activities completed prior to the installation of the wiperf code. These vary slightly between the WLAN Pi and RPi platforms, but broadly break down as: Software image preparation CLI Access Configure the device hostname Configure network connectivity Add pre-requisite software packages.","title":"Probe Preparation"},{"location":"probe_prepare/#wlan-pi","text":"","title":"WLAN Pi"},{"location":"probe_prepare/#software-image","text":"There is little to do in terms of software image preparation for the WLAN Pi. Visit the WLAN Pi documentation site to find out how to obtain the WLAN Pi image: https://wlan-pi.github.io/wlanpi-documentation/ . If you install the a WLAN Pi image, wiperf will already be installed as part of the image. (Note: all information provided below assumes you are using a 2.0 or later version of the WLAN Pi image)","title":"Software Image"},{"location":"probe_prepare/#probe-cli-access","text":"To perform some of the configuration activities required, CLI access to the WLAN Pi is required. The easiest way to achieve this is to SSH to the probe over an OTG connection, or plug the WLAN Pi in to an ethernet network port and SSH to its DHCP assigned IP address (shown on the front panel). Visit the WLAN Pi documentation site for more details: https://wlan-pi.github.io/wlanpi-documentation/","title":"Probe CLI Access"},{"location":"probe_prepare/#hostname-configuration","text":"By default, the hostname of your WLAN Pi is : wlanpi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change to a more meaningful hostname, then you will need to SSH to your WLAN Pi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the WLAN Pi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'wlanpi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Alternatively, you may also use the following CLI command to achieve the same result: sudo hostnamectl set-hostname <name> Whichever method is used to update the hostname file, next edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'wlanpi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your WLAN Pi: sudo reboot","title":"Hostname Configuration"},{"location":"probe_prepare/#network-connectivity","text":"","title":"Network Connectivity"},{"location":"probe_prepare/#ethernet","text":"If the probe is to be connected by Ethernet only, then there is no additional configuration required. By default, if a switch port that can supply a DHCP address is used, then the probe will have the required network connection.","title":"Ethernet"},{"location":"probe_prepare/#wireless-configuration-wpa_supplicantconf","text":"If wiperf is running in wireless mode, when the WLAN Pi is flipped in to wiperf mode, it will need to join the SSID under test to run the configured tests. We need to provide a configuration (that is only used in wiperf mode) to allow the WLAN Pi to join a WLAN. Edit the following file with the configuration and credentials that will be used by the WLAN Pi to join the SSID under test once it is switched in to wiperf mode: cd /etc/wiperf/conf/etc/wpa_supplicant sudo nano ./wpa_supplicant.conf There are a number of sample configurations included in the default file provided (PSK, PEAP & Open auth). Uncomment the required section and add in the correct SSID & authentication details. (For EAP-TLS, it's time to check-out Google as I've not had opportunity to figure that scenario out...) (Note: This configuration is only used when the WLAN Pi is flipped in to wiperf mode, not for standard (classic mode) connectivity)","title":"Wireless Configuration (wpa_supplicant.conf)"},{"location":"probe_prepare/#raspberry-pi","text":"","title":"Raspberry Pi"},{"location":"probe_prepare/#software-image_1","text":"I would strongly recommend starting with a fresh image using the latest and greatest Raspberry Pi OS (previously called Raspbian): https://www.raspberrypi.org/downloads/raspberry-pi-os/ . For the development and testing of the wiperf code, version 10 (Buster) was used. You can check the version on your RPi using the cat /etc/os0-release command. Here is my sample output: pi@probe7:~$ cat /etc/os-release PRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\" NAME=\"Raspbian GNU/Linux\" VERSION_ID=\"10\" VERSION=\"10 (buster)\" VERSION_CODENAME=buster ID=raspbian ID_LIKE=debian HOME_URL=\"http://www.raspbian.org/\" SUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\" BUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\" Note that you will likely be able to use any recent version, so don't feel compelled to use this exact version. The download page provided above also has links to resources to guide you through burning the image on to your SD card. (You may also like to check out the 'Probe CLI Access' section below to setup SSH access to your headless RPI before booting from your new image) Once you have burned your image, I'd also recommend you apply all latest updates & give it a reboot for good measure: sudo apt-get update && sudo apt-get upgrade sudo reboot","title":"Software Image"},{"location":"probe_prepare/#probe-cli-access_1","text":"You will need CLI access to perform the required configuration steps for wiperf. There are a number of ways of gaining this access that are detailed in this document: https://www.raspberrypi.org/documentation/remote-access/ssh/ . My personal favourite is to enable SSH on a headless RPi by adding an 'ssh' file to the SD card prior to boot-up.","title":"Probe CLI Access"},{"location":"probe_prepare/#default-login-account-password","text":"If using a fresh RPI image (which is recommended), remember to either update the default 'pi' username with a new password so that your are not running with the default login of : pi/raspberry (user/pwd) Change password : sudo passwd pi","title":"Default Login Account Password"},{"location":"probe_prepare/#hostname-configuration_1","text":"By default, the hostname of your RPi is : pi . It is strongly advised to change its hostname if you have several probes reporting in to the same data server. If all use the same hostname, there will be no way of distinguishing data between devices. (Note that if you decide to skip this step and subsequently change the hostname, historical data from the probe will not be associated with the data sent with the new hostname in your data server) If you'd like to change this to a more meaningful hostname, then you will need to SSH to your WLAN Pi and update the /etc/hostname and /etc/hosts files, followed by a reboot of the RPi: Edit the /etc/hostname file using the command: sudo nano /etc/hostname There is a single line that says 'pi'. Change this to your required hostname. Then hit Ctrl-X and \"y\" to save your changes. Alternatively, you may also use the following CLI command to achieve the same result: sudo hostnamectl set-hostname <name> Whichever method is used to update the hostname file, next edit the /etc/hosts file: sudo nano /etc/hosts Change each instance of 'pi' to the new hostname (there are usually two instances). Then hit Ctrl-X and \"y\" to save your changes. Finally, reboot your RPi: sudo reboot","title":"Hostname Configuration"},{"location":"probe_prepare/#network-connectivity_1","text":"","title":"Network Connectivity"},{"location":"probe_prepare/#ethernet_1","text":"If the probe is to be connected by Ethernet you will need to make some additions to the /etc/network/interfaces file to ensure you have network connectivity. Add the following lines to configure the Ethernet port for DHCP connectivity: # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp These lines may be added anywhere in the file, using a CLI editor such as nano: sudo nano /etc/network/interfaces","title":"Ethernet"},{"location":"probe_prepare/#wireless-configuration","text":"The RPi needs to be configured to join the wireless network that you'd like to test. To join a network, we need to configure the wireless interface and provide the network credentials to join the network. To achieve this, we need to edit two files on the CLI of the RPI: sudo nano /etc/wpa_supplicant/wpa_supplicant.conf sudo nano /etc/network/interfaces Sample configurations for both files are provided below.","title":"Wireless Configuration"},{"location":"probe_prepare/#etcnetworkinterfaces","text":"# wiperf interface config file # Wired adapter #1 allow-hotplug eth0 iface eth0 inet dhcp # Wireless adapter #1 allow-hotplug wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf # wireless-power off # post-up iw dev wlan0 set power_save off # Local loopback auto lo iface lo inet loopback Note: The wireless power off commands are commented out in the file above. One of these needs to be uncommented to stop the wireless NC dropping in to power save mode. If you see huge drops in the wireless connection speed in the wireless connection graph, it is being caused by power save mode. Unfortunately, the command to use seems to vary between RPi model and operating system version. When you see the connection speed issue, try uncommenting one of the commands and reboot. If it doesn't fix the issue, try the other command. (see this article for more info )","title":"/etc/network/interfaces"},{"location":"probe_prepare/#etcwpa_supplicantwpa_supplicantconf","text":"ap_scan=1 # WPA2 PSK Network sample (highest priority - joined first) network={ ssid=\"enter SSID Name\" psk=\"enter key\" priority=10 } ####################################################################################### # NOTE: to use the templates below, remove the hash symbols at the start of each line ####################################################################################### # WPA2 PSK Network sample (next priority - joined if first priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # psk=\"enter key\" # priority=3 #} # WPA2 PEAP example (next priority - joined if second priority not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=WPA-EAP # eap=PEAP # anonymous_identity=\"anonymous\" # identity=\"enter your username\" # password=\"enter your password\" # phase2=\"autheap=MSCHAPV2\" # priority=2 #} # Open network example (lowest priority, only joined other 3 networks not available) - don't unhash this line #network={ # ssid=\"enter SSID Name\" # key_mgmt=NONE # priority=1 #} Note that the file includes several samples for a variety of security methods. You will need to uncomment the security mthod for your environment and comment out all other methods. By default, the PSK method is used, bit requires that you enter an SSID and shared key.","title":"/etc/wpa_supplicant/wpa_supplicant.conf"},{"location":"probe_prepare/#test-wireless-connection","text":"Once configuration is complete, reboot the RPI. To test if the wireless connection has come up OK, use the following commands to see if wireless interface has joined the wireless network and has an IP address: iwconfig ifconfig","title":"Test Wireless Connection"},{"location":"splunk_configure/","text":"Splunk Configuration Now that we have a Splunk server setup, we need to customize it to report our probe data. The steps required are all via the Splunk web GUI and are the same for all OS flavours. Configure Data Input To Splunk We need to tell Splunk how we\u2019ll be sending the data from our probe in to Splunk. We need to configure a data input that will prepare Splunk to receive the data, and to generate an authorization key to be used by the probe when sending its results data. Log In To Splunk The first step is to login to Splunk using the credentials created during the Splunk install. The URL to use is: http://<Splunk_server_IP>:8000 Configure HTTP Event Collector Global Options After login, the following page will be seen: Follow the \u201cSettings > Data > Data Inputs\u201d menu options : Click on the HTTP Event Collector link in the Data Inputs page shown: Click on the \u201cGlobal Settings\u201d button as indicated in the graphic above to reveal the global configuration panel for the HTTP Event Collector: Ensure the panel is configured to look like the example shown above. This should require the following steps: Make sure you hit the All Tokens > Enabled button (this is disabled by default which stops everything working) Default Source Type: Structured > _json Hit: Save to take you back to the HTTP Event Collector page Create a HEC Token After returning to the HTTP Event Collector page, hit the New Token button. This will start a token creation wizard. Enter a name for the token (probe HEC Token) then hit Next > : In the next wizard panel select Source type: Select > Structured > _json : Scroll down to the indexes and make the following selections: Select Allowed Indexes > add all Default Index : main Next, hit the Review > button: The token review panel is now should and should look like the graphic below. Finally hit the Submit> button: A final confirmation message will be provided as shown below: If you return to Settings > Data Input > HTTPS Event Collector , you will now see the token your probe will need to communicate with the Splunk server: At this point, the Splunk server is ready to receive data from the probe. Ensure that your probe has been configured with the correct server IP address, port number and the token we have just created above (copy and paste the \u201cToken Value\u201d in to your probe config.ini file). If using the WLAN Pi, ensure that it is flipped in to wiperf mode. On the RPI, ensure that the required cron job has been configured to start polling. Perform a Test Search After a few minutes, when the probe has run a test cycle, data should start to appear in Splunk. The quickest way to check is to do a general search for data in Splunk and see what is being received. Go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and enter a \u201c*\u201d in the \u201cNew Search\u201d text box. Results data should be seen as shown below: If your search result looks like this (no results found message), then you need to wait a little longer for data to arrive, or there is likely a comms problem between your probe and Splunk: Create a Dashboard Now that we have data arriving at our Splunk server, we need to view the data in an interesting format. Splunk allows us to create reporting dashboards to visualize our data. We will now create a simple dashboard to demonstrate the visualization capabilities. In the probe\u2019s /usr/share/wiperf/dashboards directory, a number of pre-canned dashboard files have been provided to allow a simple copy & paste operation to create a dashboard. These files are also available on the GitHub page of the wiperf project: https://github.com/wifinigel/wiperf/tree/main/dashboards Use an SFTP client to pull the \u201c01 - probe_summary.xml\u201d file from your probe, or open the file on the GitHub page and select \u201cRaw\u201d to copy and paste the code in to a local file on your laptop. In the Splunk GUI, go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and hit the \u201cDashboards\u201d link: Hit the \"Create New Dashboard\" button: In the pop-up panel, enter a dashboard name and hit the \"Create Dashboard\" button: In the \u201cEdit Dashboard\u201d panel that opens, hit the \u201cSource\u201d button: By default, some basic XML configuration will exist in the dashboard definition: Open up the dashboard definition file previously downloaded from your probe or the GitHub site in a text editor. Then simply paste in the code as shown below (make sure the original code was all removed): After hitting the Save button, the dashboard will now be shown: Using the hostname and time period selector above the graphs, different probes and reporting periods may be viewed. The process above may be repeated using each of the xml files found in the dashboards folder to create a series of separate dashboards that focus on different aspects of data available using wiperf.","title":"Splunk Configuration"},{"location":"splunk_configure/#splunk-configuration","text":"Now that we have a Splunk server setup, we need to customize it to report our probe data. The steps required are all via the Splunk web GUI and are the same for all OS flavours.","title":"Splunk Configuration"},{"location":"splunk_configure/#configure-data-input-to-splunk","text":"We need to tell Splunk how we\u2019ll be sending the data from our probe in to Splunk. We need to configure a data input that will prepare Splunk to receive the data, and to generate an authorization key to be used by the probe when sending its results data.","title":"Configure Data Input To Splunk"},{"location":"splunk_configure/#log-in-to-splunk","text":"The first step is to login to Splunk using the credentials created during the Splunk install. The URL to use is: http://<Splunk_server_IP>:8000","title":"Log In To Splunk"},{"location":"splunk_configure/#configure-http-event-collector-global-options","text":"After login, the following page will be seen: Follow the \u201cSettings > Data > Data Inputs\u201d menu options : Click on the HTTP Event Collector link in the Data Inputs page shown: Click on the \u201cGlobal Settings\u201d button as indicated in the graphic above to reveal the global configuration panel for the HTTP Event Collector: Ensure the panel is configured to look like the example shown above. This should require the following steps: Make sure you hit the All Tokens > Enabled button (this is disabled by default which stops everything working) Default Source Type: Structured > _json Hit: Save to take you back to the HTTP Event Collector page","title":"Configure  HTTP Event Collector Global Options"},{"location":"splunk_configure/#create-a-hec-token","text":"After returning to the HTTP Event Collector page, hit the New Token button. This will start a token creation wizard. Enter a name for the token (probe HEC Token) then hit Next > : In the next wizard panel select Source type: Select > Structured > _json : Scroll down to the indexes and make the following selections: Select Allowed Indexes > add all Default Index : main Next, hit the Review > button: The token review panel is now should and should look like the graphic below. Finally hit the Submit> button: A final confirmation message will be provided as shown below: If you return to Settings > Data Input > HTTPS Event Collector , you will now see the token your probe will need to communicate with the Splunk server: At this point, the Splunk server is ready to receive data from the probe. Ensure that your probe has been configured with the correct server IP address, port number and the token we have just created above (copy and paste the \u201cToken Value\u201d in to your probe config.ini file). If using the WLAN Pi, ensure that it is flipped in to wiperf mode. On the RPI, ensure that the required cron job has been configured to start polling.","title":"Create a HEC Token"},{"location":"splunk_configure/#perform-a-test-search","text":"After a few minutes, when the probe has run a test cycle, data should start to appear in Splunk. The quickest way to check is to do a general search for data in Splunk and see what is being received. Go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and enter a \u201c*\u201d in the \u201cNew Search\u201d text box. Results data should be seen as shown below: If your search result looks like this (no results found message), then you need to wait a little longer for data to arrive, or there is likely a comms problem between your probe and Splunk:","title":"Perform a Test Search"},{"location":"splunk_configure/#create-a-dashboard","text":"Now that we have data arriving at our Splunk server, we need to view the data in an interesting format. Splunk allows us to create reporting dashboards to visualize our data. We will now create a simple dashboard to demonstrate the visualization capabilities. In the probe\u2019s /usr/share/wiperf/dashboards directory, a number of pre-canned dashboard files have been provided to allow a simple copy & paste operation to create a dashboard. These files are also available on the GitHub page of the wiperf project: https://github.com/wifinigel/wiperf/tree/main/dashboards Use an SFTP client to pull the \u201c01 - probe_summary.xml\u201d file from your probe, or open the file on the GitHub page and select \u201cRaw\u201d to copy and paste the code in to a local file on your laptop. In the Splunk GUI, go to \u201cApps : Search & Reporting > Search & Reporting\u201d (top menu bar) and hit the \u201cDashboards\u201d link: Hit the \"Create New Dashboard\" button: In the pop-up panel, enter a dashboard name and hit the \"Create Dashboard\" button: In the \u201cEdit Dashboard\u201d panel that opens, hit the \u201cSource\u201d button: By default, some basic XML configuration will exist in the dashboard definition: Open up the dashboard definition file previously downloaded from your probe or the GitHub site in a text editor. Then simply paste in the code as shown below (make sure the original code was all removed): After hitting the Save button, the dashboard will now be shown: Using the hostname and time period selector above the graphs, different probes and reporting periods may be viewed. The process above may be repeated using each of the xml files found in the dashboards folder to create a series of separate dashboards that focus on different aspects of data available using wiperf.","title":"Create a Dashboard"},{"location":"splunk_install/","text":"Splunk Installation Once the software is downloaded, follow the instructions that are appropriate for your OS in the Splunk installation manual: https://docs.splunk.com/Documentation/Splunk/latest/Installation/Chooseyourplatform The installation process for all platforms is very straightforward and is detailed in the official install guides, so will not be covered in detail here. Note When installing the Linux flavour of Splunk, make sure you do not miss the additional step required to ensure that Splunk starts after a server reboot. The following command needs to be executred after the software is installed (but please verify this isn the official installation documents): sudo /opt/splunk/bin/splunk enable boot-start Once installation has been completed, it should be possible to access the web dashboard of Splunk at the URL: http://<Splunk_server_IP>:8000","title":"Splunk Installation"},{"location":"splunk_install/#splunk-installation","text":"Once the software is downloaded, follow the instructions that are appropriate for your OS in the Splunk installation manual: https://docs.splunk.com/Documentation/Splunk/latest/Installation/Chooseyourplatform The installation process for all platforms is very straightforward and is detailed in the official install guides, so will not be covered in detail here. Note When installing the Linux flavour of Splunk, make sure you do not miss the additional step required to ensure that Splunk starts after a server reboot. The following command needs to be executred after the software is installed (but please verify this isn the official installation documents): sudo /opt/splunk/bin/splunk enable boot-start Once installation has been completed, it should be possible to access the web dashboard of Splunk at the URL: http://<Splunk_server_IP>:8000","title":"Splunk Installation"},{"location":"splunk_platform/","text":"Splunk Platform To collect and view the test results data, an instance of Splunk is required (unless you choose to use InfluxDB/Grafana ). Splunk is a flexible data collection and reporting package that can take data sent by the wiperf probe and present it in a nice report format. Splunk can be installed on a wide variety of platforms that can be viewed at : https://www.splunk.com/en_us/download/splunk-enterprise.html This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. Note that a free account sign-up is required when downloading the software from the link listed above. To install Splunk and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . The product being installed is Splunk Enterprise. This is a paid-for product, but it has a free-tier for low data volumes (500Mbytes per day). Install initially with all the licensing defaults and then drop back to the free-tier by selecting Settings > Licensing and selecting the free tier. The free tier is plenty for the low volume rates that the wiperf probe generates when deploying probes at small-scale. Attention If you forget to select the free tier and your trial license expires, you may become locked out of the GUI with a \u201clicense expired\u201d message. If this happens, from the CLI of your Splunk server, find the file \u201cserver.conf\u201d and add the following line to the bottom of the file: [license] active_group = Lite_Free Then, restart the Splunk server and the login issue should be fixed. (The file is /opt/splunk/etc/system/local/server.conf on Linux) Connectivity Planning One area to consider is network connectivity between the wiperf probe and the Splunk instance. The wiperf probe needs to be able to access the Splunk server to send its results data. If the wiperf probe probe is being deployed on a wireless network, how is the results data generated going to get back to the Splunk server? If the probe is being deployed on a customer network to perform temporary monitoring, it will need to join the wireless network under test (or be plugged in to an ethernet switch port if testing a wired connection). But, how is the wiperf probe going to send its data to the Splunk server ? Many environments may not be comfortable with hooking up the wiperf probe to their internal wired network, potentially bridging wired and wireless networks. In some instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier.) Three topology deployment options are supported: Results data over wireless Results data over Ethernet Results data over VPN/wireless The method used is configured on the wiperf probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the Splunk server. The 3 connectivity options are discussed below. Results Data Over Wireless In this topology the wiperf probe is configured to join an SSID that has the Splunk server accessible via its WLAN interface. Typically, the Splunk server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the Splunk server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of Splunk server> Results data over Ethernet If the Splunk server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of Splunk server> Results data over Zerotier/wireless A simple way of getting the wiperf probe talking with your Splunk server, if it has no direct access, is to use the Zerotier service to create a virtual overlay network via the Internet. In summary, both the Splunk server and wiperf probe have a Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same virtual network in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the Splunk server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees. It\u2019s very easy to use, plus your Splunk server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the Splunk server and wiperf probe when the client is installed. config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of Splunk server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Splunk Platform"},{"location":"splunk_platform/#splunk-platform","text":"To collect and view the test results data, an instance of Splunk is required (unless you choose to use InfluxDB/Grafana ). Splunk is a flexible data collection and reporting package that can take data sent by the wiperf probe and present it in a nice report format. Splunk can be installed on a wide variety of platforms that can be viewed at : https://www.splunk.com/en_us/download/splunk-enterprise.html This guide does not cover all installation details of the software package, these may be obtained when downloading and installing the software. Note that a free account sign-up is required when downloading the software from the link listed above. To install Splunk and use it with a handful of probes, a modest server may be built (e.g. I use a low-end Intel NUC running Ubuntu), so for testing purposes, don\u2019t get too hung up on sourcing a high end server. If you'd like to look into server requirements further, then check out this page . The product being installed is Splunk Enterprise. This is a paid-for product, but it has a free-tier for low data volumes (500Mbytes per day). Install initially with all the licensing defaults and then drop back to the free-tier by selecting Settings > Licensing and selecting the free tier. The free tier is plenty for the low volume rates that the wiperf probe generates when deploying probes at small-scale. Attention If you forget to select the free tier and your trial license expires, you may become locked out of the GUI with a \u201clicense expired\u201d message. If this happens, from the CLI of your Splunk server, find the file \u201cserver.conf\u201d and add the following line to the bottom of the file: [license] active_group = Lite_Free Then, restart the Splunk server and the login issue should be fixed. (The file is /opt/splunk/etc/system/local/server.conf on Linux)","title":"Splunk Platform"},{"location":"splunk_platform/#connectivity-planning","text":"One area to consider is network connectivity between the wiperf probe and the Splunk instance. The wiperf probe needs to be able to access the Splunk server to send its results data. If the wiperf probe probe is being deployed on a wireless network, how is the results data generated going to get back to the Splunk server? If the probe is being deployed on a customer network to perform temporary monitoring, it will need to join the wireless network under test (or be plugged in to an ethernet switch port if testing a wired connection). But, how is the wiperf probe going to send its data to the Splunk server ? Many environments may not be comfortable with hooking up the wiperf probe to their internal wired network, potentially bridging wired and wireless networks. In some instances an alternative is required (e.g. send the results data over the wireless network itself out to the Internet to a cloud instance or via a VPN solution such as Zerotier.) Three topology deployment options are supported: Results data over wireless Results data over Ethernet Results data over VPN/wireless The method used is configured on the wiperf probe in its config.ini file. It is important to understand the (viable) connectivity path prior to deploying both the probe and the Splunk server. The 3 connectivity options are discussed below.","title":"Connectivity Planning"},{"location":"splunk_platform/#results-data-over-wireless","text":"In this topology the wiperf probe is configured to join an SSID that has the Splunk server accessible via its WLAN interface. Typically, the Splunk server will reside in a cloud or perhaps on a publicly accessible VPS. The wiperf probe will continually run the performance tests over the wireless connection and then upload the results directly to the Splunk server over the WLAN connection. config.ini settings: mgt_if: wlan0 data_host: <public IP address of Splunk server>","title":"Results Data Over Wireless"},{"location":"splunk_platform/#results-data-over-ethernet","text":"If the Splunk server is being run on the inside of a network environment, it may be preferable to return results data via the Ethernet port of the wiperf probe. This topology also has the advantage of results data not being impacted if there are wireless connectivity issues on the wiperf probe WLAN connection. To achieve the correct traffic flow, a static route for management traffic is automatically injected into the route table of the wiperf probe to force results data over the Ethernet port. config.ini settings: mgt_if: eth0 data_host: <IP address of Splunk server>","title":"Results data over Ethernet"},{"location":"splunk_platform/#results-data-over-zerotierwireless","text":"A simple way of getting the wiperf probe talking with your Splunk server, if it has no direct access, is to use the Zerotier service to create a virtual overlay network via the Internet. In summary, both the Splunk server and wiperf probe have a Zerotier client installed. Both are then added to your Zerotier dashboard (by you) and they start talking! Under the hood, both devices have a new virtual network interface created and they connect to the Zerotier cloud-based network service so that they can communicate on the same virtual network in the cloud. As they are on the same subnet from a networking perspective, there are no routing issues to worry about to get results data from the wiperf probe to the Splunk server. Zerotier has a free subscription tier which allows up to 100 devices to be hooked up without having to pay any fees. It\u2019s very easy to use, plus your Splunk server can be anywhere! (e.g. on your laptop at home). Both devices need access to the Internet for this solution to work. You can sign up for free, create a virtual network and then just add the IDs that are created by the Splunk server and wiperf probe when the client is installed. config.ini settings: mgt_if: ztxxxxxx (check your local ZeroTier interface designation using ```ifconfig```) data_host: <IP address of Splunk server shown in Zerotier dashboard> Install ZeroTier To install Zerotier on the wiperf probe (or an Ubuntu server), enter the following: curl -s https://install.zerotier.com | sudo bash sudo zerotier-cli join <network number from your Zerotier dashboard> sudo zerotier-cli status # To remove at a later date: sudo apt remove zerotier-one","title":"Results data over Zerotier/wireless"},{"location":"splunk_software/","text":"Splunk Software To obtain the Splunk software for your data server, get along to the Splunk web site and sign up for an account if you don\u2019t already have one: https://www.splunk.com/en_us/download/splunk-enterprise.html Once you\u2019re logged in to the Splunk site, you\u2019ll have a number of OS options to choose from ( supported platforms can be viewed here ). There are options for Windows, Linux & Mac OS: (Note: the version of Splunk you can download will likely be different to the version shown below - choose the latest version of 8.x) Once you've hit the download button for your OS choice, the Splunk Enterprise software chosen will start to download to your local machine, ready for installation. It\u2019s worth checking the download page to see if there are further download options. If you check the graphic below, you can see there is a \u201cDownload via Command Line (wget)\u201d option, which can be a much easier way to get the code directly on to your server. The options you will see here will vary between OS selections: (Note: the version of Splunk you can download will likely be different to the version shown below - choose the latest version of 8.x)","title":"Splunk Software"},{"location":"splunk_software/#splunk-software","text":"To obtain the Splunk software for your data server, get along to the Splunk web site and sign up for an account if you don\u2019t already have one: https://www.splunk.com/en_us/download/splunk-enterprise.html Once you\u2019re logged in to the Splunk site, you\u2019ll have a number of OS options to choose from ( supported platforms can be viewed here ). There are options for Windows, Linux & Mac OS: (Note: the version of Splunk you can download will likely be different to the version shown below - choose the latest version of 8.x) Once you've hit the download button for your OS choice, the Splunk Enterprise software chosen will start to download to your local machine, ready for installation. It\u2019s worth checking the download page to see if there are further download options. If you check the graphic below, you can see there is a \u201cDownload via Command Line (wget)\u201d option, which can be a much easier way to get the code directly on to your server. The options you will see here will vary between OS selections: (Note: the version of Splunk you can download will likely be different to the version shown below - choose the latest version of 8.x)","title":"Splunk Software"},{"location":"troubleshooting/","text":"Troubleshooting If things seem to be going wrong, here are a few tips to guide you in your diagnosis of the issue. Network Connectivity If you are suspect network connectivity issues, your best course of action is to check the status of any interfaces being used by the probe. This can be done by accessing the CLI if the probe and running some of the commands provided below. Once interfaces have been verified, trying to access specific targets via network connectivity checks can also be useful. The following CLI commands will help to check the status of probe interfaces: Wireless NIC: iwconfig (Is the probe joining the wireless network? The SSID to which is is joined should be shown in the \"ESSID\" field) IP address: ifconfig eth0 , ifconfig wlan0 (Are the interfaces up? Do they have an IP address?) Network connectivity to a specific host: ping 192.168.0.254 Internet connectivity: ping google.com (Can the probe get to the Internet, if that is expected?) DNS connectivity: apt-get install dnsutils (install required commands), nslookup google.com Web connectivity: wget https://google.com (check if the required website target can be reached from the probe) iperf3 server connectivity: iperf3 -c 192.168.0.1 -t 10 -i 1 (run 10 sec tcp test to 192.168.0.1 server...alter for your iperf server address) Another useful source of information for connectivity issues in the syslog logging system of the probe. Take a look through the syslog file to see if there are any issues being reported that may be impacting your connectivity: tail -f /var/log/syslog Wiperf Configuration The wiperf configuration file is quite a complex file, so it's well worth checking for typos or critical fields that have been missed. The key fields worth double checking are: probe_mode mgt_if exporter_type (splunk) splunk_host splunk_port splunk_token (influxdb) influx_host influx_port influx_username influx_password influx_database One question to consider when deploying a probe is : Is the probe deployed in the topology you originally intended? If the environment is not as you expected and you need to use a different interface, make sure you have updated config.ini so that wiperf knows where to send test and management traffic (otherwise, you may hit routing issues) Logging Wiperf has extensive logging to help diagnose issues that may be causing operational issues. SSH to the probe and monitor the output of the log file /var/log/wiperf_agent.log . This file is created the first time that wiperf runs. If the file is not created after 5 minutes, then check the log file /var/log/wiperf_cron.log for error messages, as something fundamental is wrong with the installation. To watch the output of /var/log/wiperf_agent.log in real-time and view activity as data is collected every 5 minutes, run the following command on the CLI of the probe: tail -f /var/log/wiperf_agent.log Every 5 minutes, new log output will be seen that look similar to this: 2020-07-11 11:47:04,214 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,215 - Probe_Log - INFO - Starting logging... 2020-07-11 11:47:04,216 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,240 - Probe_Log - INFO - Checking if we use remote cfg file... 2020-07-11 11:47:04,241 - Probe_Log - INFO - No remote cfg file confgured...using current local ini file. 2020-07-11 11:47:04,242 - Probe_Log - INFO - No lock file found. Creating lock file. 2020-07-11 11:47:04,243 - Probe_Log - INFO - ########## Network connection checks ########## 2020-07-11 11:47:05,245 - Probe_Log - INFO - Checking wireless connection is good...(layer 1 &2) 2020-07-11 11:47:05,246 - Probe_Log - INFO - Checking wireless connection available. 2020-07-11 11:47:05,355 - Probe_Log - INFO - Checking we're connected to the network (layer3) 2020-07-11 11:47:05,356 - Probe_Log - INFO - Checking we have an IP address. 2020-07-11 11:47:05,379 - Probe_Log - INFO - Checking we can do a DNS lookup to google.com 2020-07-11 11:47:05,406 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:05,430 - Probe_Log - INFO - Checked interface route to : 216.58.212.238. Result: 216.58.212.238 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:05,431 - Probe_Log - INFO - Checking we can get to the management platform... 2020-07-11 11:47:05,432 - Probe_Log - INFO - Checking we will send mgt traffic over configured interface 'lo' mode. 2020-07-11 11:47:05,455 - Probe_Log - INFO - Checked interface route to : 127.0.0.1. Result: local 127.0.0.1 dev lo src 127.0.0.1 uid 0 2020-07-11 11:47:05,456 - Probe_Log - INFO - Interface mgt interface route looks good. 2020-07-11 11:47:05,457 - Probe_Log - INFO - Checking port connection to InfluxDB server 127.0.0.1, port: 8086 2020-07-11 11:47:05,484 - Probe_Log - INFO - Port connection to server 127.0.0.1, port: 8086 checked OK. 2020-07-11 11:47:05,485 - Probe_Log - INFO - ########## Wireless Connection ########## 2020-07-11 11:47:05,486 - Probe_Log - INFO - Wireless connection data: SSID:BNL, BSSID:5C:5B:35:C8:4D:C2, Freq:5.5, Center Freq:5.51, Channel: 100, Channel Width: 40, Tx Phy rate:200.0, Rx Phy rate:135.0, Tx MCS: 0, Rx MCS: 0, RSSI:-42.0, Tx retries:187, IP address:192.168.0.48 2020-07-11 11:47:05,486 - Probe_Log - INFO - InfluxDB update: wiperf-network, source=Network Tests 2020-07-11 11:47:05,487 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:05,573 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:05,574 - Probe_Log - INFO - Connection results sent OK. 2020-07-11 11:47:05,595 - Probe_Log - INFO - ########## speedtest ########## 2020-07-11 11:47:05,597 - Probe_Log - INFO - Starting speedtest... 2020-07-11 11:47:06,599 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:06,623 - Probe_Log - INFO - Checked interface route to : 8.8.8.8. Result: 8.8.8.8 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:06,624 - Probe_Log - INFO - Speedtest in progress....please wait. 2020-07-11 11:47:28,761 - Probe_Log - INFO - ping_time: 31, download_rate: 41.56, upload_rate: 9.74, server_name: speedtest-net5.rapidswitch.co.uk:8080 2020-07-11 11:47:28,766 - Probe_Log - INFO - Speedtest ended. 2020-07-11 11:47:28,767 - Probe_Log - INFO - InfluxDB update: wiperf-speedtest, source=Speedtest 2020-07-11 11:47:28,768 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:28,858 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:28,860 - Probe_Log - INFO - Speedtest results sent OK. The output is quite verbose and detailed, but it will provide a good indication of where wiperf is having difficulties. Miscellaneous Checks NTP Make sure your probe and Splunk servers are NTP sync'ed and are showing the same data and time. Check the time and date of the probe using the CLI command date Hostname If you have changed the probe hostname from its default, make sure you have updated both the /etc/hosts AND the /etc/hostname files with the new name (if done incorrectly, this can cause some very weird issues!)","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"If things seem to be going wrong, here are a few tips to guide you in your diagnosis of the issue.","title":"Troubleshooting"},{"location":"troubleshooting/#network-connectivity","text":"If you are suspect network connectivity issues, your best course of action is to check the status of any interfaces being used by the probe. This can be done by accessing the CLI if the probe and running some of the commands provided below. Once interfaces have been verified, trying to access specific targets via network connectivity checks can also be useful. The following CLI commands will help to check the status of probe interfaces: Wireless NIC: iwconfig (Is the probe joining the wireless network? The SSID to which is is joined should be shown in the \"ESSID\" field) IP address: ifconfig eth0 , ifconfig wlan0 (Are the interfaces up? Do they have an IP address?) Network connectivity to a specific host: ping 192.168.0.254 Internet connectivity: ping google.com (Can the probe get to the Internet, if that is expected?) DNS connectivity: apt-get install dnsutils (install required commands), nslookup google.com Web connectivity: wget https://google.com (check if the required website target can be reached from the probe) iperf3 server connectivity: iperf3 -c 192.168.0.1 -t 10 -i 1 (run 10 sec tcp test to 192.168.0.1 server...alter for your iperf server address) Another useful source of information for connectivity issues in the syslog logging system of the probe. Take a look through the syslog file to see if there are any issues being reported that may be impacting your connectivity: tail -f /var/log/syslog","title":"Network Connectivity"},{"location":"troubleshooting/#wiperf-configuration","text":"The wiperf configuration file is quite a complex file, so it's well worth checking for typos or critical fields that have been missed. The key fields worth double checking are: probe_mode mgt_if exporter_type (splunk) splunk_host splunk_port splunk_token (influxdb) influx_host influx_port influx_username influx_password influx_database One question to consider when deploying a probe is : Is the probe deployed in the topology you originally intended? If the environment is not as you expected and you need to use a different interface, make sure you have updated config.ini so that wiperf knows where to send test and management traffic (otherwise, you may hit routing issues)","title":"Wiperf Configuration"},{"location":"troubleshooting/#logging","text":"Wiperf has extensive logging to help diagnose issues that may be causing operational issues. SSH to the probe and monitor the output of the log file /var/log/wiperf_agent.log . This file is created the first time that wiperf runs. If the file is not created after 5 minutes, then check the log file /var/log/wiperf_cron.log for error messages, as something fundamental is wrong with the installation. To watch the output of /var/log/wiperf_agent.log in real-time and view activity as data is collected every 5 minutes, run the following command on the CLI of the probe: tail -f /var/log/wiperf_agent.log Every 5 minutes, new log output will be seen that look similar to this: 2020-07-11 11:47:04,214 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,215 - Probe_Log - INFO - Starting logging... 2020-07-11 11:47:04,216 - Probe_Log - INFO - ***************************************************** 2020-07-11 11:47:04,240 - Probe_Log - INFO - Checking if we use remote cfg file... 2020-07-11 11:47:04,241 - Probe_Log - INFO - No remote cfg file confgured...using current local ini file. 2020-07-11 11:47:04,242 - Probe_Log - INFO - No lock file found. Creating lock file. 2020-07-11 11:47:04,243 - Probe_Log - INFO - ########## Network connection checks ########## 2020-07-11 11:47:05,245 - Probe_Log - INFO - Checking wireless connection is good...(layer 1 &2) 2020-07-11 11:47:05,246 - Probe_Log - INFO - Checking wireless connection available. 2020-07-11 11:47:05,355 - Probe_Log - INFO - Checking we're connected to the network (layer3) 2020-07-11 11:47:05,356 - Probe_Log - INFO - Checking we have an IP address. 2020-07-11 11:47:05,379 - Probe_Log - INFO - Checking we can do a DNS lookup to google.com 2020-07-11 11:47:05,406 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:05,430 - Probe_Log - INFO - Checked interface route to : 216.58.212.238. Result: 216.58.212.238 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:05,431 - Probe_Log - INFO - Checking we can get to the management platform... 2020-07-11 11:47:05,432 - Probe_Log - INFO - Checking we will send mgt traffic over configured interface 'lo' mode. 2020-07-11 11:47:05,455 - Probe_Log - INFO - Checked interface route to : 127.0.0.1. Result: local 127.0.0.1 dev lo src 127.0.0.1 uid 0 2020-07-11 11:47:05,456 - Probe_Log - INFO - Interface mgt interface route looks good. 2020-07-11 11:47:05,457 - Probe_Log - INFO - Checking port connection to InfluxDB server 127.0.0.1, port: 8086 2020-07-11 11:47:05,484 - Probe_Log - INFO - Port connection to server 127.0.0.1, port: 8086 checked OK. 2020-07-11 11:47:05,485 - Probe_Log - INFO - ########## Wireless Connection ########## 2020-07-11 11:47:05,486 - Probe_Log - INFO - Wireless connection data: SSID:BNL, BSSID:5C:5B:35:C8:4D:C2, Freq:5.5, Center Freq:5.51, Channel: 100, Channel Width: 40, Tx Phy rate:200.0, Rx Phy rate:135.0, Tx MCS: 0, Rx MCS: 0, RSSI:-42.0, Tx retries:187, IP address:192.168.0.48 2020-07-11 11:47:05,486 - Probe_Log - INFO - InfluxDB update: wiperf-network, source=Network Tests 2020-07-11 11:47:05,487 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:05,573 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:05,574 - Probe_Log - INFO - Connection results sent OK. 2020-07-11 11:47:05,595 - Probe_Log - INFO - ########## speedtest ########## 2020-07-11 11:47:05,597 - Probe_Log - INFO - Starting speedtest... 2020-07-11 11:47:06,599 - Probe_Log - INFO - Checking we are going to Internet on correct interface as we are in 'wireless' mode. 2020-07-11 11:47:06,623 - Probe_Log - INFO - Checked interface route to : 8.8.8.8. Result: 8.8.8.8 via 192.168.0.1 dev wlan0 src 192.168.0.48 uid 0 2020-07-11 11:47:06,624 - Probe_Log - INFO - Speedtest in progress....please wait. 2020-07-11 11:47:28,761 - Probe_Log - INFO - ping_time: 31, download_rate: 41.56, upload_rate: 9.74, server_name: speedtest-net5.rapidswitch.co.uk:8080 2020-07-11 11:47:28,766 - Probe_Log - INFO - Speedtest ended. 2020-07-11 11:47:28,767 - Probe_Log - INFO - InfluxDB update: wiperf-speedtest, source=Speedtest 2020-07-11 11:47:28,768 - Probe_Log - INFO - Sending data to Influx host: 127.0.0.1, port: 8086, database: wiperf) 2020-07-11 11:47:28,858 - Probe_Log - INFO - Data sent to influx OK 2020-07-11 11:47:28,860 - Probe_Log - INFO - Speedtest results sent OK. The output is quite verbose and detailed, but it will provide a good indication of where wiperf is having difficulties.","title":"Logging"},{"location":"troubleshooting/#miscellaneous-checks","text":"","title":"Miscellaneous Checks"},{"location":"troubleshooting/#ntp","text":"Make sure your probe and Splunk servers are NTP sync'ed and are showing the same data and time. Check the time and date of the probe using the CLI command date","title":"NTP"},{"location":"troubleshooting/#hostname","text":"If you have changed the probe hostname from its default, make sure you have updated both the /etc/hosts AND the /etc/hostname files with the new name (if done incorrectly, this can cause some very weird issues!)","title":"Hostname"}]}